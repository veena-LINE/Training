{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In linux \n",
    "# cd ~\n",
    "# wget https://raw.githubusercontent.com/midwire/free_zipcode_data/develop/all_us_zipcodes.csv\n",
    "# hdfs dfs -copyFromLocal ./all_us_zipcodes.csv /input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "# local[*] - as many cpu available\n",
    "sc = SparkContext(\"local[*]\", \"ZipCodes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rdd part  8\n",
      "Files Partition 4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([10,20,30, 40, 50])\n",
    "print (\"rdd part \", rdd.getNumPartitions())\n",
    "\n",
    "# fileRdd = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_zipcodes.csv\")\n",
    "fileRdd = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_states.csv\")\n",
    "\n",
    "# repartition\n",
    "# shuffle partitions\n",
    "fileRdd = fileRdd.repartition(4)\n",
    "# running in the driver, jupyther shell\n",
    "print(\"Files Partition\", fileRdd.getNumPartitions())\n",
    "\n",
    "# print the content of the partitions\n",
    "# the output shown in jupyter console in jupyter webui\n",
    "# this code shall run inside worker\n",
    "def showPartData(partitionData):\n",
    "    if (partitionData is None):\n",
    "        return\n",
    "    \n",
    "    i = 0 \n",
    "    \n",
    "    print (\"Part ==========\")\n",
    "    try:\n",
    "        for element in partitionData:\n",
    "            print (element)\n",
    "            #i += 1\n",
    "            if (i == 5):\n",
    "                break\n",
    "    except:\n",
    "        print(\"ERROR\")\n",
    "        \n",
    "    return partitionData\n",
    "# for every partition, showPartData is called with partitionData\n",
    "\n",
    "fileRdd = fileRdd.mapPartitions(showPartData)\n",
    "# this read only from first partition, take only 5 records\n",
    "#fileRdd.take(5)\n",
    "fileRdd.collect() # reads from all parititions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['FL,Florida', 'GA,Georgia', 'HI,Hawaii', 'ID,Idaho', 'IL,Illinois', 'IN,Indiana', 'IA,Iowa', 'KS,Kansas', 'KY,Kentucky', 'LA,Louisiana']\n",
      "['ME,Maine', 'MD,Maryland', 'MA,Massachusetts', 'MI,Michigan', 'MN,Minnesota', 'MS,Mississippi', 'MO,Missouri', 'MT,Montana', 'NE,Nebraska', 'NV,Nevada', 'NH,New Hampshire', 'NJ,New Jersey', 'NM,New Mexico', 'NY,New York', 'NC,North Carolina', 'ND,North Dakota', 'OH,Ohio', 'OK,Oklahoma']\n",
      "['OR,Oregon', 'PA,Pennsylvania', 'RI,Rhode Island', 'SC,South Carolina', 'SD,South Dakota', 'TN,Tennessee', 'TX,Texas', 'UT,Utah', 'VT,Vermont', 'VA,Virginia']\n",
      "['abbr,name', 'AL,Alabama', 'AK,Alaska', 'AZ,Arizona', 'AR,Arkansas', 'CA,California', 'CO,Colorado', 'CT,Connecticut', 'DE,Delaware', 'DC,District of Columbia', 'WA,Washington', 'WV,West Virginia', 'WI,Wisconsin', 'WY,Wyoming']\n"
     ]
    }
   ],
   "source": [
    "# fileRdd = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_zipcodes.csv\")\n",
    "fileRdd = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_states.csv\")\n",
    "fileRdd = fileRdd.repartition(4)\n",
    "\n",
    "#fileRdd = fileRdd.map(lambda line: line.strip().split(\",\"))\\\n",
    "#                 .map(lambda data: tuple(data))\n",
    "# glom, action function, debug, to collect data from partitions\n",
    "partData = fileRdd.glom().collect()\n",
    "for pd in partData:\n",
    "    print(pd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
