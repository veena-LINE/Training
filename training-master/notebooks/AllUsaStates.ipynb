{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local\", \"AllUSAStates\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count  52\n"
     ]
    }
   ],
   "source": [
    "fileRdd = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_states.csv\")\n",
    "# action\n",
    "print(\"Count \", fileRdd.count() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abbr,name\n"
     ]
    }
   ],
   "source": [
    "print (fileRdd.first())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL,Alabama', 'AK,Alaska', 'AZ,Arizona', 'AR,Arkansas', 'CA,California', 'CO,Colorado', 'CT,Connecticut', 'DE,Delaware', 'DC,District of Columbia', 'FL,Florida', 'GA,Georgia', 'HI,Hawaii', 'ID,Idaho', 'IL,Illinois', 'IN,Indiana', 'IA,Iowa', 'KS,Kansas', 'KY,Kentucky', 'LA,Louisiana', 'ME,Maine', 'MD,Maryland', 'MA,Massachusetts', 'MI,Michigan', 'MN,Minnesota', 'MS,Mississippi', 'MO,Missouri', 'MT,Montana', 'NE,Nebraska', 'NV,Nevada', 'NH,New Hampshire', 'NJ,New Jersey', 'NM,New Mexico', 'NY,New York', 'NC,North Carolina', 'ND,North Dakota', 'OH,Ohio', 'OK,Oklahoma', 'OR,Oregon', 'PA,Pennsylvania', 'RI,Rhode Island', 'SC,South Carolina', 'SD,South Dakota', 'TN,Tennessee', 'TX,Texas', 'UT,Utah', 'VT,Vermont', 'VA,Virginia', 'WA,Washington', 'WV,West Virginia', 'WI,Wisconsin', 'WY,Wyoming']\n"
     ]
    }
   ],
   "source": [
    "# first() read first line\n",
    "header = fileRdd.first()\n",
    "\n",
    "# filter takes the content/record when the condition is true\n",
    "# filter reject the record if the condition is false\n",
    "# exclude first line /header from records\n",
    "statesRdd = fileRdd.filter (lambda line: line != header )\n",
    "print(statesRdd.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AL,Alabama', 'AK,Alaska', 'AZ,Arizona', 'AR,Arkansas', 'CA,California']\n"
     ]
    }
   ],
   "source": [
    "print(statesRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['AL', 'Alabama'], ['AK', 'Alaska'], ['AZ', 'Arizona'], ['AR', 'Arkansas'], ['CA', 'California']]\n"
     ]
    }
   ],
   "source": [
    "# map, tranform the data, split line, convert cel to faren, covner units, clean data...\n",
    "splitRdd = statesRdd.map (lambda line: line.split(\",\"))\n",
    "\n",
    "print (splitRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AL', 'Alabama'), ('AK', 'Alaska'), ('AZ', 'Arizona'), ('AR', 'Arkansas'), ('CA', 'California')]\n"
     ]
    }
   ],
   "source": [
    "# convert the list of state code, state name into tuple\n",
    "\n",
    "# state is a list ['AL', 'alabama'], then output will be ('AL', 'alabama')\n",
    "# tuppleRdd also can be called as KeyedRdd where as state[0], ie AL is a key, state name' Alabama is a value\n",
    "tupleRdd = splitRdd.map (lambda state: (state[0], state[1]) )\n",
    "print (tupleRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asc  [('AK', 'Alaska'), ('AL', 'Alabama'), ('AR', 'Arkansas'), ('AZ', 'Arizona'), ('CA', 'California')]\n"
     ]
    }
   ],
   "source": [
    "# sort the data in ascending order based on key which is the State code\n",
    "sortedRdd = tupleRdd.sortByKey()\n",
    "print(\"asc \", sortedRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('WY', 'Wyoming'), ('WV', 'West Virginia'), ('WI', 'Wisconsin'), ('WA', 'Washington'), ('VT', 'Vermont')]\n"
     ]
    }
   ],
   "source": [
    "# sort the data in decending order based on key\n",
    "# False means, decending order\n",
    "sortedDescRdd = tupleRdd.sortByKey(False)\n",
    "print(sortedDescRdd.take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sortedDescRdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the results into Hadoop /output/states-sorted\n",
    "sortedDescRdd.saveAsTextFile(\"hdfs://192.168.93.128:9000/output/states-sorted\")\n",
    "#  hdfs dfs -cat /output/states-sorted/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'AL': 1, 'AK': 1, 'AZ': 1, 'AR': 1, 'CA': 1, 'CO': 1, 'CT': 1, 'DE': 1, 'DC': 1, 'FL': 1, 'GA': 1, 'HI': 1, 'ID': 1, 'IL': 1, 'IN': 1, 'IA': 1, 'KS': 1, 'KY': 1, 'LA': 1, 'ME': 1, 'MD': 1, 'MA': 1, 'MI': 1, 'MN': 1, 'MS': 1, 'MO': 1, 'MT': 1, 'NE': 1, 'NV': 1, 'NH': 1, 'NJ': 1, 'NM': 1, 'NY': 1, 'NC': 1, 'ND': 1, 'OH': 1, 'OK': 1, 'OR': 1, 'PA': 1, 'RI': 1, 'SC': 1, 'SD': 1, 'TN': 1, 'TX': 1, 'UT': 1, 'VT': 1, 'VA': 1, 'WA': 1, 'WV': 1, 'WI': 1, 'WY': 1})\n"
     ]
    }
   ],
   "source": [
    "# countByKey() is for keyed rdd\n",
    "# take the occurance of the key, count them, and produce an rdd with (key , count)\n",
    "wordCount = tupleRdd.countByKey()\n",
    "\n",
    "print(wordCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
