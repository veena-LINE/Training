{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Customize before creating SparkSession\n",
    "\n",
    "NOTE: Options for executors should not exceed the physical memory available with them.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "conf = SparkConf()\n",
    "(\n",
    "    conf.setMaster(\"spark://192.168.11.77:7077\").setAppName(\"SS:CONFIG\")\n",
    "    .set(\"spark.executor.memory\", \"2G\")\n",
    "    .set(\"spark.executor.cores\", 2)\n",
    "    .set(\"spark.cores.max\", 2)\n",
    "    .set(\"spark.driver.memory\", \"2G\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# RDD `pyspark.SparkContext`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.PipelinedRDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(sc.parallelize(range(1, 5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# DF `pyspark.sql.SparkSession`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.config(conf=conf).master(\"spark://192.168.11.77:7077\").appName(\"DF\").getOrCreate()\n",
    "\n",
    "dataframe_reader = ss.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe_data = (\n",
    "    dataframe_reader\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .csv(\"hdfs://192.168.93.128:9000/input/\")\n",
    ")\n",
    "\n",
    "type(dataframe_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_states = ss.read.format(\"csv\").load(\"../data/all_us_states.csv\")\n",
    "type(df_us_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "| _c0|    _c1|\n",
      "+----+-------+\n",
      "|abbr|   name|\n",
      "|  AL|Alabama|\n",
      "+----+-------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_states = ss.read.csv(\"../data/all_us_states.csv\")\n",
    "df_us_states.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Dataset\n",
    "Dataset APIs are not available in Python yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Optimization\n",
    "## DF optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OPTION 1 : spark.sql.codegen\n",
    "\n",
    "This aill ask Spark to compile each SQL query into Java bytecode before executing it.\n",
    "This codegen option could make longer or repeated queries substantially faster as Spark generates specific code to run them.\n",
    "\n",
    "BAD: Setting this option for shorter or non-frequently used queries add compiler overhead \n",
    "(because compiler is called for query to generate the Java bytecode)\n",
    "\"\"\"\n",
    "\n",
    "ss = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.codegen\", value=False)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "OPTION 2 : spark.sql.inMemoryColumnarStorage.batchSize\n",
    "\n",
    "Defaut value: 1000\n",
    "While chching a DF, Spark groups together rows in batches of 1000 and compresses them.\n",
    "Small batch size --> low compression ratio.\n",
    "Larger batch size --> better memory utilization and compression.\n",
    "\"\"\"\n",
    "\n",
    "ss = (\n",
    "    SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.sql.inMemoryColumnarStorage.batchSize\", value=1000)\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `spark-submit` options\n",
    "\n",
    "```bash\n",
    "$ spark-submit --executor-memory 20G --total-executor-cores 100 filename.py\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
