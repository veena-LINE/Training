{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BTFVSHG:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sparkDataFrame</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1d8d04ff348>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Since Spark 2.x, Spark unified Spark APIs, DF, Datasets, & SQL.\n",
    "SparkSession uses SparkContext internally.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "ss = SparkSession.builder.master(\"local\").appName(\"sparkDataFrame\").getOrCreate()\n",
    "\n",
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is A88C-3222\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\veena\\data\n",
      "\n",
      "05/04/2021  08:41 AM    <DIR>          .\n",
      "05/04/2021  08:41 AM    <DIR>          ..\n",
      "04/30/2021  09:02 PM            38,049 all_us_counties.csv\n",
      "04/30/2021  09:00 PM               656 all_us_states.csv\n",
      "04/30/2021  09:02 PM         2,072,181 all_us_zipcodes.csv\n",
      "               3 File(s)      2,110,886 bytes\n",
      "               2 Dir(s)  449,245,831,168 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir ..\\data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Read data from CSV\n",
    "\"\"\"\n",
    "\n",
    "df_us_states = ss.read.format(\"csv\").load(\"../data/all_us_states.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method load in module pyspark.sql.readwriter:\n",
      "\n",
      "load(path=None, format=None, schema=None, **options) method of pyspark.sql.readwriter.DataFrameReader instance\n",
      "    Loads data from a data source and returns it as a :class`DataFrame`.\n",
      "    \n",
      "    :param path: optional string or a list of string for file-system backed data sources.\n",
      "    :param format: optional string for format of the data source. Default to 'parquet'.\n",
      "    :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      "                   or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      "    :param options: all other string options\n",
      "    \n",
      "    >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
      "    ...     opt1=True, opt2=1, opt3='str')\n",
      "    >>> df.dtypes\n",
      "    [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      "    \n",
      "    >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      "    ...     'python/test_support/sql/people1.json'])\n",
      "    >>> df.dtypes\n",
      "    [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      "    \n",
      "    .. versionadded:: 1.4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ss.read.load)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(_c0='abbr', _c1='name')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_states.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_states.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_c0', 'string'), ('_c1', 'string')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_us_states.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_us_states = (\n",
    "    ss\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"../data/all_us_states.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- abbr: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_states.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on DataFrameReader in module pyspark.sql.readwriter object:\n",
      "\n",
      "class DataFrameReader(OptionUtils)\n",
      " |  DataFrameReader(spark)\n",
      " |  \n",
      " |  Interface used to load a :class:`DataFrame` from external storage systems\n",
      " |  (e.g. file systems, key-value stores, etc). Use :attr:`SparkSession.read`\n",
      " |  to access this.\n",
      " |  \n",
      " |  .. versionadded:: 1.4\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      DataFrameReader\n",
      " |      OptionUtils\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, spark)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  csv(self, path, schema=None, sep=None, encoding=None, quote=None, escape=None, comment=None, header=None, inferSchema=None, ignoreLeadingWhiteSpace=None, ignoreTrailingWhiteSpace=None, nullValue=None, nanValue=None, positiveInf=None, negativeInf=None, dateFormat=None, timestampFormat=None, maxColumns=None, maxCharsPerColumn=None, maxMalformedLogPerPartition=None, mode=None, columnNameOfCorruptRecord=None, multiLine=None, charToEscapeQuoteEscaping=None, samplingRatio=None, enforceSchema=None, emptyValue=None)\n",
      " |      Loads a CSV file and returns the result as a  :class:`DataFrame`.\n",
      " |      \n",
      " |      This function will go through the input once to determine the input schema if\n",
      " |      ``inferSchema`` is enabled. To avoid going through the entire data once, disable\n",
      " |      ``inferSchema`` option or specify the schema explicitly using ``schema``.\n",
      " |      \n",
      " |      :param path: string, or list of strings, for input path(s),\n",
      " |                   or RDD of Strings storing CSV rows.\n",
      " |      :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |                     or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      :param sep: sets a single character as a separator for each field and value.\n",
      " |                  If None is set, it uses the default value, ``,``.\n",
      " |      :param encoding: decodes the CSV files by the given encoding type. If None is set,\n",
      " |                       it uses the default value, ``UTF-8``.\n",
      " |      :param quote: sets a single character used for escaping quoted values where the\n",
      " |                    separator can be part of the value. If None is set, it uses the default\n",
      " |                    value, ``\"``. If you would like to turn off quotations, you need to set an\n",
      " |                    empty string.\n",
      " |      :param escape: sets a single character used for escaping quotes inside an already\n",
      " |                     quoted value. If None is set, it uses the default value, ``\\``.\n",
      " |      :param comment: sets a single character used for skipping lines beginning with this\n",
      " |                      character. By default (None), it is disabled.\n",
      " |      :param header: uses the first line as names of columns. If None is set, it uses the\n",
      " |                     default value, ``false``.\n",
      " |      :param inferSchema: infers the input schema automatically from data. It requires one extra\n",
      " |                     pass over the data. If None is set, it uses the default value, ``false``.\n",
      " |      :param enforceSchema: If it is set to ``true``, the specified or inferred schema will be\n",
      " |                            forcibly applied to datasource files, and headers in CSV files will be\n",
      " |                            ignored. If the option is set to ``false``, the schema will be\n",
      " |                            validated against all headers in CSV files or the first header in RDD\n",
      " |                            if the ``header`` option is set to ``true``. Field names in the schema\n",
      " |                            and column names in CSV headers are checked by their positions\n",
      " |                            taking into account ``spark.sql.caseSensitive``. If None is set,\n",
      " |                            ``true`` is used by default. Though the default value is ``true``,\n",
      " |                            it is recommended to disable the ``enforceSchema`` option\n",
      " |                            to avoid incorrect results.\n",
      " |      :param ignoreLeadingWhiteSpace: A flag indicating whether or not leading whitespaces from\n",
      " |                                      values being read should be skipped. If None is set, it\n",
      " |                                      uses the default value, ``false``.\n",
      " |      :param ignoreTrailingWhiteSpace: A flag indicating whether or not trailing whitespaces from\n",
      " |                                       values being read should be skipped. If None is set, it\n",
      " |                                       uses the default value, ``false``.\n",
      " |      :param nullValue: sets the string representation of a null value. If None is set, it uses\n",
      " |                        the default value, empty string. Since 2.0.1, this ``nullValue`` param\n",
      " |                        applies to all supported types including the string type.\n",
      " |      :param nanValue: sets the string representation of a non-number value. If None is set, it\n",
      " |                       uses the default value, ``NaN``.\n",
      " |      :param positiveInf: sets the string representation of a positive infinity value. If None\n",
      " |                          is set, it uses the default value, ``Inf``.\n",
      " |      :param negativeInf: sets the string representation of a negative infinity value. If None\n",
      " |                          is set, it uses the default value, ``Inf``.\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param maxColumns: defines a hard limit of how many columns a record can have. If None is\n",
      " |                         set, it uses the default value, ``20480``.\n",
      " |      :param maxCharsPerColumn: defines the maximum number of characters allowed for any given\n",
      " |                                value being read. If None is set, it uses the default value,\n",
      " |                                ``-1`` meaning unlimited length.\n",
      " |      :param maxMalformedLogPerPartition: this parameter is no longer used since Spark 2.2.0.\n",
      " |                                          If specified, it is ignored.\n",
      " |      :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |                   set, it uses the default value, ``PERMISSIVE``. Note that Spark tries to\n",
      " |                   parse only required columns in CSV under column pruning. Therefore, corrupt\n",
      " |                   records can be different based on required set of fields. This behavior can\n",
      " |                   be controlled by ``spark.sql.csv.parser.columnPruning.enabled``\n",
      " |                   (enabled by default).\n",
      " |      \n",
      " |              * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string \\\n",
      " |                into a field configured by ``columnNameOfCorruptRecord``, and sets other \\\n",
      " |                fields to ``null``. To keep corrupt records, an user can set a string type \\\n",
      " |                field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a \\\n",
      " |                schema does not have the field, it drops corrupt records during parsing. \\\n",
      " |                A record with less/more tokens than schema is not a corrupted record to CSV. \\\n",
      " |                When it meets a record having fewer tokens than the length of the schema, \\\n",
      " |                sets ``null`` to extra fields. When the record has more tokens than the \\\n",
      " |                length of the schema, it drops extra tokens.\n",
      " |              * ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      " |              * ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      " |                                        created by ``PERMISSIVE`` mode. This overrides\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |                                        it uses the value specified in\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      :param multiLine: parse records, which may span multiple lines. If None is\n",
      " |                        set, it uses the default value, ``false``.\n",
      " |      :param charToEscapeQuoteEscaping: sets a single character used for escaping the escape for\n",
      " |                                        the quote character. If None is set, the default value is\n",
      " |                                        escape character when escape and quote characters are\n",
      " |                                        different, ``\\0`` otherwise.\n",
      " |      :param samplingRatio: defines fraction of rows used for schema inferring.\n",
      " |                            If None is set, it uses the default value, ``1.0``.\n",
      " |      :param emptyValue: sets the string representation of an empty value. If None is set, it uses\n",
      " |                         the default value, empty string.\n",
      " |      \n",
      " |      >>> df = spark.read.csv('python/test_support/sql/ages.csv')\n",
      " |      >>> df.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/ages.csv')\n",
      " |      >>> df2 = spark.read.csv(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('_c0', 'string'), ('_c1', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 2.0\n",
      " |  \n",
      " |  format(self, source)\n",
      " |      Specifies the input data source format.\n",
      " |      \n",
      " |      :param source: string, name of the data source, e.g. 'json', 'parquet'.\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load('python/test_support/sql/people.json')\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  jdbc(self, url, table, column=None, lowerBound=None, upperBound=None, numPartitions=None, predicates=None, properties=None)\n",
      " |      Construct a :class:`DataFrame` representing the database table named ``table``\n",
      " |      accessible via JDBC URL ``url`` and connection ``properties``.\n",
      " |      \n",
      " |      Partitions of the table will be retrieved in parallel if either ``column`` or\n",
      " |      ``predicates`` is specified. ``lowerBound`, ``upperBound`` and ``numPartitions``\n",
      " |      is needed when ``column`` is specified.\n",
      " |      \n",
      " |      If both ``column`` and ``predicates`` are specified, ``column`` will be used.\n",
      " |      \n",
      " |      .. note:: Don't create too many partitions in parallel on a large cluster;\n",
      " |          otherwise Spark might crash your external database systems.\n",
      " |      \n",
      " |      :param url: a JDBC URL of the form ``jdbc:subprotocol:subname``\n",
      " |      :param table: the name of the table\n",
      " |      :param column: the name of a column of numeric, date, or timestamp type\n",
      " |                     that will be used for partitioning;\n",
      " |                     if this parameter is specified, then ``numPartitions``, ``lowerBound``\n",
      " |                     (inclusive), and ``upperBound`` (exclusive) will form partition strides\n",
      " |                     for generated WHERE clause expressions used to split the column\n",
      " |                     ``column`` evenly\n",
      " |      :param lowerBound: the minimum value of ``column`` used to decide partition stride\n",
      " |      :param upperBound: the maximum value of ``column`` used to decide partition stride\n",
      " |      :param numPartitions: the number of partitions\n",
      " |      :param predicates: a list of expressions suitable for inclusion in WHERE clauses;\n",
      " |                         each one defines one partition of the :class:`DataFrame`\n",
      " |      :param properties: a dictionary of JDBC database connection arguments. Normally at\n",
      " |                         least properties \"user\" and \"password\" with their corresponding values.\n",
      " |                         For example { 'user' : 'SYSTEM', 'password' : 'mypassword' }\n",
      " |      :return: a DataFrame\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  json(self, path, schema=None, primitivesAsString=None, prefersDecimal=None, allowComments=None, allowUnquotedFieldNames=None, allowSingleQuotes=None, allowNumericLeadingZero=None, allowBackslashEscapingAnyCharacter=None, mode=None, columnNameOfCorruptRecord=None, dateFormat=None, timestampFormat=None, multiLine=None, allowUnquotedControlChars=None, lineSep=None, samplingRatio=None, dropFieldIfAllNull=None, encoding=None)\n",
      " |      Loads JSON files and returns the results as a :class:`DataFrame`.\n",
      " |      \n",
      " |      `JSON Lines <http://jsonlines.org/>`_ (newline-delimited JSON) is supported by default.\n",
      " |      For JSON (one record per file), set the ``multiLine`` parameter to ``true``.\n",
      " |      \n",
      " |      If the ``schema`` parameter is not specified, this function goes\n",
      " |      through the input once to determine the input schema.\n",
      " |      \n",
      " |      :param path: string represents path to the JSON dataset, or a list of paths,\n",
      " |                   or RDD of Strings storing JSON objects.\n",
      " |      :param schema: an optional :class:`pyspark.sql.types.StructType` for the input schema or\n",
      " |                     a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      :param primitivesAsString: infers all primitive values as a string type. If None is set,\n",
      " |                                 it uses the default value, ``false``.\n",
      " |      :param prefersDecimal: infers all floating-point values as a decimal type. If the values\n",
      " |                             do not fit in decimal, then it infers them as doubles. If None is\n",
      " |                             set, it uses the default value, ``false``.\n",
      " |      :param allowComments: ignores Java/C++ style comment in JSON records. If None is set,\n",
      " |                            it uses the default value, ``false``.\n",
      " |      :param allowUnquotedFieldNames: allows unquoted JSON field names. If None is set,\n",
      " |                                      it uses the default value, ``false``.\n",
      " |      :param allowSingleQuotes: allows single quotes in addition to double quotes. If None is\n",
      " |                                      set, it uses the default value, ``true``.\n",
      " |      :param allowNumericLeadingZero: allows leading zeros in numbers (e.g. 00012). If None is\n",
      " |                                      set, it uses the default value, ``false``.\n",
      " |      :param allowBackslashEscapingAnyCharacter: allows accepting quoting of all character\n",
      " |                                                 using backslash quoting mechanism. If None is\n",
      " |                                                 set, it uses the default value, ``false``.\n",
      " |      :param mode: allows a mode for dealing with corrupt records during parsing. If None is\n",
      " |                   set, it uses the default value, ``PERMISSIVE``.\n",
      " |      \n",
      " |              * ``PERMISSIVE`` : when it meets a corrupted record, puts the malformed string                   into a field configured by ``columnNameOfCorruptRecord``, and sets other                   fields to ``null``. To keep corrupt records, an user can set a string type                   field named ``columnNameOfCorruptRecord`` in an user-defined schema. If a                   schema does not have the field, it drops corrupt records during parsing.                   When inferring a schema, it implicitly adds a ``columnNameOfCorruptRecord``                   field in an output schema.\n",
      " |              *  ``DROPMALFORMED`` : ignores the whole corrupted records.\n",
      " |              *  ``FAILFAST`` : throws an exception when it meets corrupted records.\n",
      " |      \n",
      " |      :param columnNameOfCorruptRecord: allows renaming the new field having malformed string\n",
      " |                                        created by ``PERMISSIVE`` mode. This overrides\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``. If None is set,\n",
      " |                                        it uses the value specified in\n",
      " |                                        ``spark.sql.columnNameOfCorruptRecord``.\n",
      " |      :param dateFormat: sets the string that indicates a date format. Custom date formats\n",
      " |                         follow the formats at ``java.text.SimpleDateFormat``. This\n",
      " |                         applies to date type. If None is set, it uses the\n",
      " |                         default value, ``yyyy-MM-dd``.\n",
      " |      :param timestampFormat: sets the string that indicates a timestamp format. Custom date\n",
      " |                              formats follow the formats at ``java.text.SimpleDateFormat``.\n",
      " |                              This applies to timestamp type. If None is set, it uses the\n",
      " |                              default value, ``yyyy-MM-dd'T'HH:mm:ss.SSSXXX``.\n",
      " |      :param multiLine: parse one record, which may span multiple lines, per file. If None is\n",
      " |                        set, it uses the default value, ``false``.\n",
      " |      :param allowUnquotedControlChars: allows JSON Strings to contain unquoted control\n",
      " |                                        characters (ASCII characters with value less than 32,\n",
      " |                                        including tab and line feed characters) or not.\n",
      " |      :param encoding: allows to forcibly set one of standard basic or extended encoding for\n",
      " |                       the JSON files. For example UTF-16BE, UTF-32LE. If None is set,\n",
      " |                       the encoding of input JSON will be detected automatically\n",
      " |                       when the multiLine option is set to ``true``.\n",
      " |      :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      " |                      set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      " |      :param samplingRatio: defines fraction of input JSON objects used for schema inferring.\n",
      " |                            If None is set, it uses the default value, ``1.0``.\n",
      " |      :param dropFieldIfAllNull: whether to ignore column of all null values or empty\n",
      " |                                 array/struct during schema inference. If None is set, it\n",
      " |                                 uses the default value, ``false``.\n",
      " |      \n",
      " |      >>> df1 = spark.read.json('python/test_support/sql/people.json')\n",
      " |      >>> df1.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      >>> rdd = sc.textFile('python/test_support/sql/people.json')\n",
      " |      >>> df2 = spark.read.json(rdd)\n",
      " |      >>> df2.dtypes\n",
      " |      [('age', 'bigint'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  load(self, path=None, format=None, schema=None, **options)\n",
      " |      Loads data from a data source and returns it as a :class`DataFrame`.\n",
      " |      \n",
      " |      :param path: optional string or a list of string for file-system backed data sources.\n",
      " |      :param format: optional string for format of the data source. Default to 'parquet'.\n",
      " |      :param schema: optional :class:`pyspark.sql.types.StructType` for the input schema\n",
      " |                     or a DDL-formatted string (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      :param options: all other string options\n",
      " |      \n",
      " |      >>> df = spark.read.format(\"parquet\").load('python/test_support/sql/parquet_partitioned',\n",
      " |      ...     opt1=True, opt2=1, opt3='str')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      >>> df = spark.read.format('json').load(['python/test_support/sql/people.json',\n",
      " |      ...     'python/test_support/sql/people1.json'])\n",
      " |      >>> df.dtypes\n",
      " |      [('age', 'bigint'), ('aka', 'string'), ('name', 'string')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  option(self, key, value)\n",
      " |      Adds an input option for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n",
      " |              in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  options(self, **options)\n",
      " |      Adds input options for the underlying data source.\n",
      " |      \n",
      " |      You can set the following option(s) for reading files:\n",
      " |          * ``timeZone``: sets the string that indicates a timezone to be used to parse timestamps\n",
      " |              in the JSON/CSV datasources or partition values.\n",
      " |              If it isn't set, it uses the default value, session local timezone.\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  orc(self, path)\n",
      " |      Loads ORC files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      >>> df = spark.read.orc('python/test_support/sql/orc_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('a', 'bigint'), ('b', 'int'), ('c', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.5\n",
      " |  \n",
      " |  parquet(self, *paths)\n",
      " |      Loads Parquet files, returning the result as a :class:`DataFrame`.\n",
      " |      \n",
      " |      You can set the following Parquet-specific option(s) for reading Parquet files:\n",
      " |          * ``mergeSchema``: sets whether we should merge schemas collected from all                 Parquet part-files. This will override ``spark.sql.parquet.mergeSchema``.                 The default value is specified in ``spark.sql.parquet.mergeSchema``.\n",
      " |      \n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  schema(self, schema)\n",
      " |      Specifies the input schema.\n",
      " |      \n",
      " |      Some data sources (e.g. JSON) can infer the input schema automatically from data.\n",
      " |      By specifying the schema here, the underlying data source can skip the schema\n",
      " |      inference step, and thus speed up data loading.\n",
      " |      \n",
      " |      :param schema: a :class:`pyspark.sql.types.StructType` object or a DDL-formatted string\n",
      " |                     (For example ``col0 INT, col1 DOUBLE``).\n",
      " |      \n",
      " |      >>> s = spark.read.schema(\"col0 INT, col1 DOUBLE\")\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  table(self, tableName)\n",
      " |      Returns the specified table as a :class:`DataFrame`.\n",
      " |      \n",
      " |      :param tableName: string, name of the table.\n",
      " |      \n",
      " |      >>> df = spark.read.parquet('python/test_support/sql/parquet_partitioned')\n",
      " |      >>> df.createOrReplaceTempView('tmpTable')\n",
      " |      >>> spark.read.table('tmpTable').dtypes\n",
      " |      [('name', 'string'), ('year', 'int'), ('month', 'int'), ('day', 'int')]\n",
      " |      \n",
      " |      .. versionadded:: 1.4\n",
      " |  \n",
      " |  text(self, paths, wholetext=False, lineSep=None)\n",
      " |      Loads text files and returns a :class:`DataFrame` whose schema starts with a\n",
      " |      string column named \"value\", and followed by partitioned columns if there\n",
      " |      are any.\n",
      " |      \n",
      " |      By default, each line in the text file is a new row in the resulting DataFrame.\n",
      " |      \n",
      " |      :param paths: string, or list of strings, for input path(s).\n",
      " |      :param wholetext: if true, read each file from input path(s) as a single row.\n",
      " |      :param lineSep: defines the line separator that should be used for parsing. If None is\n",
      " |                      set, it covers all ``\\r``, ``\\r\\n`` and ``\\n``.\n",
      " |      \n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt')\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello'), Row(value='this')]\n",
      " |      >>> df = spark.read.text('python/test_support/sql/text-test.txt', wholetext=True)\n",
      " |      >>> df.collect()\n",
      " |      [Row(value='hello\\nthis')]\n",
      " |      \n",
      " |      .. versionadded:: 1.6\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from OptionUtils:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ss.read.option(\"encoding\", \"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- area_code: string (nullable = true)\n",
      " |-- lat: string (nullable = true)\n",
      " |-- lon: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_us_zipcodes = (\n",
    "    ss\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .load(\"../data/all_us_zipcodes.csv\")\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Notice that RDD DF creates all column types as String by default.\n",
    "\"\"\"\n",
    "df_us_zipcodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- area_code: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Enable Spark to infer schema *NOT GOOD practice*\n",
    "\"\"\"\n",
    "df_us_zipcodes = (\n",
    "    ss\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .load(\"../data/all_us_zipcodes.csv\")\n",
    ")\n",
    "\n",
    "\"\"\"\n",
    "Notice that RDD DF creates all column types as String by default.\n",
    "\"\"\"\n",
    "df_us_zipcodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- code: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- county: string (nullable = true)\n",
      " |-- area_code: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- lon: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Provide custom schema *GOOD practice*\n",
    "\"\"\"\n",
    "from pyspark.sql.types import StructType, StringType, IntegerType, DoubleType\n",
    "\n",
    "schema_zipcode = (\n",
    "    StructType()\n",
    "    .add(\"code\", IntegerType(), True)\n",
    "    .add(\"city\", StringType(), True)\n",
    "    .add(\"state\", StringType(), True)\n",
    "    .add(\"county\", StringType(), True)\n",
    "    .add(\"area_code\", StringType(), True)\n",
    "    .add(\"lat\", DoubleType(), True)\n",
    "    .add(\"lon\", DoubleType(), True)\n",
    ")\n",
    "\n",
    "df_us_zipcodes = (\n",
    "    ss\n",
    "    .read\n",
    "    .format(\"csv\")\n",
    "    .schema(schema_zipcode)\n",
    "    .option(\"header\", True)\n",
    "    .load(\"../data/all_us_zipcodes.csv\")\n",
    ")\n",
    "\n",
    "df_us_zipcodes.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- DF Schema --\n",
      "StructField(code,IntegerType,true)\n",
      "StructField(city,StringType,true)\n",
      "StructField(state,StringType,true)\n",
      "StructField(county,StringType,true)\n",
      "StructField(area_code,StringType,true)\n",
      "StructField(lat,DoubleType,true)\n",
      "StructField(lon,DoubleType,true)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Access DF schema \n",
    "\"\"\"\n",
    "print(\"-- DF Schema --\")\n",
    "for s in df_us_zipcodes.schema:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.types in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.types\n",
      "\n",
      "DESCRIPTION\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "    # contributor license agreements.  See the NOTICE file distributed with\n",
      "    # this work for additional information regarding copyright ownership.\n",
      "    # The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "    # (the \"License\"); you may not use this file except in compliance with\n",
      "    # the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #    http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        DataType\n",
      "            ArrayType\n",
      "            MapType\n",
      "            NullType\n",
      "            StructField\n",
      "            StructType\n",
      "    AtomicType(DataType)\n",
      "        BinaryType\n",
      "        BooleanType\n",
      "        DateType\n",
      "        StringType\n",
      "        TimestampType\n",
      "    FractionalType(NumericType)\n",
      "        DecimalType\n",
      "        DoubleType\n",
      "        FloatType\n",
      "    IntegralType(NumericType)\n",
      "        ByteType\n",
      "        IntegerType\n",
      "        LongType\n",
      "        ShortType\n",
      "    \n",
      "    class ArrayType(DataType)\n",
      "     |  ArrayType(elementType, containsNull=True)\n",
      "     |  \n",
      "     |  Array data type.\n",
      "     |  \n",
      "     |  :param elementType: :class:`DataType` of each element in the array.\n",
      "     |  :param containsNull: boolean, whether the array can contain null (None) values.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ArrayType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, elementType, containsNull=True)\n",
      "     |      >>> ArrayType(StringType()) == ArrayType(StringType(), True)\n",
      "     |      True\n",
      "     |      >>> ArrayType(StringType(), False) == ArrayType(StringType())\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BinaryType(AtomicType)\n",
      "     |  Binary (byte array) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BinaryType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BooleanType(AtomicType)\n",
      "     |  Boolean data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BooleanType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ByteType(IntegralType)\n",
      "     |  Byte data type, i.e. a signed integer in a single byte.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ByteType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataType(builtins.object)\n",
      "     |  Base class for data types.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DateType(AtomicType)\n",
      "     |  Date (datetime.date) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DateType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fromInternal(self, v)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, d)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  EPOCH_ORDINAL = 719163\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DecimalType(FractionalType)\n",
      "     |  DecimalType(precision=10, scale=0)\n",
      "     |  \n",
      "     |  Decimal (decimal.Decimal) data type.\n",
      "     |  \n",
      "     |  The DecimalType must have fixed precision (the maximum total number of digits)\n",
      "     |  and scale (the number of digits on the right of dot). For example, (5, 2) can\n",
      "     |  support the value from [-999.99 to 999.99].\n",
      "     |  \n",
      "     |  The precision can be up to 38, the scale must be less or equal to precision.\n",
      "     |  \n",
      "     |  When create a DecimalType, the default precision and scale is (10, 0). When infer\n",
      "     |  schema from decimal.Decimal objects, it will be DecimalType(38, 18).\n",
      "     |  \n",
      "     |  :param precision: the maximum total number of digits (default: 10)\n",
      "     |  :param scale: the number of digits on right side of dot. (default: 0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecimalType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision=10, scale=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DoubleType(FractionalType)\n",
      "     |  Double data type, representing double precision floats.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DoubleType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class FloatType(FractionalType)\n",
      "     |  Float data type, representing single precision floats.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FloatType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IntegerType(IntegralType)\n",
      "     |  Int data type, i.e. a signed 32-bit integer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IntegerType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LongType(IntegralType)\n",
      "     |  Long data type, i.e. a signed 64-bit integer.\n",
      "     |  \n",
      "     |  If the values are beyond the range of [-9223372036854775808, 9223372036854775807],\n",
      "     |  please use :class:`DecimalType`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LongType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MapType(DataType)\n",
      "     |  MapType(keyType, valueType, valueContainsNull=True)\n",
      "     |  \n",
      "     |  Map data type.\n",
      "     |  \n",
      "     |  :param keyType: :class:`DataType` of the keys in the map.\n",
      "     |  :param valueType: :class:`DataType` of the values in the map.\n",
      "     |  :param valueContainsNull: indicates whether values can contain null (None) values.\n",
      "     |  \n",
      "     |  Keys in a map data type are not allowed to be null (None).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MapType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, keyType, valueType, valueContainsNull=True)\n",
      "     |      >>> (MapType(StringType(), IntegerType())\n",
      "     |      ...        == MapType(StringType(), IntegerType(), True))\n",
      "     |      True\n",
      "     |      >>> (MapType(StringType(), IntegerType(), False)\n",
      "     |      ...        == MapType(StringType(), FloatType()))\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NullType(DataType)\n",
      "     |  Null type.\n",
      "     |  \n",
      "     |  The data type representing None, used for the types that cannot be inferred.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NullType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ShortType(IntegralType)\n",
      "     |  Short data type, i.e. a signed 16-bit integer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShortType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StringType(AtomicType)\n",
      "     |  String data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StringType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StructField(DataType)\n",
      "     |  StructField(name, dataType, nullable=True, metadata=None)\n",
      "     |  \n",
      "     |  A field in :class:`StructType`.\n",
      "     |  \n",
      "     |  :param name: string, name of the field.\n",
      "     |  :param dataType: :class:`DataType` of the field.\n",
      "     |  :param nullable: boolean, whether the field can be null (None) or not.\n",
      "     |  :param metadata: a dict from string to simple type that can be toInternald to JSON automatically\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StructField\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name, dataType, nullable=True, metadata=None)\n",
      "     |      >>> (StructField(\"f1\", StringType(), True)\n",
      "     |      ...      == StructField(\"f1\", StringType(), True))\n",
      "     |      True\n",
      "     |      >>> (StructField(\"f1\", StringType(), True)\n",
      "     |      ...      == StructField(\"f2\", StringType(), True))\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  typeName(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StructType(DataType)\n",
      "     |  StructType(fields=None)\n",
      "     |  \n",
      "     |  Struct type, consisting of a list of :class:`StructField`.\n",
      "     |  \n",
      "     |  This is the data type representing a :class:`Row`.\n",
      "     |  \n",
      "     |  Iterating a :class:`StructType` will iterate its :class:`StructField`\\s.\n",
      "     |  A contained :class:`StructField` can be accessed by name or position.\n",
      "     |  \n",
      "     |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |  >>> struct1[\"f1\"]\n",
      "     |  StructField(f1,StringType,true)\n",
      "     |  >>> struct1[0]\n",
      "     |  StructField(f1,StringType,true)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StructType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |      Access fields by name or slice.\n",
      "     |  \n",
      "     |  __init__(self, fields=None)\n",
      "     |      >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      "     |      ...     StructField(\"f2\", IntegerType(), False)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      False\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate the fields\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of fields.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add(self, field, data_type=None, nullable=True, metadata=None)\n",
      "     |      Construct a StructType by adding new elements to it to define the schema. The method accepts\n",
      "     |      either:\n",
      "     |      \n",
      "     |          a) A single parameter which is a StructField object.\n",
      "     |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      "     |             metadata(optional). The data_type parameter may be either a String or a\n",
      "     |             DataType object.\n",
      "     |      \n",
      "     |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n",
      "     |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      \n",
      "     |      :param field: Either the name of the field or a StructField object\n",
      "     |      :param data_type: If present, the DataType of the StructField to create\n",
      "     |      :param nullable: Whether the field to add should be nullable (default True)\n",
      "     |      :param metadata: Any additional metadata (default None)\n",
      "     |      :return: a new updated StructType\n",
      "     |  \n",
      "     |  fieldNames(self)\n",
      "     |      Returns all field names in a list.\n",
      "     |      \n",
      "     |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct.fieldNames()\n",
      "     |      ['f1']\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TimestampType(AtomicType)\n",
      "     |  Timestamp (datetime.datetime) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimestampType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fromInternal(self, ts)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, dt)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DataType', 'NullType', 'StringType', 'BinaryType', 'Boolea...\n",
      "\n",
      "FILE\n",
      "    c:\\spark-2.4.7-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "help(pyspark.sql.types)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
