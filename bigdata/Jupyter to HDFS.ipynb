{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "#### `$pip install findspark`\n",
    "<span style=\"color:red\">Note: This is not to be done on production!</span>\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This starts the Spark Driver and the Executor\n",
    "# Check Jupyter console for the Spark session port (4040/1/?)\n",
    "\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"Read-Write-HDFS\")\n",
    "sc.setLogLevel(\"WARN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark import SparkContext\n",
    "\n",
    "# sc = SparkContext(\"local\", \"Read-Write-HDFS\")\n",
    "\n",
    "# rdd_data = sc.textFile(\"hdfs://192.168.93.128:9000/input/README.md\")\n",
    "# print(type(rdd_data))\n",
    "\n",
    "# rdd_data_title = rdd_data.map(lambda line: line.title())\n",
    "\n",
    "# rdd_data_title.saveAsTextFile(\"hdfs://192.168.93.128:9000/output/README2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_data = sc.textFile(\"hdfs://192.168.93.128:9000/output/README2\")\n",
    "\n",
    "# rdd_data.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT: Lazy\n",
    "rdd_data = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_states.csv\")\n",
    "\n",
    "# Check Spark UI Monitoring (nothing happens as this is lazy loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRANSFORMATION, Lazy\n",
    "rdd_upper = rdd_data.map(lambda line: line.upper())\n",
    "\n",
    "# Check Spark UI Monitoring (nothing happens as this is lazy loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ABBR,NAME',\n",
       " 'AL,ALABAMA',\n",
       " 'AK,ALASKA',\n",
       " 'AZ,ARIZONA',\n",
       " 'AR,ARKANSAS',\n",
       " 'CA,CALIFORNIA',\n",
       " 'CO,COLORADO',\n",
       " 'CT,CONNECTICUT',\n",
       " 'DE,DELAWARE',\n",
       " 'DC,DISTRICT OF COLUMBIA']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ACTION\n",
    "\n",
    "rdd_upper.take(10)\n",
    "\n",
    "# Because this is an ACTION, Spark will convert the RDD into a DAG (with RRDD creation as the parent, the action (.take()) as the child)\n",
    "# Every ACTION triggers a JOB\n",
    "# A JOB contains STAGEs of TASKs inside it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_data_header = rdd_data.first()\n",
    "rdd_data_states = rdd_data.filter(lambda line: line != rdd_data_header)\n",
    "rdd_data_states.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(rdd_data_header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Tuple RDD is a.k.a Keyed RDD\n",
    "rdd_states_tuple = rdd_data_states.map(lambda line: line.split(\",\")).map(lambda state: (state[0], state[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_states_tuple.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default sort order = ASCENDING\n",
    "rdd_states_sorted = rdd_states_tuple.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_states_tuple.sortByKey(False).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd_states_tuple.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing results (in tuple form) into HDFS\n",
    "rdd_states_sorted.saveAsTextFile(\"hdfs://192.168.93.128:9000/output/states_sorted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.textFile(\"hdfs://192.168.93.128:9000/output/states_sorted\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_count = rdd_states_tuple.countByKey()\n",
    "print(states_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rdd_states_count = rdd_states_tuple.map(lambda state: (state[0], 1)).reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "print(rdd_data_states.map(lambda line: (line.split(\",\")[0],)).countByKey())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
