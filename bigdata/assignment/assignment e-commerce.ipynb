{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "```\n",
    "REQUIREMENT:\n",
    "    hdfs://192.168.93.128:9000/input/e-commerce/data.csv\n",
    "    Spark Session using SparkConf,\n",
    "        use 4 executor cores, \n",
    "        max 4 excutor cores,\n",
    "        use Spark Cluster\n",
    "    Write the result to hdfs \n",
    "        mostPopularMoviesDf.write.mode('overwrite')\\\n",
    "                          .csv(\"hdfs:....../output/top-movies.csv\")\n",
    "\n",
    "My Understanding:\n",
    "    Read from HDFS\n",
    "\n",
    "    spark-config\n",
    "    4 executor cores\n",
    "    4 max cores\n",
    "\n",
    "    Write back to HDFS\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1fb62b7e448>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Since Spark 2.x, Spark unified Spark APIs, DF, Datasets, & SQL.\n",
    "SparkSession uses SparkContext internally.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "config = SparkConf()\n",
    "config.setMaster(\"spark://192.168.11.77:7077\").setAppName(\"E-COMMERCE:CLUSTER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure before creating SparkSession\n",
    "\"\"\"\n",
    "\n",
    "conf = \\\n",
    "(\n",
    "    config\n",
    "    .set(\"spark.executor.memory\", \"2g\")\n",
    "    .set(\"spark.executor.cores\", 4)\n",
    "    .set(\"spark.cores.max\", 4)\n",
    "    .set(\"spark.driver.memory\", \"2g\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BTFVSHG:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.11.77:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>E-COMMERCE:CLUSTER</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1fb6441be48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Read e-commerce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Read CSV from HDFS\n",
    "\"\"\"\n",
    "\n",
    "import datetime as dt\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, StringType, DateType\n",
    "from pyspark.sql.functions import col, asc, desc, count, sum, avg, to_date, to_timestamp\n",
    "\n",
    "schema_ecomm = (\n",
    "    StructType()\n",
    "    .add(\"InvoiceNo\", StringType(), True)\n",
    "    .add(\"StockCode\", StringType(), True)\n",
    "    .add(\"Description\", StringType(), True)\n",
    "    .add(\"Quantity\", IntegerType(), True)\n",
    "    .add(\"InvoiceDate\", DateType(), True)\n",
    "    .add(\"UnitPrice\", DoubleType(), True)\n",
    "    .add(\"CustomerId\", StringType(), True)\n",
    "    .add(\"Country\", StringType(), True)\n",
    ")\n",
    "\n",
    "df_ecomm_full = (\n",
    "    ss.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"dateFormat\", \"MM/dd/yyyy HH:mm\")\n",
    "    .schema(schema_ecomm)\n",
    "    .load(\"hdfs://192.168.93.128:9000/input/e-commerce/data.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DROP un-necessary data/columns\n",
    ".drop('column_name', 'column_name')\n",
    "\"\"\"\n",
    "df_ecomm_full = df_ecomm_full[[\"Country\", \"CustomerId\", \"Quantity\", \"UnitPrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerId|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ecomm_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module pyspark.sql.types in pyspark.sql:\n",
      "\n",
      "NAME\n",
      "    pyspark.sql.types\n",
      "\n",
      "DESCRIPTION\n",
      "    # Licensed to the Apache Software Foundation (ASF) under one or more\n",
      "    # contributor license agreements.  See the NOTICE file distributed with\n",
      "    # this work for additional information regarding copyright ownership.\n",
      "    # The ASF licenses this file to You under the Apache License, Version 2.0\n",
      "    # (the \"License\"); you may not use this file except in compliance with\n",
      "    # the License.  You may obtain a copy of the License at\n",
      "    #\n",
      "    #    http://www.apache.org/licenses/LICENSE-2.0\n",
      "    #\n",
      "    # Unless required by applicable law or agreed to in writing, software\n",
      "    # distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "    # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "    # See the License for the specific language governing permissions and\n",
      "    # limitations under the License.\n",
      "    #\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        DataType\n",
      "            ArrayType\n",
      "            MapType\n",
      "            NullType\n",
      "            StructField\n",
      "            StructType\n",
      "    AtomicType(DataType)\n",
      "        BinaryType\n",
      "        BooleanType\n",
      "        DateType\n",
      "        StringType\n",
      "        TimestampType\n",
      "    FractionalType(NumericType)\n",
      "        DecimalType\n",
      "        DoubleType\n",
      "        FloatType\n",
      "    IntegralType(NumericType)\n",
      "        ByteType\n",
      "        IntegerType\n",
      "        LongType\n",
      "        ShortType\n",
      "    \n",
      "    class ArrayType(DataType)\n",
      "     |  ArrayType(elementType, containsNull=True)\n",
      "     |  \n",
      "     |  Array data type.\n",
      "     |  \n",
      "     |  :param elementType: :class:`DataType` of each element in the array.\n",
      "     |  :param containsNull: boolean, whether the array can contain null (None) values.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ArrayType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, elementType, containsNull=True)\n",
      "     |      >>> ArrayType(StringType()) == ArrayType(StringType(), True)\n",
      "     |      True\n",
      "     |      >>> ArrayType(StringType(), False) == ArrayType(StringType())\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BinaryType(AtomicType)\n",
      "     |  Binary (byte array) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BinaryType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class BooleanType(AtomicType)\n",
      "     |  Boolean data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      BooleanType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ByteType(IntegralType)\n",
      "     |  Byte data type, i.e. a signed integer in a single byte.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ByteType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DataType(builtins.object)\n",
      "     |  Base class for data types.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DateType(AtomicType)\n",
      "     |  Date (datetime.date) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DateType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fromInternal(self, v)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, d)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  EPOCH_ORDINAL = 719163\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DecimalType(FractionalType)\n",
      "     |  DecimalType(precision=10, scale=0)\n",
      "     |  \n",
      "     |  Decimal (decimal.Decimal) data type.\n",
      "     |  \n",
      "     |  The DecimalType must have fixed precision (the maximum total number of digits)\n",
      "     |  and scale (the number of digits on the right of dot). For example, (5, 2) can\n",
      "     |  support the value from [-999.99 to 999.99].\n",
      "     |  \n",
      "     |  The precision can be up to 38, the scale must be less or equal to precision.\n",
      "     |  \n",
      "     |  When create a DecimalType, the default precision and scale is (10, 0). When infer\n",
      "     |  schema from decimal.Decimal objects, it will be DecimalType(38, 18).\n",
      "     |  \n",
      "     |  :param precision: the maximum total number of digits (default: 10)\n",
      "     |  :param scale: the number of digits on right side of dot. (default: 0)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DecimalType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, precision=10, scale=0)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class DoubleType(FractionalType)\n",
      "     |  Double data type, representing double precision floats.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      DoubleType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class FloatType(FractionalType)\n",
      "     |  Float data type, representing single precision floats.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      FloatType\n",
      "     |      FractionalType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class IntegerType(IntegralType)\n",
      "     |  Int data type, i.e. a signed 32-bit integer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      IntegerType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class LongType(IntegralType)\n",
      "     |  Long data type, i.e. a signed 64-bit integer.\n",
      "     |  \n",
      "     |  If the values are beyond the range of [-9223372036854775808, 9223372036854775807],\n",
      "     |  please use :class:`DecimalType`.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      LongType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class MapType(DataType)\n",
      "     |  MapType(keyType, valueType, valueContainsNull=True)\n",
      "     |  \n",
      "     |  Map data type.\n",
      "     |  \n",
      "     |  :param keyType: :class:`DataType` of the keys in the map.\n",
      "     |  :param valueType: :class:`DataType` of the values in the map.\n",
      "     |  :param valueContainsNull: indicates whether values can contain null (None) values.\n",
      "     |  \n",
      "     |  Keys in a map data type are not allowed to be null (None).\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      MapType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, keyType, valueType, valueContainsNull=True)\n",
      "     |      >>> (MapType(StringType(), IntegerType())\n",
      "     |      ...        == MapType(StringType(), IntegerType(), True))\n",
      "     |      True\n",
      "     |      >>> (MapType(StringType(), IntegerType(), False)\n",
      "     |      ...        == MapType(StringType(), FloatType()))\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class NullType(DataType)\n",
      "     |  Null type.\n",
      "     |  \n",
      "     |  The data type representing None, used for the types that cannot be inferred.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      NullType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class ShortType(IntegralType)\n",
      "     |  Short data type, i.e. a signed 16-bit integer.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      ShortType\n",
      "     |      IntegralType\n",
      "     |      NumericType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes inherited from IntegralType:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StringType(AtomicType)\n",
      "     |  String data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StringType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StructField(DataType)\n",
      "     |  StructField(name, dataType, nullable=True, metadata=None)\n",
      "     |  \n",
      "     |  A field in :class:`StructType`.\n",
      "     |  \n",
      "     |  :param name: string, name of the field.\n",
      "     |  :param dataType: :class:`DataType` of the field.\n",
      "     |  :param nullable: boolean, whether the field can be null (None) or not.\n",
      "     |  :param metadata: a dict from string to simple type that can be toInternald to JSON automatically\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StructField\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, name, dataType, nullable=True, metadata=None)\n",
      "     |      >>> (StructField(\"f1\", StringType(), True)\n",
      "     |      ...      == StructField(\"f1\", StringType(), True))\n",
      "     |      True\n",
      "     |      >>> (StructField(\"f1\", StringType(), True)\n",
      "     |      ...      == StructField(\"f2\", StringType(), True))\n",
      "     |      False\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  typeName(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class StructType(DataType)\n",
      "     |  StructType(fields=None)\n",
      "     |  \n",
      "     |  Struct type, consisting of a list of :class:`StructField`.\n",
      "     |  \n",
      "     |  This is the data type representing a :class:`Row`.\n",
      "     |  \n",
      "     |  Iterating a :class:`StructType` will iterate its :class:`StructField`\\s.\n",
      "     |  A contained :class:`StructField` can be accessed by name or position.\n",
      "     |  \n",
      "     |  >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |  >>> struct1[\"f1\"]\n",
      "     |  StructField(f1,StringType,true)\n",
      "     |  >>> struct1[0]\n",
      "     |  StructField(f1,StringType,true)\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      StructType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __getitem__(self, key)\n",
      "     |      Access fields by name or slice.\n",
      "     |  \n",
      "     |  __init__(self, fields=None)\n",
      "     |      >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True),\n",
      "     |      ...     StructField(\"f2\", IntegerType(), False)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      False\n",
      "     |  \n",
      "     |  __iter__(self)\n",
      "     |      Iterate the fields\n",
      "     |  \n",
      "     |  __len__(self)\n",
      "     |      Return the number of fields.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  add(self, field, data_type=None, nullable=True, metadata=None)\n",
      "     |      Construct a StructType by adding new elements to it to define the schema. The method accepts\n",
      "     |      either:\n",
      "     |      \n",
      "     |          a) A single parameter which is a StructField object.\n",
      "     |          b) Between 2 and 4 parameters as (name, data_type, nullable (optional),\n",
      "     |             metadata(optional). The data_type parameter may be either a String or a\n",
      "     |             DataType object.\n",
      "     |      \n",
      "     |      >>> struct1 = StructType().add(\"f1\", StringType(), True).add(\"f2\", StringType(), True, None)\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True), \\\n",
      "     |      ...     StructField(\"f2\", StringType(), True, None)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType().add(StructField(\"f1\", StringType(), True))\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      >>> struct1 = StructType().add(\"f1\", \"string\", True)\n",
      "     |      >>> struct2 = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct1 == struct2\n",
      "     |      True\n",
      "     |      \n",
      "     |      :param field: Either the name of the field or a StructField object\n",
      "     |      :param data_type: If present, the DataType of the StructField to create\n",
      "     |      :param nullable: Whether the field to add should be nullable (default True)\n",
      "     |      :param metadata: Any additional metadata (default None)\n",
      "     |      :return: a new updated StructType\n",
      "     |  \n",
      "     |  fieldNames(self)\n",
      "     |      Returns all field names in a list.\n",
      "     |      \n",
      "     |      >>> struct = StructType([StructField(\"f1\", StringType(), True)])\n",
      "     |      >>> struct.fieldNames()\n",
      "     |      ['f1']\n",
      "     |  \n",
      "     |  fromInternal(self, obj)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  toInternal(self, obj)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods defined here:\n",
      "     |  \n",
      "     |  fromJson(json) from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class TimestampType(AtomicType)\n",
      "     |  Timestamp (datetime.datetime) data type.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      TimestampType\n",
      "     |      AtomicType\n",
      "     |      DataType\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  fromInternal(self, ts)\n",
      "     |      Converts an internal SQL object into a native Python object.\n",
      "     |  \n",
      "     |  needConversion(self)\n",
      "     |      Does this type need to conversion between Python object and internal SQL object.\n",
      "     |      \n",
      "     |      This is used to avoid the unnecessary conversion for ArrayType/MapType/StructType.\n",
      "     |  \n",
      "     |  toInternal(self, dt)\n",
      "     |      Converts a Python object into an internal SQL object.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  __metaclass__ = <class 'pyspark.sql.types.DataTypeSingleton'>\n",
      "     |      Metaclass for DataType\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from DataType:\n",
      "     |  \n",
      "     |  __eq__(self, other)\n",
      "     |      Return self==value.\n",
      "     |  \n",
      "     |  __hash__(self)\n",
      "     |      Return hash(self).\n",
      "     |  \n",
      "     |  __ne__(self, other)\n",
      "     |      Return self!=value.\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  json(self)\n",
      "     |  \n",
      "     |  jsonValue(self)\n",
      "     |  \n",
      "     |  simpleString(self)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Class methods inherited from DataType:\n",
      "     |  \n",
      "     |  typeName() from builtins.type\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from DataType:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DataType', 'NullType', 'StringType', 'BinaryType', 'Boolea...\n",
      "\n",
      "FILE\n",
      "    c:\\spark-2.4.7-bin-hadoop2.7\\python\\pyspark\\sql\\types.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "help(pyspark.sql.types)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### Troubleshooting.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_ecomm_full = (\n",
    "#     ss.read\n",
    "#     .format(\"csv\")\n",
    "#     .option(\"header\", True)\n",
    "#     .schema(schema_ecomm)\n",
    "#     .load(\"hdfs://192.168.93.128:9000/input/e-commerce/data.csv\")\n",
    "# )\n",
    "# df_ecomm_full.show(2)\n",
    "\n",
    "# from pyspark.sql.functions import expr\n",
    "\n",
    "# (\n",
    "#     df_ecomm_full\n",
    "#     .select(col(\"InvoiceDate\"))\n",
    "#     .withColumn(\n",
    "#         \"InvDate\",\n",
    "#         expr(\"to_timestamp('InvoiceDate', 'MM-dd-yyyy HH:mm')\"))\n",
    "#     .show(10)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructField(InvoiceNo,StringType,true)\n",
      "StructField(StockCode,StringType,true)\n",
      "StructField(Description,StringType,true)\n",
      "StructField(Quantity,IntegerType,true)\n",
      "StructField(InvoiceDate,TimestampType,true)\n",
      "StructField(UnitPrice,DoubleType,true)\n",
      "StructField(CustomerId,StringType,true)\n",
      "StructField(Country,StringType,true)\n"
     ]
    }
   ],
   "source": [
    "for s in df_ecomm_full.schema:\n",
    "    print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerId|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|       null|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|       null|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ecomm_full.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Orders by Country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 Unique Countries\n"
     ]
    }
   ],
   "source": [
    "\n",
    "count_country = df_ecomm_full.select(\"Country\").distinct().count()\n",
    "print(f\"{count_country} Unique Countries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Total Orders by Countries --\n",
      "+--------------+------------+\n",
      "|       Country|total_orders|\n",
      "+--------------+------------+\n",
      "|United Kingdom|      495478|\n",
      "|       Germany|        9495|\n",
      "+--------------+------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"-- Total Orders by Countries --\")\n",
    "\n",
    "df_ecomm_country_ordertotal = \\\n",
    "(\n",
    "    df_ecomm_full\n",
    "    .groupby(col(\"Country\"))\n",
    "    .count().withColumnRenamed(\"count\", \"total_orders\")\n",
    "    .sort(desc(\"count\"))\n",
    ")\n",
    "df_ecomm_country_ordertotal.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "\n",
    "##  Customers by highest orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|InvoiceDate|UnitPrice|CustomerId|       Country|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6| 2010-12-01|     2.55|     17850|United Kingdom|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6| 2010-12-01|     3.39|     17850|United Kingdom|\n",
      "+---------+---------+--------------------+--------+-----------+---------+----------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_ecomm_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerId|       total_order|\n",
      "+----------+------------------+\n",
      "|     14646| 280206.0199999998|\n",
      "|     18102|259657.29999999993|\n",
      "+----------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4338, None)"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecomm_customer_ordertotal = \\\n",
    "(\n",
    "    df_ecomm_full\n",
    "    # .filter(\"CustomerId IS NOT NULL AND Quantity > 0 AND UnitPrice > 0\")  --> SQL style, OR\n",
    "    .filter((col(\"CustomerId\").isNotNull()) & (col(\"Quantity\")>0) & (col(\"UnitPrice\")>0))\n",
    "    .withColumn(\"total_order\", df_ecomm_full.Quantity * df_ecomm_full.UnitPrice)\n",
    "    .groupby(\"CustomerId\")\n",
    "    .agg(sum(\"total_order\").alias(\"total_order\"))\n",
    "    .sort(desc(\"total_order\"))\n",
    ")\n",
    "\n",
    "df_ecomm_customer_ordertotal.count(), df_ecomm_customer_ordertotal.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df_ecomm_country_ordertotal\n",
    "    .coalesce(1)\n",
    "    .write.mode('overwrite')\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"hdfs://192.168.93.128:9000/output/e-commerce/country_totalorders\")\n",
    ")\n",
    "\n",
    "(\n",
    "    df_ecomm_customer_ordertotal\n",
    "    .coalesce(1)\n",
    "    .write.mode('overwrite')\n",
    "    .option(\"header\", True)\n",
    "    .csv(\"hdfs://192.168.93.128:9000/output/e-commerce/customer_totalorders\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### How may partitions did I coalesce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38, 200)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecomm_country_ordertotal.rdd.getNumPartitions(), df_ecomm_customer_ordertotal.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ss.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecomm = df_ecomm_full[[\"Country\", \"CustomerId\", \"Quantity\", \"UnitPrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(Country,StringType,true),StructField(CustomerId,StringType,true),StructField(Quantity,IntegerType,true),StructField(UnitPrice,DoubleType,true)))"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ecomm.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Project [unresolvedalias('Country, None), unresolvedalias('CustomerId, None), unresolvedalias('Quantity, None), unresolvedalias('UnitPrice, None)]\n",
      "+- Relation[InvoiceNo#2230,StockCode#2231,Description#2232,Quantity#2233,InvoiceDate#2234,UnitPrice#2235,CustomerId#2236,Country#2237] csv\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "Country: string, CustomerId: string, Quantity: int, UnitPrice: double\n",
      "Project [Country#2237, CustomerId#2236, Quantity#2233, UnitPrice#2235]\n",
      "+- Relation[InvoiceNo#2230,StockCode#2231,Description#2232,Quantity#2233,InvoiceDate#2234,UnitPrice#2235,CustomerId#2236,Country#2237] csv\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Project [Country#2237, CustomerId#2236, Quantity#2233, UnitPrice#2235]\n",
      "+- Relation[InvoiceNo#2230,StockCode#2231,Description#2232,Quantity#2233,InvoiceDate#2234,UnitPrice#2235,CustomerId#2236,Country#2237] csv\n",
      "\n",
      "== Physical Plan ==\n",
      "*(1) Project [Country#2237, CustomerId#2236, Quantity#2233, UnitPrice#2235]\n",
      "+- *(1) FileScan csv [Quantity#2233,UnitPrice#2235,CustomerId#2236,Country#2237] Batched: false, Format: CSV, Location: InMemoryFileIndex[hdfs://192.168.93.128:9000/input/e-commerce/data.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<Quantity:int,UnitPrice:double,CustomerId:string,Country:string>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "EXPLAIN PLAN\n",
    "\"\"\"\n",
    "\n",
    "df_ecomm.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
