{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kafka / Spark Streaming Assessment\n",
    "\n",
    "Following is my VINSYS id: bigdata10\n",
    "<br>I have attached the code path for each question. Please find it below.\n",
    "\n",
    "__Abstract__\n",
    "<br>The assessment is based on Spark + Kafka, basic Python, streaming of records, performing a real time analytics using On Premise Kafka and Spark.\n",
    "<br>Total 3 assessments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement a Kafka + Spark Integration.\n",
    "|Column|Type|Description|\n",
    "|:--|:--|:--|\n",
    "|order_id|Integer|random number, use random generator to generate id upto 100000|\n",
    "|item_id|String|random number, use random generator to generate id up to 100, convert to string|\n",
    "|price|Int|Random generator value from 1 to 50|\n",
    "|qty|Int|Random generator, 1 to 10|\n",
    "|state|String|User state, use USA statecodes, pick random state on every publish|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q2\n",
    "Implement a Spark Stream, that subscribes to data from Kafka topic `orders` [from Q1] and sums the amount [price * qty] based on state, on every `5 minutes` window interval.\n",
    "<br>The result shall be published to Kafka with (state, amount).\n",
    "<br>For example, if the customers ordered 10 items from NY and CO, with-in last 5 minutes, then we will sum all items purchased on NY [sum (item.qty * item.price)] and CO [sum (item.qty * item.price)]\n",
    "\n",
    "a. publish the consolidated output [State name, Amount] to Kafka topic [“statewise_earning”] in json format.\n",
    "\n",
    "b. Publish the consolidated output [State name, Amount] to Amazon RDS table using JDBC in append mode\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consume from `orders` topic.\n",
    "<br>Calculate aggregate `sum(qty * price)` grouped by `state` every `5 min` + print data to console.\n",
    "<br>Publish the aggregated values back to Kafka as JSON.\n",
    "\n",
    "```bash\n",
    "# Run consumer to listen on messages from `orders`\n",
    "$kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic orders\n",
    "\n",
    "\n",
    "# Create a topic called `statewise_earning` to post aggregated data\n",
    "$kafka-topics.sh  --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic statewise_earning\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"SparkStream:KafkaOrders\").getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Read from Kafka topic 'orders'\n",
    "\"\"\"\n",
    "\n",
    "df_orders = (\n",
    "    ss.readStream.format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.93.128:9092\")\n",
    "    .option(\"subscribe\", \"orders\")\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "root\n",
    " |-- key: binary (nullable = true)\n",
    " |-- value: binary (nullable = true)\n",
    " |-- topic: string (nullable = true)\n",
    " |-- partition: integer (nullable = true)\n",
    " |-- offset: long (nullable = true)\n",
    " |-- timestamp: timestamp (nullable = true)\n",
    " |-- timestampType: integer (nullable = true)\n",
    "\"\"\"\n",
    "\n",
    "df_orders.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "I need only 2 things: timestamp + value\n",
    "DeSerialize just the byte-stream value as only this is needed for further use.\n",
    "\"\"\"\n",
    "\n",
    "ordersJsonRawDf = df_orders.selectExpr(\"timestamp\", \"CAST(value AS STRING)\")\n",
    "ordersJsonRawDf.printSchema() # we get only value as string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, StringType, DoubleType\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "JSON is an object, Spark DF needs a schema\n",
    "\"\"\" \n",
    "schema = StructType(\n",
    "        [\n",
    "            StructField(\"order_id\", IntegerType(), True),\n",
    "            StructField(\"item_id\", StringType(), True),\n",
    "            StructField(\"price\", DoubleType(), True),\n",
    "            StructField(\"qty\", IntegerType(), True),\n",
    "            StructField(\"state\", StringType(), True),\n",
    "        ]\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Replace just a JSON-string with a JSON-object-with-schema\n",
    "Now, value is a single column, it contains a struct\n",
    "\"\"\"\n",
    "jsonDf = ordersJsonRawDf.withColumn(\"value\", F.from_json(\"value\", schema))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Extract JSON object elements as individual columns\n",
    "\"\"\"\n",
    "ordersDf = jsonDf.select(\"timestamp\", F.col(\"value.*\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Q: What are the state-wise sales for the last 300 seconds/5 minutes?\n",
    "\"\"\"\n",
    "\n",
    "ordersDf = ordersDf.withColumn(\"Amount\", F.col(\"qty\") * F.col(\"price\") )\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "I'm interested only in aggregating the sales by state for the last 5mins.\n",
    "\"\"\"\n",
    "windowedAmountSum_interim = (\n",
    "    ordersDf\n",
    "    .groupBy(\"state\", F.window(ordersDf.timestamp, \"5 minutes\", \"5 minutes\"))\n",
    "    .agg(F.sum(\"Amount\").alias(\"Amount\"))\n",
    ")\n",
    "\n",
    "windowedAmountSum = (\n",
    "    windowedAmountSum_interim\n",
    "    .selectExpr(\"to_json(struct(*)) AS value\")\n",
    "    .selectExpr(\"CAST(value as STRING)\")\n",
    ")\n",
    "\n",
    "# windowedAmountSum = (\n",
    "#     ordersDf\n",
    "#     .groupBy(\"state\", F.window(ordersDf.timestamp, \"300 seconds\", \"300 seconds\"))\n",
    "#     .agg(F.sum(\"Amount\").alias(\"Amount\"))\n",
    "#     .selectExpr(\"to_json(struct(*)) AS value\")\n",
    "#     .selectExpr(\"CAST(value as STRING)\")\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print to console + publish"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.StreamingQuery at 0x243376a03c8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Print to console to check what you'll be publishing next.\n",
    "\"\"\"\n",
    "echoOnconsole = (\n",
    "    windowedAmountSum\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")\n",
    "    .format(\"console\")\n",
    "    .start() # start the query. spark will subscribe to this data\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Publish the aggregated results to a new topic `statewise_earning` to be consumed by other consumers.\n",
    "\"\"\"\n",
    "(\n",
    "    windowedAmountSum\n",
    "    .writeStream\n",
    "    .format(\"kafka\")\n",
    "    .outputMode(\"complete\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.93.128:9092\")\n",
    "    .option(\"topic\", \"statewise_earning\")\n",
    "    .option(\"checkpointLocation\", \"file:///c:/spark/temp\")\n",
    "    .start()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Write/append to RDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure this is already run in PostgreSQL:\n",
    "```sql\n",
    "CREATE TABLE IF NOT EXISTS statewise_earning(state varchar(100), Amount real);\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windowed_report_df = (\n",
    "#     windowedAmountSum\n",
    "#     .withWatermark(\"delivered_time\", \"24 hours\")\n",
    "#     .groupBy('source_id', window('delivered_time', '5 minute'))\n",
    "#     .count()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowed_state_amount = windowedAmountSum_interim.selectExpr(\"state\", \"Amount\")\n",
    "\n",
    "\n",
    "def sparkstream_to_postgres(df, epoch_id):\n",
    "    (\n",
    "        df.write\n",
    "        .mode('append')\n",
    "        .format('jdbc')\n",
    "        .option(\"url\", \"jdbc:postgresql://bond-psql.ckzprcrersc2.us-east-2.rds.amazonaws.com:5432/productdb\")\n",
    "        .option(\"driver\", \"org.postgresql.Driver\")\n",
    "        .option(\"dbtable\", \"statewise_earning\")\n",
    "        .option(\"user\", \"postgres\")\n",
    "        .option(\"password\", \"AwCT$May21\")\n",
    "        .save()\n",
    "    )\n",
    "\n",
    "(\n",
    "    windowed_state_amount.writeStream\n",
    "    .foreachBatch(sparkstream_to_postgres)\n",
    "    .option(\"checkpointLocation\", 'file:///c:/spark/rds')\n",
    "    .outputMode('update')\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Straight saving stream-data DOESN'T WORK\n",
    "\n",
    "(\n",
    "    windowedAmountSum\n",
    "    .select(\"state\", \"Amount\")\n",
    "    .writeStream\n",
    "    .format(\"jdbc\")\n",
    "    .option(\"url\", \"jdbc:postgresql://bond-psql.ckzprcrersc2.us-east-2.rds.amazonaws.com:5432/productdb\")\n",
    "    .option(\"driver\", \"org.postgresql.Driver\")\n",
    "    .option(\"dbtable\", \"statewise_earning\")\n",
    "    .option(\"user\", \"postgres\")\n",
    "    .option(\"password\", \"AwCT$May21\")\n",
    "    .outputMode(\"append\")\n",
    "    .start()\n",
    "    .awaitTermination()\n",
    ")\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
