{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notebooks/Json.ipynb\n",
    "<br>https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notebooks/Kakfa-Invoice-Producer.ipynb\n",
    "<br>https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notebooks/Spark-Invoice-Kakfa-Stream.ipynb\n",
    "<br>https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notebooks/Kakfa-Invoice-Consumer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" DOESN't WORK as of now\n",
    "Load kafka-spark driver to consume kafka messages.\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = \"\"\n",
    "\"\"\"\n",
    "\n",
    "\"\"\" IMPORTANT\n",
    "# Load kafka spark driver, to receive kafka stream messages\n",
    "# https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/spark-setup-for-packages.md\n",
    "\n",
    "# $KAFKA_HOME/bin/kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n",
    "# $KAFKA_HOME/bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test\n",
    "\"\"\"\n",
    "\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = (\n",
    "    SparkSession.builder.master(\"local[*]\")\n",
    "    .appName(\"sparkSTREAM:InvoicesfromKAFKA\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "SETUP: ensure Kafka Zookeeper/Broker/Producer are running\n",
    "\n",
    "Read from Kafka.\n",
    "Here, spark is the Consumer for kafka topic 'test'.\n",
    "Spark streaming works as DataFrame/SQL\n",
    "\"\"\"\n",
    "\n",
    "topic = \"invoices\"\n",
    "\n",
    "df_kafka = (\n",
    "    ss.readStream\n",
    "    .format(\"kafka\")\n",
    "    .option(\"kafka.bootstrap.servers\", \"192.168.93.128:9092\")\n",
    "    .option(\"subscribe\", topic)\n",
    "    .load()\n",
    ")\n",
    "# df_lines = (\n",
    "#     ss.readStream\n",
    "#     .format(\"socket\")\n",
    "#     .option(\"host\", \"192.168.93.128\")\n",
    "#     .option(\"port\", 9999)\n",
    "#     .load()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- key: binary (nullable = true)\n",
      " |-- value: binary (nullable = true)\n",
      " |-- topic: string (nullable = true)\n",
      " |-- partition: integer (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- timestampType: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n# key: kafka key, in binary format\\n# value: kafka value, in binary format\\n# topic: string\\n# parition: integer\\n# offset: long \\n# timestamp: longint in ms\\n# timestampType: Source Time, Record write time\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    ".show()/print won't work as it's streaming data (not batch data)\n",
    ".printSchema() works\n",
    "\"\"\"\n",
    "\n",
    "df_kafka.printSchema()\n",
    "\n",
    "\"\"\"\n",
    "# key: kafka key, in binary format\n",
    "# value: kafka value, in binary format\n",
    "# topic: string\n",
    "# parition: integer\n",
    "# offset: long \n",
    "# timestamp: longint in ms\n",
    "# timestampType: Source Time, Record write time\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Convert kafka value (arrives as bytes) -> STRING; ignore the key for now..\n",
    "Interested only in 'value' from the stream..\n",
    "\"\"\"\n",
    "df_lines = df_kafka.selectExpr(\"CAST(value AS STRING)\")\n",
    "df_lines.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Split lines into words.\n",
    "Convert words into individual spark records using explode (after splitting on the delimiter \" \")\n",
    ".explode() will convert invidual words into separate spark records\n",
    "\"\"\"\n",
    "import pyspark.sql.functions as f\n",
    "\n",
    "df_words = df_lines.select(f.explode(f.split(df_lines.value, \" \")).alias(\"word\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_word_count = df_words.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "PRINT schema on console.\n",
    "\"\"\"\n",
    "\n",
    "echoOnConsole = (\n",
    "    df_words\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")  # options: append/complete\n",
    "    .format(\"console\")\n",
    "    .start()  # Starts the query, and spark will subscribe to stream data\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"CHECK JUPYTER CONSOLE\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FINALLY: Await termination (before exiting this notebook)\n",
    "\"\"\"\n",
    "\n",
    "# echoOnConsole.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
