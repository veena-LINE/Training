{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "REQUIREMENT:\n",
    "    Refactor SparkMovieData notebook \n",
    "      1. ratings.csv, movies.csv should be loaded from hadoop\n",
    "      2. Spark Session using SparkConf, use 4 executor core, \n",
    "                                         max 4 excutor core , use Spark Cluster\n",
    "      3. Write the result to hdfs \n",
    "            mostPopularMoviesDf.write.mode('overwrite')\\\n",
    "                              .csv(\"hdfs:....../output/top-movies.csv\")\n",
    "\n",
    "My Understanding:\n",
    "    Read from HDFS\n",
    "\n",
    "    spark-config\n",
    "    4 executor cores\n",
    "    4 max cores\n",
    "\n",
    "    Write back to HDFS\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x1f419fb7c88>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"\n",
    "Since Spark 2.x, Spark unified Spark APIs, DF, Datasets, & SQL.\n",
    "SparkSession uses SparkContext internally.\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "\n",
    "config = SparkConf()\n",
    "config.setMaster(\"spark://192.168.11.77:7077\").setAppName(\"MOVIELENSonCLUSTER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Configure before creating SparkSession\n",
    "\"\"\"\n",
    "\n",
    "conf = \\\n",
    "(\n",
    "    config\n",
    "    .set(\"spark.executor.memory\", \"2g\")\n",
    "    .set(\"spark.executor.cores\", 4)\n",
    "    .set(\"spark.cores.max\", 4)\n",
    "    .set(\"spark.driver.memory\", \"2g\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-BTFVSHG:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://192.168.11.77:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>MOVIELENSonCLUSTER</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1f41b83ba48>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Read CSV from HDFS\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, StringType, LongType\n",
    "\n",
    "schema_movies = (\n",
    "    StructType()\n",
    "    .add(\"movieId\", IntegerType(), True)\n",
    "    .add(\"title\", StringType(), True)\n",
    "    .add(\"genres\", StringType(), True)\n",
    ")\n",
    "schema_ratings = (\n",
    "    StructType()\n",
    "    .add(\"userId\", IntegerType(), True)\n",
    "    .add(\"movieId\", IntegerType(), True)\n",
    "    .add(\"rating\", DoubleType(), True)\n",
    "    .add(\"timestamp\", LongType(), True)\n",
    ")\n",
    "\n",
    "df_movies_full = (\n",
    "    ss.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema_movies)\n",
    "    .load(\"hdfs://192.168.93.128:9000/input/movie_lens/movies.csv\")\n",
    ")\n",
    "\n",
    "df_ratings_full = (\n",
    "    ss.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .schema(schema_ratings)\n",
    "    .load(\"hdfs://192.168.93.128:9000/input/movie_lens/ratings.csv\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Row(movieId=1, title='Toy Story (1995)', genres='Adventure|Animation|Children|Comedy|Fantasy'),\n",
       " Row(userId=1, movieId=1, rating=4.0, timestamp=964982703))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies_full.first(), df_ratings_full.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(StructType(List(StructField(movieId,IntegerType,true),StructField(title,StringType,true),StructField(genres,StringType,true))),\n",
       " StructType(List(StructField(userId,IntegerType,true),StructField(movieId,IntegerType,true),StructField(rating,DoubleType,true),StructField(timestamp,LongType,true))))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_movies_full.schema, df_ratings_full.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|movieId|avg(rating)|\n",
      "+-------+-----------+\n",
      "| 157775|        5.0|\n",
      "|   8911|        5.0|\n",
      "+-------+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sorted averaged ratings\n",
    "\"\"\"\n",
    "\n",
    "df_ratings_avg = \\\n",
    "df_ratings_full.groupBy(\"movieId\").mean().sort(\"avg(rating)\", ascending=False)[[\"movieId\", \"avg(rating)\"]]\n",
    "\n",
    "df_ratings_avg.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fetch top 10 first, then join with movies\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "top = 100000\n",
    "df_ratings_top = df_ratings_avg.limit(top) --> Will reduce the dataset into 1 partition (outputs a single HDFS file)\n",
    "\"\"\"\n",
    "df_ratings_top = df_ratings_avg\n",
    "\n",
    "\n",
    "df_top10 = df_ratings_top.join(df_movies_full, on=\"movieId\", how=\"inner\")\n",
    "df_top10 = df_top10.withColumnRenamed(\"avg(rating)\", \"rating_average\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n.option(\"header\", True)\\ndf_top10.write.mode(\\'overwrite\\').csv(\"hdfs://192.168.93.128:9000/output/all-movies\")\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top10.write.mode('overwrite').csv(\"hdfs://192.168.93.128:9000/output/all-movies\")\n",
    "\n",
    "\"\"\"\n",
    "df_top10.write.mode('overwrite').optoin(\"header\", True).csv(\"hdfs://192.168.93.128:9000/output/all-movies\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_top10.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Each of those partitions above writes a separate file in HDFS\n",
    "BAD Solution: .repartition(1) --> performance overhead + loses sorting\n",
    "\"\"\"\n",
    "\n",
    "df_top10.repartition(1).write.mode('overwrite').csv(\"hdfs://192.168.93.128:9000/output/all-movies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Each of those partitions above writes a separate file in HDFS\n",
    "BETTER Solution: .coalesce() --> may reduce shuffling, but cannot totally stop shuffling\n",
    "\n",
    "Coalesce helps in repartitioning, but with minimal reshuffling than .repartition(n)\n",
    "Coalesce will try and reduce shuffling that usually happens while repartitioning.\n",
    "\"\"\"\n",
    "\n",
    "df_top10.coalesce().write.mode('overwrite').csv(\"hdfs://192.168.93.128:9000/output/all-movies\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
