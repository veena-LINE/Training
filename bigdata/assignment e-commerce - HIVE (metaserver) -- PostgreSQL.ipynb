{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirement\n",
    "\n",
    "```\n",
    "1. Ensure use the spark cluster\n",
    "2. Read Ecommerce data from hadoop as csv\n",
    "3. Create a database \"ecommercedb\" in hive / SQL/PySpark  if it is not exist\n",
    "4. Save the ecommerce data as sparkTable [parquet]\n",
    "5. Compare the size difference between csv and parquest in the HDFS browser http://192.168.93.128:50070\n",
    "6. Using Hive meta data url\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# \"\"\"\n",
    "# Since Spark 2.x, Spark unified Spark APIs, DF, Datasets, & SQL.\n",
    "# SparkSession uses SparkContext internally.\n",
    "# \"\"\"\n",
    "\n",
    "# from pyspark.conf import SparkConf\n",
    "\n",
    "# config = SparkConf()\n",
    "# config.setMaster(\"spark://192.168.11.77:7077\").setAppName(\"E-COMMERCE:CLUSTER\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# winutils.exe  chmod 777  C:\\spark-temp\n",
    "\n",
    "from pyspark.conf import SparkConf\n",
    "conf = SparkConf()\n",
    "\n",
    "\n",
    "# config.setMaster(\"spark://192.168.11.71:7077\").setAppName(\"HiveApp\")\n",
    "(\n",
    "    conf.setMaster(\"local[1]\").setAppName(\"ecommerce->HIVE\")\n",
    "    .set(\"spark.executor.memory\", \"4G\")\n",
    "    .set(\"spark.executor.cores\", 2)\n",
    "    .set(\"spark.cores.max\", 2)\n",
    "    .set(\"spark.driver.memory\", \"4G\")\n",
    "\n",
    "    .set(\"hive.metastore.uris\", \"thrift://192.168.93.128:9083\")\n",
    "\n",
    "    # .set(\"spark.sql.warehouse.dir\", \"/home/ubuntu/spark-warehose\")\n",
    "    .set(\"spark.sql.warehouse.dir\", \"hdfs://192.168.93.128:9000/user/hive/warehouse\")  # spark warehouse\n",
    "\n",
    "    # .set(\"hive.metastore.warehouse.dir\", \"hdfs://192.168.93.128:9000/user/hive/warehouse\")  # ?\n",
    "    # .set(\"spark.local.dir\", \"c:/spark-temp\")\n",
    ")\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession, HiveContext\n",
    "ss = (\n",
    "    SparkSession.builder\n",
    "    .config(conf=conf)\n",
    "    .enableHiveSupport()\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "ss = SparkSession.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Read e-commerce data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Read CSV from HDFS\n",
    "\"\"\"\n",
    "\n",
    "import datetime as dt\n",
    "from pyspark.sql.types import StructType, IntegerType, DoubleType, StringType, DateType\n",
    "from pyspark.sql.functions import col, asc, desc, count, sum, avg, to_date, to_timestamp\n",
    "\n",
    "schema_ecomm = (\n",
    "    StructType()\n",
    "    .add(\"InvoiceNo\", StringType(), True)\n",
    "    .add(\"StockCode\", StringType(), True)\n",
    "    .add(\"Description\", StringType(), True)\n",
    "    .add(\"Quantity\", IntegerType(), True)\n",
    "    .add(\"InvoiceDate\", DateType(), True)\n",
    "    .add(\"UnitPrice\", DoubleType(), True)\n",
    "    .add(\"CustomerId\", StringType(), True)\n",
    "    .add(\"Country\", StringType(), True)\n",
    ")\n",
    "\n",
    "df_ecomm_full = (\n",
    "    ss.read\n",
    "    .format(\"csv\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"dateFormat\", \"MM/dd/yyyy HH:mm\")\n",
    "    .schema(schema_ecomm)\n",
    "    .load(\"hdfs://192.168.93.128:9000/input/e-commerce/data.csv\")\n",
    ")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DROP un-necessary data/columns\n",
    ".drop('column_name', 'column_name')\n",
    "\"\"\"\n",
    "df_ecomm_full = df_ecomm_full[[\"Country\", \"CustomerId\", \"Quantity\", \"UnitPrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecomm_full.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"CREATE DATABASE IF NOT EXISTS ecommercedb_pg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss.sql(\"USE ecommercedb_pg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|  databaseName|\n",
      "+--------------+\n",
      "|       default|\n",
      "|ecommercedb_pg|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Write to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     df_ecomm_full\n",
    "#     .coalesce(1)\n",
    "#     .write.mode('overwrite')\n",
    "#     .option(\"header\", True)\n",
    "#     .csv(\"hdfs://192.168.93.128:9000/output/e-commerce/ecommerce\")\n",
    "# )\n",
    "df_ecomm_full.coalesce(1).write.mode(\"overwrite\").saveAsTable(\"ecommerce\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+--------+---------+\n",
      "|       Country|CustomerId|Quantity|UnitPrice|\n",
      "+--------------+----------+--------+---------+\n",
      "|United Kingdom|     17850|       6|     2.55|\n",
      "|United Kingdom|     17850|       6|     3.39|\n",
      "+--------------+----------+--------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ss.sql(\"SELECT * FROM ecommercedb_pg.ecommerce\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecomm_full.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "df_ecomm_full.groupby(\"Country\").sum(\"Quantity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "ss.sql(\"\"\"\n",
    "SELECT country, SUM(quantity)\n",
    "  FROM ecommercedb.ecommerce\n",
    " GROUP BY 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devanshu's code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = spark.read.format(\"csv\") \\\n",
    "                .option(\"header\", True) \\\n",
    "                .schema(schema) \\\n",
    "                .option(\"dateFormat\", \"MM/dd/yyyy HH:mm\")\\\n",
    "                .load(\"hdfs://192.168.93.128:9000/ecommerce/data.csv\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS ecommercedb\")\n",
    "\n",
    "dataSet.write.parquet(\"sparkTable.parquet\")\n",
    "\n",
    "parquetFile = spark.read.parquet(\"sparkTable.parquet\")\n",
    "\n",
    "parquetFile.write.mode('overwrite')\\\n",
    "                    .parquet(\"hdfs://192.168.93.128:9000/ecommerce/sparkTable.parquet\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# DELETE BELOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### How may partitions did I coalesce?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecomm = df_ecomm_full[[\"Country\", \"CustomerId\", \"Quantity\", \"UnitPrice\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "EXPLAIN PLAN\n",
    "\"\"\"\n",
    "\n",
    "df_ecomm.explain(True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
