{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"root\"></root>\n",
    "__| [DAY 1](#day1)__\n",
    "Python (`List`, `Tuple`, `Dictionary`)<br>\n",
    "__| [DAY 2](#day2)__\n",
    "Python (`while..else`, `Strings`, String `formatting` (History), `Classes`)<br>\n",
    "__| [DAY 3](#day3)__\n",
    "Python Class variables (`static`), Class `Inheritance`, Scopes, `Set`, `Exceptions`, `DateTime`, `Math`, `Random`_<br>\n",
    "__| [DAY 4](#day4)__\n",
    "File Handling (with `context manager`), List `Comprehension`, `Spark` Installation on Windows<br>\n",
    "__| [DAY 5](#day5)__\n",
    "`RDD`s, Hadoop `MapReduce`, `MapReduce vs Spark`, Running with `spark-submit`, Linux `basic command-line commands`<br>\n",
    "__| [DAY 6](#day6)__\n",
    "JDK+spark+Hadoop Installation on Ubuntu via SSH!<br>\n",
    "__| [DAY 7](#day7)__\n",
    "Reconfig `core-site.xml` & `hdfs-site.xml` (enable read/write from/to HDFS from pyspark), Python-Spark-Jupyter Integration (working from notebook)<br>\n",
    "__| [DAY 8](#day8)__\n",
    "`YARN` Fundamentals, Driver Executor Jobs Stages Tasks, RDD Partitioning<br>\n",
    "__| [DAY 9](#day9)__\n",
    "Broadcast, Accumulator, Caching/Persisting<br>\n",
    "__| [DAY 10](#day10)__\n",
    "Hive, Databricks community (account)<br>\n",
    "__| [DAY 11](#day11)__\n",
    "Spark DataFrames (and its APIs)<br>\n",
    "__| [DAY 12](#day12)__\n",
    "Spark Cluster (Create Master | Workers | assign Workers to Driver), `spark-config`<br>\n",
    "__| [DAY 13](#day13)__\n",
    "RDD DF JOINs<br>\n",
    "__| [DAY 14](#day14)__\n",
    "Spark SQL<br>\n",
    "__| [DAY 15](#day15)__\n",
    ">Running Hive's `meta-server`<br>\n",
    ">Interface PostgreSQL with Hive<br>\n",
    ">Running ThriftServer (for web/jdbc/odbc interfaces)<br>\n",
    ">Connect hive using JDBC interface<br>\n",
    "\n",
    "__| [DAY 16](#day16)__\n",
    "`AWS EC2` | `AWS RDS` (PostgreSQL)<br>\n",
    "__| [DAY 17](#day17)__\n",
    "`Apache Kafka` (Zookeeper, Broker, Producer, Consumer) | RDS (PostgreSQL)<br>\n",
    "__| [DAY 18](#day18)__\n",
    ">`TOPIC` with multiple-partitions<br>\n",
    ">pub-sub with multiple partitions (run with/out key)<br>\n",
    ">`OFFSET`<br>\n",
    ">`TOPIC` (create/delete/alter)<br>\n",
    ">`Kafka-Python` (ConsumerGroup)<br>\n",
    "\n",
    "__| [DAY 19](#day19)__\n",
    "Kafka stream simulated Invoices | Spark Consume Invoices <br>\n",
    "__| [DAY 20](#day20)__\n",
    "S3 | boto3 + Databricks<br>\n",
    "__| [DAY 21](#day21)__\n",
    "AWS RDS | AWS Glue overview<br>\n",
    "__| [DAY 22](#day22)__\n",
    "How AWS Glue Works | Why VPC endpoint } assignment<br>\n",
    "__| [DAY 23](#day23)__\n",
    "abc<br>\n",
    "__| [DAY 24](#day24)__\n",
    "AWS Lambda | Lambda functions <br>\n",
    "__| [DAY 25](#day25)__\n",
    "Connecting Kafka to AWS <br>\n",
    "__| [References](#references)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day1\">[DAY 1](#root)</a>\n",
    "\n",
    "## List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del numbers[-1]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 40]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.remove(10)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 30]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numbers.pop())\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.insert(0, 10)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## TUPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ('python',)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "d['a'] = 1\n",
    "d['b'] = 2\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.clear()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## FUNCTION `*args`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome .veena ..veenas ...veenaas\n"
     ]
    }
   ],
   "source": [
    "def special_greeting(message, *names):\n",
    "    print(message, *names)\n",
    "    \n",
    "special_greeting('Welcome', '.veena', '..veenas' , '...veenaas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome veena veenas veenaas\n"
     ]
    }
   ],
   "source": [
    "def special_greeting(message, *names):\n",
    "    print(message, *names)\n",
    "    \n",
    "special_greeting('Welcome', *['veena', 'veenas' , 'veenaas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day2\">[DAY 2](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `while`..`else`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Final value of i = 6\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "while i < 6:\n",
    "    print(i)\n",
    "    i += 1\n",
    "else:\n",
    "    print(f\"Final value of i = {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `while`, `for` loops have `else` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_multiline = \"\"\"string\"\"\"\n",
    "type(s_multiline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In Windows systems, strings on new line always end in `\\r\\n`.\n",
    "<br>In Linux systems, strings always end only in `\\n`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\s = space\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\s = space\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t = tab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t = tab\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 Line 2 Line 3\n",
      "\\ = line continuation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Line 1 \\\n",
    "Line 2 \\\n",
    "Line 3\\n\\\n",
    "\\\\ = line continuation\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\tto\tPython\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome\\tto\\tPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second `\\t` always adds extra space.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode strings - TO BE DISCUSSED LATER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unicode strings - TO BE DISCUSSED LATER\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Earthlingss    \n",
      "False False\n",
      "1\n",
      "2 -1\n",
      "2\n",
      "earthlingss\n",
      "earthlingss\n"
     ]
    }
   ],
   "source": [
    "s = \" earthlingss    \"\n",
    "print(s.title())\n",
    "print(s.startswith('Ea'), s.startswith('ea'))\n",
    "print(s.count('a'))\n",
    "\n",
    "# User find over index (.index() errors if match not found while .find() returns -1 when not found)\n",
    "print(s.find('a'), s.find('H'))\n",
    "print(s.index('a'))\n",
    "\n",
    "print(s.lstrip().rstrip())\n",
    "\n",
    "print(s.lstrip().rstrip().zfill(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('123'.zfill(10), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _String formatting (History)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veena is 18 years old.\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old, and she can vote!\n",
      "Veena is 18 years old, and she can vote!\n"
     ]
    }
   ],
   "source": [
    "name, age = 'Veena', 18\n",
    "\n",
    "print(\"%s is %d years old.\" % (name, age))\n",
    "print(\"{} is {} years old. As she's {} years old, she can vote!\".format(name, age, age))\n",
    "print(\"{0} is {1} years old. As she's {1} years old, she can vote!\".format(name, age))\n",
    "print(\"{name} is {age} years old. As she's {age} years old, she can vote!\".format(name=name, age=age))\n",
    "print(f\"{name} is {age} years old, and she can vote!\")\n",
    "print(F\"{name} is {age} years old, and she can vote!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe is 20 years old, and he can vote!\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Joe'} is {age+2} years old, and he can vote!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know just enough about Classes and Objects in Python, but I'll still try something!\n"
     ]
    }
   ],
   "source": [
    "print(\"I know just enough about Classes and Objects in Python, but I'll still try something!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    \"\"\"Simple Car Model\"\"\"\n",
    "    __slots__ = ('brand', 'model', 'year', 'color')\n",
    "    \n",
    "    def __init__(self, brand, model, year, color):\n",
    "        self.brand = brand\n",
    "        self.model = model\n",
    "        self.year = year\n",
    "        self.color = color\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.brand}/{self.model}/{self.year}/{self.color}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} {self.__str__()!r}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honda/Civic/2014/Black\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Car 'Honda/Civic/2014/Black'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honda = Car('Honda', 'Civic', 2014, 'Black')\n",
    "\n",
    "print(honda)\n",
    "honda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Order is (Amount:200, Discount:10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Order (Amount:200, Discount:10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Order:\n",
    "    \"\"\"Simple Order!\"\"\"\n",
    "    \n",
    "    def __init__(self, amount, discount=0):\n",
    "        self.amount = amount\n",
    "        self.discount = discount\n",
    "        \n",
    "    def calculate(self):\n",
    "        return self.amount - (self.amount)*(self.discount/100)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Your {self.__class__.__name__} is (Amount:{self.amount}, Discount:{self.discount})\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} (Amount:{self.amount}, Discount:{self.discount})\"\n",
    "    \n",
    "order = Order(200, 10)\n",
    "print(order)\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Order is (Amount:500, Discount:10) Grand Total: 450.0\n",
      "Your Order is (Amount:1000, Discount:20) Grand Total: 800.0\n",
      "Your Order is (Amount:100, Discount:5) Grand Total: 95.0\n"
     ]
    }
   ],
   "source": [
    "orders = [(500, 10), (1000, 20), (100, 5)]\n",
    "\n",
    "for order in orders:\n",
    "    o = Order(*order)\n",
    "    print(o, f\"Grand Total: {o.calculate()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day3\">[DAY 3](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class variables (static variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Cognizant, Balance: 140,000,000,000,000\n",
      "Welcome! Tesla, Balance: 1,000,000,000,000,000,000,000\n",
      "\n",
      "CapitalOne, You have 2 loyal customers!\n"
     ]
    }
   ],
   "source": [
    "class CapitalOne:\n",
    "    \"\"\" Static variables (other languages) <==> Class variablea (Python)\n",
    "    \"\"\"\n",
    "    _customers = 0\n",
    "    __slots__ = ('__name', '__balance')\n",
    "    \n",
    "    def __init__(self, name, balance):\n",
    "        self.__name = name\n",
    "        self.__balance = balance\n",
    "        self._customer_add()\n",
    "        \n",
    "    def _customer_add(self):\n",
    "        CapitalOne._customers += 1\n",
    "        \n",
    "    @classmethod\n",
    "    def customers(cls):\n",
    "        return f\"{cls.__name__}, You have {cls._customers} loyal customer{'s' if cls._customers > 1 else ''}!\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.info()} {self.__name}, Balance: {self.__balance:,}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} (Customer: {self.__name}, Balance: {self.__balance:,})\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def info():\n",
    "        \"\"\"staticmethod is a class method that doesn't need a cls reference\"\"\"\n",
    "        return \"Welcome!\"\n",
    "\n",
    "\n",
    "cognizant = CapitalOne('Cognizant', 140_000_000_000_000)\n",
    "tesla = CapitalOne('Tesla', 1000_000_000_000_000_000_000)\n",
    "\n",
    "print(cognizant)\n",
    "print(tesla)\n",
    "print()\n",
    "print(CapitalOne.customers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Class Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Cognizant, Balance: 140,000,000,000,000\n",
      "Welcome! Tesla, Balance: 1,000,000,000,000,000,000,000\n",
      "\n",
      "BankOfAmerica, You have 2 loyal customers!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Welcome!, Tesla,\n",
       "          You're with Bank Of America,\n",
       "          Your balance: 1,000,000,000,000,000,000,000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Bank:\n",
    "    \"\"\"Banking Base class\n",
    "    \"\"\"\n",
    "    name = \"Set your name\"\n",
    "    address = \"Set your address\"\n",
    "    currency = \"Set your currency\"\n",
    "    \n",
    "    def __init__(self, name, address, currency):\n",
    "        Bank.name = name\n",
    "        Bank.address = address\n",
    "        Bank.currency = currency\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{Bank.name}/{Bank.address}/{Bank.currency}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    \n",
    "    \n",
    "class BankOfAmerica(Bank):\n",
    "    \"\"\" Static variables (other languages) <==> Class variablea (Python)\n",
    "    \"\"\"\n",
    "    _customers = 0\n",
    "    __slots__ = ('__name', '__balance')\n",
    "    \n",
    "    \n",
    "    def __init__(self, name, balance):\n",
    "        super().__init__(\"Bank Of America\",\n",
    "                         \"Colony 1, USA 12345\",\n",
    "                         \"$\")\n",
    "        self.__name = name\n",
    "        self.__balance = balance\n",
    "        self._customer_add()\n",
    "        \n",
    "    @classmethod\n",
    "    def _customer_add(cls):\n",
    "        cls._customers += 1\n",
    "        \n",
    "    @classmethod\n",
    "    def customers(cls):\n",
    "        return f\"{cls.__name__}, You have {cls._customers} loyal customer{'s' if cls._customers > 1 else ''}!\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.info()} {self.__name}, Balance: {self.__balance:,}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"{self.info()}, {self.__name},\n",
    "          You're with {super().name},\n",
    "          Your balance: {self.__balance:,})\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def info():\n",
    "        \"\"\"staticmethod is a class method that doesn't need a cls reference\n",
    "        \"\"\"\n",
    "        return \"Welcome!\"\n",
    "\n",
    "\n",
    "cognizant = BankOfAmerica('Cognizant', 140_000_000_000_000)\n",
    "tesla = BankOfAmerica('Tesla', 1000_000_000_000_000_000_000)\n",
    "\n",
    "print(cognizant)\n",
    "print(tesla)\n",
    "print()\n",
    "print(BankOfAmerica.customers())\n",
    "tesla\n",
    "\n",
    "\n",
    "# del BankOfAmerica\n",
    "# del Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isinstance()`, `issubclass()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "isinstance(tesla, BankOfAmerica) ? True\n",
      "isinstance(tesla, Bank) ? True\n",
      "\n",
      "issubclass(BankOfAmerica, Bank) ? True\n",
      "issubclass(Bank, BankOfAmerica) ? False\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f\"isinstance(tesla, BankOfAmerica) ? {isinstance(tesla, BankOfAmerica)}\")\n",
    "print(f\"isinstance(tesla, Bank) ? {isinstance(tesla, Bank)}\")\n",
    "print()\n",
    "print(f\"issubclass(BankOfAmerica, Bank) ? {issubclass(BankOfAmerica, Bank)}\")\n",
    "print(f\"issubclass(Bank, BankOfAmerica) ? {issubclass(Bank, BankOfAmerica)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global\n",
      "changed in local\n"
     ]
    }
   ],
   "source": [
    "# globals\n",
    "name = \"global\"\n",
    "\n",
    "def change_scope():\n",
    "    global name\n",
    "    name = \"changed in local\"\n",
    "\n",
    "\n",
    "print(name)\n",
    "change_scope()    \n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = {'Python', 'SQL', 'Java', 'JS', 'Perl'}\n",
    "\n",
    "language.add('NoSQL')\n",
    "language.remove('Python')  # Errors, if not found\n",
    "language.discard('JS') # Silent return, if item not found\n",
    "\n",
    "language.clear()  # Empty the Set\n",
    "language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Oh no!\n",
      "Cleaning up!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(1/1)\n",
    "except:\n",
    "    print(\"Catch me if you can!\")\n",
    "else:\n",
    "    print(\"Oh no!\")\n",
    "finally:\n",
    "    print(\"Cleaning up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catch me if you can!\n",
      "Cleaning up!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(1/0)\n",
    "except:\n",
    "    print(\"Catch me if you can!\")\n",
    "else:\n",
    "    print(\"Oh no!\")\n",
    "finally:\n",
    "    print(\"Cleaning up!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<attribute 'args' of 'BaseException' objects>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raise TypeError(\"I'm trying something new\")\n",
    "except TypeError:\n",
    "    print(TypeError.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic Exception\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raise Exception(\"I'm not being specific!\")\n",
    "except Exception:\n",
    "    print(\"Generic Exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catch me if you can! <class '__main__.CustomError'>\n"
     ]
    }
   ],
   "source": [
    "class CustomError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "try:\n",
    "#     raise\n",
    "    raise CustomError(\"Catch me if you can!\")\n",
    "except CustomError as e:\n",
    "    print(e, type(e))\n",
    "except Exception as e:\n",
    "    print(e, type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-30 23:53:35.183373\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "today = dt.datetime.now()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Math, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2335600547204243\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(random.random())\n",
    "\n",
    "\n",
    "del random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day4\">[DAY 4](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is A88C-3222\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\veena\\data\n",
      "\n",
      "04/30/2021  09:02 PM    <DIR>          .\n",
      "04/30/2021  09:02 PM    <DIR>          ..\n",
      "04/30/2021  09:02 PM            38,049 all_us_counties.csv\n",
      "04/30/2021  09:00 PM               656 all_us_states.csv\n",
      "04/30/2021  09:02 PM         2,072,181 all_us_zipcodes.csv\n",
      "               3 File(s)      2,110,886 bytes\n",
      "               2 Dir(s)  493,276,196,864 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[State: abbr = name, State: AL = Alabama, State: AK = Alaska, State: AZ = Arizona, State: AR = Arkansas, State: CA = California, State: CO = Colorado, State: CT = Connecticut, State: DE = Delaware, State: DC = District of Columbia, State: FL = Florida, State: GA = Georgia, State: HI = Hawaii, State: ID = Idaho, State: IL = Illinois, State: IN = Indiana, State: IA = Iowa, State: KS = Kansas, State: KY = Kentucky, State: LA = Louisiana, State: ME = Maine, State: MD = Maryland, State: MA = Massachusetts, State: MI = Michigan, State: MN = Minnesota, State: MS = Mississippi, State: MO = Missouri, State: MT = Montana, State: NE = Nebraska, State: NV = Nevada, State: NH = New Hampshire, State: NJ = New Jersey, State: NM = New Mexico, State: NY = New York, State: NC = North Carolina, State: ND = North Dakota, State: OH = Ohio, State: OK = Oklahoma, State: OR = Oregon, State: PA = Pennsylvania, State: RI = Rhode Island, State: SC = South Carolina, State: SD = South Dakota, State: TN = Tennessee, State: TX = Texas, State: UT = Utah, State: VT = Vermont, State: VA = Virginia, State: WA = Washington, State: WV = West Virginia, State: WI = Wisconsin, State: WY = Wyoming]\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    def __init__(self, code, name):\n",
    "        self.code = code\n",
    "        self.name = name\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.code} = {self.name}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}: {self.__str__()}\"\n",
    "\n",
    "\n",
    "states = []\n",
    "with open(\"data/all_us_states.csv\") as f:\n",
    "    states.extend(State(*state_info.strip().split(',')) for state_info in f)\n",
    "    # .readlines() reads all of file into memory. NO! Simply iterate over file handle.\n",
    "\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds = [o for o in range(10) if o%2 if o>4]\n",
    "odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Spark\n",
    "\n",
    "__Requirements:__\n",
    ">`Java 1.8 only` (a.k.a Java 8) (as Spark works only on 1.8)\n",
    "<br>Oracle Java (stable, performant, no support for unpaid version)\n",
    "<br>Open source community (`AdaptJava JDK 8`) https://adoptopenjdk.net/\n",
    "<br>- JDK (compiler+JRE)\n",
    "<br>- JRE (no compiler, just a runtime environment)\n",
    "<br>- Language |Statically-typed Compiled language\n",
    "\n",
    ">`Spark 2.x only`\n",
    "<br>Supports: Java, Scala (largely writen in Java), Python, R, SQL\n",
    "\n",
    ">`Hadoop`\n",
    "<br>* HDFS/Hive/YARN/Kafka\n",
    "\n",
    "Look out for the installation of the above (in order) with necessary system settings on Windows below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Installation (in dependency order):__\n",
    ">1. Install `python 3.7` only (as Spark2.x works best with Python 3.7)\n",
    "<br>To install 3.7 on conda:\n",
    "<br>`$conda create -n py37 anaconda=2020.07 python=3.7`\n",
    "<br>`$conda activate py37`\n",
    "<br>`$conda deactivate`\n",
    "\n",
    ">2. Install `AdaptJDK 8` (check `JAVA_HOME` (system variable) with `echo %JAVA_HOME%`, else set)\n",
    "\n",
    ">3. Install `Spark 2.7 only` (google spark download)\n",
    "<br>Set `SPARK_HOME` (system variable) to the spark installation directory\n",
    "<br>Set `Path` (user/system variable) to Spark's bin directory\n",
    "<br>Launch `spark-shell` (allow access if prompted)\n",
    "<br>- `$spark-shell`\n",
    "<br>Launch `pyspark`\n",
    "<br>- `$pyspark`\n",
    "<br>pyspark will require winutils.\n",
    "\n",
    ">4. Install `winutils` (google `github winutils`) (https://github.com/steveloughran/winutils)\n",
    "<br>Extract Hadoop2.7.x folder\n",
    "<br>Set `HADOOP_HOME` to this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day5\">[DAY 5](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecosystem compatibility:\n",
    "<br>Spark 2.4\n",
    "<br>JDK 8 (1.8)\n",
    "<br>Python 3.7\n",
    "<br>Scala 2.11\n",
    "<br>Hadoop 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spark:__\n",
    "<br>Apache Spark is a general-purpose, distributed, fast engine for large-scale data-processing.\n",
    "\n",
    "\n",
    "__Spark Core Ideas:__\n",
    "<br>1. Data Processing Framework\n",
    "<br>2. Expressive computing system (not limited to MapReduce model)\n",
    "<br>3. Facilitates faster memory (no saving disks + caching)\n",
    "<br>In-memory processing (instead of writing to disks) [DDR-Double Data Rate]\n",
    "\n",
    "__Apache Spark__\n",
    "<br>Runs on JVM.\n",
    "<br>Written using Scala (Scala is written entirely in Java)\n",
    "<br>Faster adaption into Big Data world.\n",
    "<br>Supports Python, Java, R, Scala, SQL.\n",
    "\n",
    "Spark is more developer focused (not focused on end-user query processing engines)\n",
    "<br>(insert picture here)\n",
    "<br>Java + JVM Knowledge required. Having `Scala` knowledge is grand.\n",
    "\n",
    "__Hadoop MapReduce__\n",
    "<br>Offers `Parallelization`\n",
    "<br>`Scalable`\n",
    "<br>`Fault-tolerant` (via redundancy)\n",
    "<br>A programming model for distributed proessing of large datasets.\n",
    "<br>basic unit of information is a (key, value) pair\n",
    "<br>Map-function + Reduce-function with Shuffling in-between\n",
    "<br>Mapper brings structure to unstructured data\n",
    "<br>Shuffler consolidates output from Mapper (moved the data from Mapper, and passes it to Reducer)\n",
    "<br>Reducer outputs summarized values from Shuffler\n",
    "<br>Hadoop's MappReduce is SLOW because there's lot of I/O operations & no in-memory processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext (sc)\n",
    "\n",
    "\n",
    "__`$ pyspark`__\n",
    "\n",
    "__`>>> sc`__\n",
    "<br><SparkContext master=local[*] appName=PySparkShell>\n",
    "\n",
    "__`>>> spark`__\n",
    "<br><pyspark.sql.session.SparkSession object at 0x000001BBF7C76648>\n",
    "\n",
    "`Ctrl+l` to clear the screen\n",
    "\n",
    "__`>>> data = [10, 15, 20, 25, 30, 35, 40, 45, 50]`__\n",
    "\n",
    "__`>>> type(data)`__\n",
    "<br><class 'list'>\n",
    "\n",
    "__`>>> data_rdd = sc.parallelize(data)`__\n",
    "<br># This will load data into a Spark partition._\n",
    "\n",
    "__`>>> data_rdd.count`__\n",
    "<br>data_rdd.count(               data_rdd.countApprox(         data_rdd.countApproxDistinct( data_rdd.countByKey(          data_rdd.countByValue(\n",
    "\n",
    "__`>>> data_rdd.count()`__\n",
    "<br>9\n",
    "\n",
    "__`>>>  type(data_rdd)`__\n",
    "<br><class 'pyspark.rdd.RDD'>\n",
    "\n",
    "__`>>> data_rdd.filter(lambda x: x*x)`__\n",
    "<br>PythonRDD[2] at RDD at PythonRDD.scala:53\n",
    "\n",
    "__`>>> data_squares = data_rdd.filter(lambda x: x * x)`   # .filter() is a Transformation (not an Action)__\n",
    "\n",
    "__`>>> data_squares`__\n",
    "<br>PythonRDD[3] at RDD at PythonRDD.scala:53\n",
    "\n",
    "__`>>> data_squares.collect()`  # .collect(), .sum() are all Actions (not Transformation)__\n",
    "<br>[10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "__`>>> data_squares.sum()`__\n",
    "<br>270\n",
    "\n",
    "__`>>> data_`__\n",
    "<br>data_rdd     data_squares\n",
    "\n",
    "__`>>> data_rdd`__\n",
    "<br>ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195  # .scala shows that data_rdd is an internal Java object\n",
    "\n",
    "__`>>> data_100times = data_rdd.map(lambda x: x * 100)`__\n",
    "\n",
    "__`>>> data_100times.collect()`__\n",
    "<br>[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "\n",
    "__`>>> type(data_100times)`__\n",
    "<br><class 'pyspark.rdd.PipelinedRDD'>\n",
    "\n",
    "__`>>> result = data_100times.collect()`__\n",
    "\n",
    "__`>>> type(result)`__\n",
    "<br><class 'list'>\n",
    "\n",
    "__`>>> def odds(n):`\n",
    "<br>...     `return n % 2`\n",
    "<br>...__\n",
    "\n",
    "__`>>> data_odds = data_rdd.filter(odds)`__\n",
    "\n",
    "__`>>> data_odds.collect()`__\n",
    "<br>[15, 25, 35, 45]\n",
    "\n",
    "__`>>> sc.`__\n",
    "```python\n",
    "sc.PACKAGE_EXTENSIONS   sc.cancelAllJobs(       sc.getOrCreate(         sc.pythonExec           sc.setLocalProperty(    sc.textFile(\n",
    "sc.accumulator(         sc.cancelJobGroup(      sc.hadoopFile(          sc.pythonVer            sc.setLogLevel(         sc.uiWebUrl\n",
    "sc.addFile(             sc.defaultMinPartitions sc.hadoopRDD(           sc.range(               sc.setSystemProperty(   sc.union(\n",
    "sc.addPyFile(           sc.defaultParallelism   sc.master               sc.runJob(              sc.show_profiles(       sc.version\n",
    "sc.appName              sc.dump_profiles(       sc.newAPIHadoopFile(    sc.sequenceFile(        sc.sparkHome            sc.wholeTextFiles(\n",
    "sc.applicationId        sc.emptyRDD(            sc.newAPIHadoopRDD(     sc.serializer           sc.sparkUser(\n",
    "sc.binaryFiles(         sc.environment          sc.parallelize(         sc.setCheckpointDir(    sc.startTime\n",
    "sc.binaryRecords(       sc.getConf(             sc.pickleFile(          sc.setJobDescription(   sc.statusTracker(\n",
    "sc.broadcast(           sc.getLocalProperty(    sc.profiler_collector   sc.setJobGroup(         sc.stop(\n",
    "```\n",
    "__`>>> exit()`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>To run a custom spark file from the command-line:\n",
    "<br>__`$ spark-submit.py spark_hello.py`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is A88C-3222\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\veena\\bigdata\n",
      "\n",
      "05/04/2021  12:21 AM    <DIR>          .\n",
      "05/04/2021  12:21 AM    <DIR>          ..\n",
      "05/04/2021  12:28 AM               292 spark_hello.py\n",
      "               1 File(s)            292 bytes\n",
      "               2 Dir(s)  490,375,802,880 bytes free\n"
     ]
    }
   ],
   "source": [
    "!cat ./bigdata/spark_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: Reading Text File\n",
    "`sc.textFile()`  # __TRANSFORMATION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: What's in the Text File?\n",
    "`rdd.collect()`  # __ACTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: rdd.flatMap()\n",
    "```python\n",
    "counts =\n",
    "data_rdd.flatMap(lambda line: line.split(\" \")).\\   # __TRANSFORMATION__\n",
    " map(lambda word: (word, 1)).\\\n",
    " reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "<br># Each .map().reduceByKey() will operate on its own chunk of data.\n",
    "\n",
    "__rdd.map()__:\n",
    "<br>Returns a new RDD by applying the function to each element of the RDD.\n",
    "<br>Function in map returns only one item.\n",
    "                                             \n",
    "__rdd.flatMap()__:\n",
    "<br>Similar to .map(), returns a new RDD by applying a function to each element of the RDD, but the output is flattened.\n",
    "<br>This means, .flatMap() gets rid of any structure within the RDD.\n",
    "\n",
    "`counts.take(4)`  # Similar to DataFrame.head(4)  # __ACTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark: rdd.saveAsTextFile()\n",
    "counts.saveAsTextFile(\"state_count.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day6\">[DAY 6](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S p a r k - H a d o o p    O n   U b u n t u\n",
    "\n",
    "<br>\n",
    "\n",
    "### Install OpenJDK\n",
    "`$sudo apt install openjdk-8-jdk -y`\n",
    "\n",
    "__Where's Java installed?__\n",
    "<br>`/usr/bin/jvm/java-8-openjdk-amd64`\n",
    "\n",
    "__Set /etc/environment:__\n",
    "<br>__JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64__\n",
    "<br>__JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64__\n",
    "<br>with `$sudo nano /etc/environment`\n",
    "\n",
    "\n",
    "### Install Spark\n",
    "`$wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz`\n",
    "<br>`tar xf spark-2.4.7-bin-hadoop2.7.tgz`\n",
    "<br>`sudo mv spark-2.4.7-bin-hadoop2.7 /opt`  # `/opt` is the Optional Packages Directory on Linux\n",
    "<br>`sudo chmod 777 /opt/spark-2.4.7-bin-hadoop2.7`  # In production environment, Read+Execute permission to User ONLY!\n",
    "\n",
    "Set /etc/environment:\n",
    "<br>__SPARK_HOME=/opt/spark-2.4.7-bin-hadoop2.7__\n",
    "\n",
    "### Setup PATH for user:\n",
    "```bash\n",
    "cd ~\n",
    "nano .profile  # Similar to User-Profile on Windows\n",
    "# Paste:\n",
    "export SPARK_HOME=/opt/spark-2.4.7-bin-hadoop2.7\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "source .profile\n",
    "```\n",
    "\n",
    "__note__ Spark directory includes a YARN, and this will conflict with Hadoop's YARN.\n",
    "<br>To run YARN command, use $YARN_HOME/bin/yarn instead of plain YARN command.\n",
    "\n",
    "__Install Python__\n",
    "<br>`sudo add-apt-repository ppa:deadsnakes/ppa`  -- Makes repositories/builds available for python installation\n",
    "<br>`sudo apt-get update`  -- Now, update the repositores\n",
    "<br>`sudo apt-get install python3.7`\n",
    "\n",
    "I'm creating my own alias for python3.7 in ~/.bash_aliases:\n",
    "<br>`alias python36=python3`\n",
    "<br>`alias python37=python3.7`\n",
    "<br>`$source .bashrc`\n",
    "\n",
    "\n",
    "### To enable Spark to run pyspark..\n",
    "`sudo ln -s /usr/bin/python3.7 /usr/bin/python`  -- Soft link\n",
    "\n",
    "### Reboot Ubuntu [This is for changes in /etc/environment file to take effect, needs restart]\n",
    "`sudo reboot`\n",
    "<br>`connect via SSH (if using SSH)`\n",
    "<br>Test with `$pyspark`, or\n",
    "<br>Test with `$spark-shell`\n",
    "\n",
    "\n",
    "### Install Hadoop\n",
    "__Download Hadoop Binaries__\n",
    "```bash\n",
    "wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz\n",
    "tar xf hadoop-2.7.7.tar.gz\n",
    "sudo mv hadoop-2.7.7 /opt\n",
    "```\n",
    "\n",
    "Assign read/write/execute permission for complete hadoop folder. [not recommended for production]\n",
    "```bash\n",
    "sudo chmod 777 /opt/hadoop-2.7.7\n",
    "sudo nano /etc/environment\n",
    "# Paste in /etc/environment:\n",
    "HADOOP_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_INSTALL=/opt/hadoop-2.7.7\n",
    "HADOOP_MAPRED_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_COMMON_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_HDFS_HOME=/opt/hadoop-2.7.7\n",
    "YARN_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_COMMON_LIB_NATIVE_DIR=/opt/hadoop-2.7.7/lib/native\n",
    "```\n",
    "\n",
    "Update profile (per user) environment (no `sudo` here)\n",
    "```bash\n",
    "nano ~/.profile\n",
    "# Paste below at the end of the file:\n",
    "export HADOOP_HOME=/opt/hadoop-2.7.7\n",
    "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n",
    "# Close existing terminal and open new terminal.\n",
    "```\n",
    "\n",
    "<br>__Backup original Hadoop config files, mapred-site.xml won't be there by default, ignore the error__\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml.original`\n",
    "\n",
    "<br>Truncate the existing content in the conf file [for easy editing, optional]:\n",
    "```bash\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "```\n",
    "\n",
    "<br>__Edit config files:__\n",
    "<br>__-- core-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/core-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value> \n",
    "    </property>\n",
    "  \n",
    "    <property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/data/hdfs</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>hadoop.proxyuser.hive.hosts</name>\n",
    "        <value>*</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>hadoop.proxyuser.hive.groups</name>\n",
    "        <value>*</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- hdfs-site.xml--__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml`\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/hdfs-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "    \n",
    "    <property>\n",
    "        <name>dfs.datanode.use.datanode.hostname</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- mapred-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/mapred-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- yarn-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/yarn-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "        <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>yarn.log-aggregation-enable</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "__Restart the linux system once__\n",
    "\n",
    "For running hadoop cluster with ssh, as hadoop user needs ssh permission.\n",
    "\n",
    "Install ssh server if not there\n",
    "`sudo apt install openssh-server` -- installation\n",
    "`sudo systemctl status ssh` -- Check SSH status\n",
    "`sudo systemctl enable ssh` -- enable server if status is down\n",
    "`sudo systemctl start ssh` -- start server\n",
    "`sudo ufw allow ssh` -- Ubuntu Firewall (ufw)\n",
    "`sudo systemctl status ssh` -- Re-check\n",
    "\n",
    "\n",
    "___Hadoop internally uses SSH for communication. So enable Hadoop to use SSH. So, SSH again from within Ubuntu (to white-list ubuntu to be used in Hadoop):___\n",
    "\n",
    "Test if all's ok:\n",
    "<br>`ssh username@ip_address`  -- White-listing Ubuntu to be used in Hadoop\n",
    "<br>`ssh-keygen`  -- To enable password-less authentication\n",
    "<br>`ssh-keyscan localhost,0.0.0.0 > ~/.ssh/known_hosts`\n",
    "<br>`chmod +x $HADOOP_HOME/sbin/start-all.sh`\n",
    "\n",
    "__Prepare HDFS data directories:__\n",
    "<br>`sudo mkdir -p /data/hdfs`\n",
    "<br>`sudo chmod 777 /data/hdfs`\n",
    "\n",
    "__Format NameNode:__\n",
    "<br>`hdfs namenode -format`\n",
    "\n",
    "__Now, start all Hadoop services:__\n",
    "<br>`$HADOOP_HOME/sbin/start-all.sh` -- Note: Spark also has start-all.sh\n",
    "<br>`$HADOOP_HOME/sbin/stop-all.sh`  -- To shutdown (Spark also has one by the same name)\n",
    "\n",
    "__Check with browser:__\n",
    "<br>http://hostname:50070, or\n",
    "<br>http://localhost:50070, or\n",
    "<br>http://localhost:50070/explorer.html#/\n",
    "\n",
    "\n",
    "__YARN__\n",
    "<br>http://localhost:8088/cluster\n",
    "\n",
    "Default port references https://kontext.tech/column/hadoop/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn\n",
    "\n",
    "__Hadoop Commmands__\n",
    "<br>`jps`  -- Similar to Task Manager on Windows (jps = Java Processes)\n",
    "<br>`hdfs dfs -ls /` -- List all HDFS directories \n",
    "<br>`hdfs dfs -ls -R /*.csv`\n",
    "\n",
    "__NOTE__\n",
    "```bash\n",
    "# If shart-all.sh doesn't work normally\n",
    "$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave\n",
    "```\n",
    "\n",
    "__Hadoop Utility commands__\n",
    "<br>`hdfs dfs -du /directory_name` -- Disk usage\n",
    "<br>`hdfs dfs -dus /directory_name` -- Total size of file/directory\n",
    "<br>`hdfs dfs -stat /directory_name` -- Get last modified information\n",
    "<br>`hdfs dfs -stat /path_to_filename` -- Get last modified information\n",
    "\n",
    "<br>`hdfs/bin -mkdir /user`\n",
    "<br>`hdfs/bin -mkdir /user/username`\n",
    "\n",
    "Write the username of your computer. Example:\n",
    "<br>bin/hdfs dfs -mkdir /geeks\n",
    "<br>bin/hdfs dfs -mkdir geeks2\n",
    "\n",
    "Examples:To run sample pi application\n",
    "\n",
    "Number of Maps = 4 Samples per Map = 4\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 4 4\n",
    "\n",
    "<br>To list out all the applications\n",
    "```bash\n",
    "yarn application -list -appStates ALL\n",
    "works only when HA enabled, we will discuss in later sessions\n",
    "\n",
    "yarn rmadmin -checkHealth\n",
    "To get application ID use yarn application -list\n",
    "\n",
    "yarn application -status application_XXXYYYZZZKKKK_0002\n",
    "To view logs of application,\n",
    "\n",
    "yarn logs -applicationId application_XXXYYYZZZKKKK_0002\n",
    "yarn application -kill application_XXXYYYZZZKKKK_0002\n",
    "yarn application -list\n",
    "\n",
    "yarn application -list -appStates FINISHED\n",
    "yarn application -list -appStates ALL\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "more https://docs.cloudera.com/runtime/7.0.0/yarn-monitoring-clusters-applications/topics/yarn-use-cli-view-logs-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day7\">[Day 7](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit `$HADOOP_HOME/etc/hadoop/core-site.xml` (set `fs.defaultFS` property to `hdfs://192.168.93.128:9000`)\n",
    "<br>This will enable reading/writing of `hdfs://hostip:port/hdfs_paths` from within python.\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://192.168.93.128:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "<name>hadoop.tmp.dir</name>\n",
    "<value>/data/hdfs</value>\n",
    "</property>\n",
    "\n",
    "  <property>\n",
    "    <name>hadoop.proxyuser.hive.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hive.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "<br>\n",
    "\n",
    "### Edit `$HADOOP_HOME/etc/hadoop/hdfs-site.xml` (set `dfs.permissions` property to `false`)\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Python, Spark, Jupyter\n",
    "__2 options:__\n",
    "<br>1. Initialize $SPARK_HOME/findspark library  (NOTE: Not for Production!)\n",
    "<br>`$pip install findspark`\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "....\n",
    "```\n",
    "<br>2. Configure ENV setting enabling pyspark to launch Jupyter\n",
    "<br>__?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day8\">[Day 8](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yarn fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs, Stages, Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## RDD Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[1]\", \"US-ZIPCODES\")\n",
    "\n",
    "rdd_us_zipcodes = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_zipcodes.csv\")\n",
    "\n",
    "# How many partitions currently?\n",
    "print(f\"Partitions : {rdd_us_zipcodes.getNumPartitions()}\")\n",
    "\n",
    "## One way to scale up/down partitioning/chunking data\n",
    "rdd_us_zipcodes = rdd_us_zipcodes.repartition(4)\n",
    "print(f\"Partitions : {rdd_us_zipcodes.getNumPartitions()}\")\n",
    "\n",
    "# Print partition data\n",
    "def show_data(partition_name):\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(element for i, element in enumerate(partition_name) if i <= 5)\n",
    "rdd_mappedPartition = rdd_us_zipcodes.mapPartitions(show_data)\n",
    "rdd_mappedPartition.collect()\n",
    "\n",
    "rdd_header = rdd_us_zipcodes.first()\n",
    "rdd_zipcodes = (\n",
    "    rdd_us_zipcodes\n",
    "    .filter(lambda l: l != rdd_header)\n",
    "    .map(lambda l: tuple(l.split(\",\"))\n",
    "        )\n",
    "\n",
    "## Another way of partitioning data\n",
    "rdd_zipcodes_partitions = rdd_zipcodes.partitionBy(7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day9\">[Day 9](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast, Accumulator, Caching/Persisting\n",
    "\n",
    "[Code Files](./bigdata)<br>\n",
    "--> PySpark RDD APIs.ipynb)<br>\n",
    "--> PySpark Broadcasting (basics).ipynb<br>\n",
    "--> PySpark Accumulator (basics).ipynb<br>\n",
    "--> PySpark Caching (basics).ipynb<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day10\">[Day 10](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive, Databricks community (account)\n",
    "\n",
    "Databricks - maintains Spark\n",
    "Databricks - works with AWS, Azure.\n",
    "\n",
    "Databricks - Community Edition\n",
    "Create a cluster (valid for 2Hrs once idle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive commands\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "\n",
    "wget https://mirrors.estointernet.in/apache/hive/hive-2.3.8/apache-hive-2.3.8-bin.tar.gz\n",
    "\n",
    "tar xf apache-hive-2.3.8-bin.tar.gz\n",
    "sudo mv apache-hive-2.3.8-bin /opt/apache-hive-2.3.8\n",
    "\n",
    "sudo chmod 777 /opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "```bash\n",
    "# paste HIVE_HOME \n",
    "sudo nano /etc/environment \n",
    "HIVE_HOME=/opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Now, export after exiting from file\n",
    "export HIVE_HOME=/opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "cp $HIVE_HOME/conf/hive-site.xml $HIVE_HOME/conf/hive-site.xml.bak\n",
    "\n",
    "wget -P $HIVE_HOME/conf https://raw.githubusercontent.com/nodesense/cts-aws-spark-april-2021/main/hive-site.xml\n",
    "\n",
    "ls $HIVE_HOME/conf\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/schematool -initSchema -dbType derby\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "# You should see these stats:\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.8/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true\n",
    "Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver\n",
    "Metastore connection User:       APP\n",
    "Starting metastore schema initialization to 2.3.0\n",
    "Initialization script hive-schema-2.3.0.derby.sql\n",
    "Initialization script completed\n",
    "schemaTool completed\n",
    "```\n",
    "\n",
    "```bash\n",
    "# NOTE: Stay in the same directory as above!\n",
    "$HIVE_HOME/bin/hive\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "hive> show tables;\n",
    "\n",
    "# to create database, you have two options, create database/schema\n",
    "hive> SHOW DATABASES;\n",
    "hive > CREATE DATABASE ordersdb;\n",
    "hive> SHOW DATABASES;\n",
    "# Create tables in the new DB\n",
    "hive? USER ordersdb;\n",
    "\n",
    "hive> CREATE TABLE IF NOT EXISTS brands(id INT, name STRING);\n",
    "hive> INSERT INTO brands(id,name) values(1, 'Apple');\n",
    "hive> SELECT * from brands;\n",
    "hive>  DROP TABLE BRANDS;\n",
    "hive >  DROP DATABASE ordersdb;\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "create table test(id int ,name string ) clustered by (id) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');\n",
    "\n",
    "INSERT INTO test(id,name) values(1, 'Apple');\n",
    " SELECT * from test;\n",
    "\n",
    "update test set name='Google' where id=1;\n",
    "\n",
    "SELECT * from test;\n",
    "delete from test;\n",
    "truncate table test;\n",
    "SELECT * from test; \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive help\n",
    "\n",
    "```bash\n",
    "\n",
    "# remove meta data store\n",
    "cd $HIVE_HOME\n",
    "rm -rf metastore_db/\n",
    "\n",
    "\n",
    "# Create a new one\n",
    "$HIVE_HOME/bin/schematool -initSchema -dbType derby\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All about Hive\n",
    "\n",
    "Hive - a Datawarehousing solution over a MapReduce framework\n",
    "<br>Hive doesn't use Spark, instead Spark uses Hive\n",
    "\n",
    "Hive Key priciples:\n",
    "1. SQL like HQL (HiveQL) - not compatible with ANSI SQL.\n",
    "2. Allows programmers to plugin custom Mappers and Reducers (Extension)\n",
    "3. Data Warehouse Infrastructure\n",
    "4. Provides tools to enable easy ETL\n",
    "\n",
    "Hive Data Model: Data in HIve organized into..\n",
    "<br>Tables\n",
    "<br>Partitions\n",
    "<br>Buckets\n",
    "\n",
    "\n",
    "NOTE: Look into External Tables support for Hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive Tables\n",
    "2 types of tables:\n",
    "<br>1. Managed Tables\n",
    "<br>2. External Tables\n",
    "\n",
    "#### Managed Tables / Internal Tables\n",
    "Table ops/ data in HDFS/ lifecycle(CRUD ops)) - Managed via Hive\n",
    "<br>We cannot directly use data managed by Hive. It has to be only done via CLI/JDBC/ODBC via HQL\n",
    "\n",
    "#### [External Tables](https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notes/may-10-2021-hive-tables.md)\n",
    "Only tables (metadata) can be managed by Hive. Everything else (data/ lifecycle ops) are managed externally (developers/ other systems)\n",
    "<br>Data can also be stored in AWS S3 buckets (insted of HDFS) These external locations are supported by Drivers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hive  External Tables](https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/hive-external-table.md)\n",
    "\n",
    "```bash\n",
    "echo \"1,jane,sales\" > employees1.csv\n",
    "echo \"2,john,marketting\" >> employees1.csv\n",
    "\n",
    "echo \"3,will,account\" > employees2.csv\n",
    "echo \"4,smith,qa\" >> employees2.csv\n",
    "\n",
    "hdfs dfs -mkdir /employees\n",
    "hdfs dfs -chmod 777 /employees\n",
    "\n",
    "hdfs dfs -put employees1.csv /employees\n",
    "hdfs dfs -put employees2.csv /employees\n",
    "\n",
    "# Launch Hive ($HIVE_HOME/bin/hive)\n",
    "\n",
    "# This staging/intermediate table is called a `Layer` in DATALAKE terminology.\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS employees_ext(\n",
    "  employee_id INT, \n",
    "  name STRING, \n",
    "  dept STRING\n",
    "  )\n",
    "  COMMENT 'Employee Names'\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ','\n",
    "  STORED AS TEXTFILE\n",
    "  LOCATION '/employees';\n",
    "\n",
    "\n",
    "SELECT * FROM employees_ext;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS employees(\n",
    "  employee_id INT, \n",
    "  name STRING, \n",
    "  dept STRING\n",
    "  )\n",
    "  COMMENT 'employees names managed';\n",
    "\n",
    "INSERT OVERWRITE TABLE employees SELECT * FROM employees_ext;\n",
    "\n",
    "SELECT * from employees; \n",
    "DROP TABLE employees_ext;\n",
    "SELECT * from employees;   \n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    " insert into employees_ext (employee_id, name, dept) values(8,'krish','sales');\n",
    "```\n",
    "\n",
    "```\n",
    " hdfs dfs -ls /employees\n",
    "  hdfs dfs -cat /employees/000000_0\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "insert into employees_ext (employee_id, name, dept) values(9,'nila','sales');\n",
    "\n",
    "```\n",
    "```\n",
    "hdfs dfs -ls /employees\n",
    "\n",
    "you may notice /employees/000000_0_copy_1 created \n",
    "\n",
    "hdfs dfs -cat /employees/000000_0\n",
    "hdfs dfs -cat /employees/000000_0_copy_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable JDBC Interface for Hive (to enable python and others to connect to Hive)\n",
    "```bash\n",
    "$ cd HIVE_HOME/bin\n",
    "$ beeline -u jdbc:/hive2://\n",
    "# -u (URL)\n",
    "```\n",
    " \n",
    "_Connect to any DB using `beeline`._\n",
    "<br>___NOTE: This will not run Thrift Server. It only uses a Thrift Server library.___\n",
    "\n",
    "\n",
    "```bash\n",
    "$ beeline --service metastore\n",
    "# This will start ThriftServer\n",
    "\n",
    "$ jps\n",
    "# Look for `RunJar` entry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day11\">[Day 11](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes (and its APIs)\n",
    "\n",
    "PySpark DataFrames (Code in individual Jupyter notebooks)\n",
    "<br>PySpark SparkSession with Context\n",
    "<br>PySpark DataFrame with Custom Schema\n",
    "<br>PySpark DataFrame from CSV\n",
    "<br>PySpark RDD DataFrame from CSV\n",
    "<br>PySpark Read JSON\n",
    "<br>PySpark RDD's DataFrames and its APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day12\">[Day 12](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster (Create Master, Workers, and assign Cluster Master to the Driver)\n",
    "<br><br>\n",
    "___Create Spark Cluster Master___\n",
    "```bash\n",
    "# Open command prompt, run cluster master a.k.a cluster manager\n",
    "$ spark-class org.apache.spark.deploy.master.Master\n",
    "# This will start spark cluster manager. In Spark, it's termed Spark Master\n",
    "# Look for the URL in the terminal, ex: 192.168.11.77:7077\n",
    "\n",
    "21/05/12 18:45:51 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\n",
    "21/05/12 18:45:51 INFO Master: Starting Spark master at spark://192.168.11.77:7077\n",
    "21/05/12 18:45:51 INFO Master: Running Spark version 2.4.7\n",
    "21/05/12 18:45:51 INFO Utils: Successfully started service 'MasterUI' on port 8080.\n",
    "21/05/12 18:45:51 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8080\n",
    "21/05/12 18:45:51 INFO Master: I have been elected leader! New state: ALIVE\n",
    "21/05/12 18:47:19 INFO Master: 192.168.11.77:63058 got disassociated, removing it.\n",
    "```\n",
    "Now check, http://localhost:8080/\n",
    "\n",
    "<br><br>\n",
    "___Create Worker 1___\n",
    "<br>Open another (separate) py37 terminal:\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "\n",
    "21/05/12 18:54:53 INFO Utils: Successfully started service 'sparkWorker' on port 63142.\n",
    "21/05/12 18:54:53 INFO Worker: Starting Spark worker 192.168.11.77:63142 with 8 cores, 23.0 GB RAM\n",
    "21/05/12 18:54:53 INFO Worker: Running Spark version 2.4.7\n",
    "21/05/12 18:54:53 INFO Worker: Spark home: C:\\spark-2.4.7-bin-hadoop2.7\n",
    "21/05/12 18:54:53 INFO Utils: Successfully started service 'WorkerUI' on port 8081.\n",
    "21/05/12 18:54:53 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8081\n",
    "21/05/12 18:54:53 INFO Worker: Connecting to master 192.168.11.77:7077...\n",
    "21/05/12 18:54:53 INFO TransportClientFactory: Successfully created connection to /192.168.11.77:7077 after 40 ms (0 ms spent in bootstraps)\n",
    "21/05/12 18:54:53 INFO Worker: Successfully registered with master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "___Create Worker 2___\n",
    "<br>Open yet another (separate) py37 terminal:\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "\n",
    "21/05/12 19:06:21 INFO Utils: Successfully started service 'sparkWorker' on port 63243.\n",
    "21/05/12 19:06:22 INFO Worker: Starting Spark worker 192.168.11.77:63243 with 8 cores, 23.0 GB RAM\n",
    "21/05/12 19:06:22 INFO Worker: Running Spark version 2.4.7\n",
    "21/05/12 19:06:22 INFO Worker: Spark home: C:\\spark-2.4.7-bin-hadoop2.7\n",
    "21/05/12 19:06:22 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.\n",
    "21/05/12 19:06:22 INFO Utils: Successfully started service 'WorkerUI' on port 8082.\n",
    "21/05/12 19:06:22 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8082\n",
    "21/05/12 19:06:22 INFO Worker: Connecting to master 192.168.11.77:7077...\n",
    "21/05/12 19:06:22 INFO TransportClientFactory: Successfully created connection to /192.168.11.77:7077 after 41 ms (0 ms spent in bootstraps)\n",
    "21/05/12 19:06:22 INFO Worker: Successfully registered with master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "___Associate Cluster Master with the Driver___\n",
    "<br>Open command prompt (anaconda py37), run spark shell with default config\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ pyspark --master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "Stop spark shell.\n",
    "\n",
    "<br><br>\n",
    "___Customizing Worker requirements___\n",
    "<br>Start spark shell with specific requirements..\n",
    "\n",
    "```bash\n",
    "# Use one of these to work on locally (Run on anaconda py37)\n",
    "$ pyspark --master spark://192.168.11.77:7077 --driver-memory 6G --driver-cores 2 --executor-memory 4G --executor-cores 2\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "__Customizing Number of Executors__\n",
    "```bash\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "\n",
    "\n",
    "(py37) C:\\Users\\Administrator>pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "Python 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "21/05/12 19:37:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "21/05/12 19:37:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.7 (default, May  6 2020 11:45:54)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "```\n",
    "\n",
    "* Check the port numbers on console/terminal for Web UI.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "__CONCLUSION__\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.master.Master\n",
    "# Create Spark Master (Get the IP:port)\n",
    "\n",
    "# Launch http://192.168.11.77:8080/ --> Get this from the terminal (from above command)\n",
    "\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "# Create Workers (in separate terminals)\n",
    "\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "# Assign workers to the Driver\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `spark-config`\n",
    "PySpark spark-config.ipynb\n",
    "<br>PySpark spark-config - In Parallel.ipynb\n",
    "<br>assignment HDFS-(spark-config)-DF.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day13\">[Day 13](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment e-commerce.ipynb \n",
    "\n",
    "## RDD DF JOINs\n",
    "PySpark DF JOINs.ipynb _`INNER, OUTER, LEFT/RIGHT OUTER, LEFTSEMI, LEFTANTI`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day14\">[Day 14](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark Spark SQL - createOrReplaceGlobalTempView().ipynb\n",
    "<br>PySpark Spark SQL - createOrReplaceTempView().ipynb\n",
    "<br>PySpark Spark Database.ipynb\n",
    "<br>PySpark Spark Database - COPY (LOCAL).ipynb\n",
    "<br>PySpark Spark Database - DF JOINs (LOCAL).ipynb\n",
    "<br>PySpark Spark Database - DF JOINs (HIVE).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Spark SQL Warehouse preparation\n",
    "(Refer Day 10's Hive)\n",
    "<br>__WINDOWS ONLY__\n",
    "<br>Create `c:\\spark`, `c:\\spark-temp`\n",
    "\n",
    "In command prompt, run\n",
    "```bash\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\spark\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\spark-temp\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\tmp\\hive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day15\">[Day 15](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Hive's `meta-server`\n",
    ">NOTE: This will run Hive's `meta-server` (not `thrift-server`)\n",
    ">\n",
    ">__Start meta server on port 9083__ [`9083` is configured in `hive-site.xml`]\n",
    ">```bash\n",
    "cd $HIVE_HOME \n",
    "$HIVE_HOME/bin/hive --service metastore\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift\n",
    "\n",
    "Modified PostgreSQL == Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Interface PostgreSQL with Hive\n",
    "\n",
    "```bash\n",
    "# --------------------------------------------------------------------\n",
    "# From `git-bash`\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n",
    "\n",
    "wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
    "\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install postgresql-12\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Create Metastore database and user accounts:\n",
    "sudo -u postgres psql\n",
    "\n",
    "postgres=# CREATE USER hiveuser WITH PASSWORD 'mypassword';\n",
    "postgres=# CREATE DATABASE metastore;\n",
    "postgres=# exit\n",
    "\n",
    "\n",
    "# check username and password exist, type mypassword for password\n",
    "psql -h localhost -U hiveuser -d metastore\n",
    "metastore=# exit\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Download JDBC driver for Spark/Python program to connect.. also for Hive meta-store\n",
    "cd ~\n",
    "wget https://jdbc.postgresql.org/download/postgresql-42.2.20.jar\n",
    "sudo mv postgresql-42.2.20.jar /usr/share/java/postgresql-jdbc.jar\n",
    "\n",
    "\n",
    "sudo chmod 644 /usr/share/java/postgresql-jdbc.jar\n",
    "sudo ln -s /usr/share/java/postgresql-jdbc.jar $HIVE_HOME/lib/postgresql-jdbc.jar\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Updating config files that has postgresql user credentials..\n",
    "\n",
    "# Make postgres listen on ip 192.168.93.128 \n",
    "# Edit via Ubuntu Desktop (gedit is a GUI editor)\n",
    "sudo gedit  /etc/postgresql/12/main/postgresql.conf\n",
    "\n",
    "\n",
    "# Search & update entry \"listen_adderss\" to\n",
    "listen_addresses = '*'\n",
    "# Save and exit\n",
    "\n",
    "# Enable postgresql to accept connection from specific ip. \n",
    "sudo gedit  /etc/postgresql/12/main/pg_hba.conf \n",
    "# Paste below line at the very end (replace the one you add if there's anything with this)\n",
    "host    all             all             0.0.0.0/0            md5\n",
    "# Save and exit\n",
    "\n",
    "# Restart\n",
    "sudo service postgresql restart\n",
    "\n",
    "# Update hive-site.xml\n",
    "mv $HIVE_HOME/conf/hive-site.xml $HIVE_HOME/conf/hive-site.xml.bak\n",
    "wget -P $HIVE_HOME/conf https://raw.githubusercontent.com/nodesense/cts-aws-spark-april-2021/main/pg/hive-site.xml\n",
    "\n",
    "\n",
    "# Close any running Hive meta-server\n",
    "# init postgres schema\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/schematool -dbType postgres -initSchema\n",
    "\n",
    "# Finally, to use postgres as a metastore..\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/hive --service metastore\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Running ThriftServer (for web/jdbc interface)\n",
    "__$HIVE_HOME/bin/hive/`hiveserver2` is ThriftServer's service name.__\n",
    "<br><br>\n",
    "\n",
    "```bash\n",
    "# Running Hive's thrift-server\n",
    "# hiveserver2 for JDBC, Web Access\n",
    "\n",
    "cd $HIVE_HOME\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Option 1: Run as a server\n",
    "$HIVE_HOME/bin/hiveserver2\n",
    "\n",
    "# Option 2: Run as service --> (Used in Training)\n",
    "$HIVE_HOME/bin/hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.root.logger=INFO,console\n",
    "# $HIVE_HOME/bin/hive --service hiveserver2\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Optional check: Is hiveserver2 running on port 10000 (default)?\n",
    "netstate -anp | grep 10000\n",
    "```\n",
    "\n",
    "__Hive server web UI runs on port 10002 (displayed on thrift server console)__\n",
    "[http://192.168.93.128:10002](http://192.168.93.128:10002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Connect hive using JDBC interface\n",
    "\n",
    "JDBC is useful to connect to Database using Java, Python, other languages, including Spark.\n",
    "<br>JDBC Client application [our python,java, scala code, spark] shall connect to DB server using JDBC technologies.\n",
    "<br>Then client application send queries to DB server; DB server executes queries and returns the result.\n",
    "\n",
    "### Beeline (is a JDBC client)\n",
    ">.. is a simple, JDBC client tool\n",
    "<br>.. useful in connecting to as many databases available\n",
    " \n",
    "```bash\n",
    "# connect to hive using jdbc, using metastore_db location\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/beeline -u jdbc:hive2://\n",
    " \n",
    "# perform queries\n",
    "show tables;\n",
    "show databases;\n",
    "\n",
    "create table invoices(id STRING, amount INT);\n",
    "insert into invoices values('1', 1000); \n",
    "insert into invoices values('2', 2000);\n",
    "\n",
    "select * from invoices;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day16\">[Day 16](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS EC2 | RDS (PostgreSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day17\">[Day 17](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Kafka\n",
    "__Install KAFKA__\n",
    "```bash\n",
    "# 1 --> Set KAFKA_HOME\n",
    "ssh ubuntu@192.168.93.128\n",
    "cd ~\n",
    "\n",
    "wget https://mirrors.estointernet.in/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
    "tar xf kafka_2.12-2.8.0.tgz\n",
    "\n",
    "sudo mv kafka_2.12-2.8.0 /opt\n",
    "sudo chmod 777 /opt/kafka_2.12-2.8.0\n",
    "\n",
    "# edit /etc/environment to add KAFKA_HOME\n",
    "sudo nano /etc/environment\n",
    "    KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "    \n",
    "# Load KAFKA_HOME for immediate use w/o restarting..\n",
    "nano ~/.bashrc\n",
    "    export KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "    export PATH=$PATH:$KAFKA_HOME/bin\n",
    "\n",
    "source ~/.bashrc\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__Run ZOOKEEPER__\n",
    "```bash\n",
    "# 2 --> (new terminal) Run Zookeeper; runs on port 2181\n",
    "ssh ubuntu@192.168.93.128\n",
    "zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties\n",
    "\n",
    "# 3 --> (new terminal) Run Kafka Broker: runs on port 9092\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-server-start.sh $KAFKA_HOME/config/server.properties\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__Create TOPIC__\n",
    "```bash\n",
    "# 4 --> (new terminal) Create Kafka's Topic(s)\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# Create Kafka Topic\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n",
    "    > Created topic test.\n",
    "\n",
    "# List Kafka Topics\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "    > test\n",
    "\n",
    "# Describe Topics\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test\n",
    "    >       Topic: test     Partition: 0    Leader: 0       Replicas: 0     Isr: 0\n",
    "    # Isr --> InSyncReplicas\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__PRODUCER__\n",
    "```bash\n",
    "# 5 --> (new terminal) Create Kafka Producer(s) with comma-separated brokers [Produce message(s) to Kafka Broker]\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-console-producer.sh --broker-list localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__CONSUMER__\n",
    "```bash\n",
    "# 6 --> (new terminal) Create Kafka Consumer(s) with comma-separated brokers [Comsume message(s) from Kafka Broker]\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning \n",
    "# --------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day18\">[Day 18](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPIC with multiple-partitions\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic logs\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "### pub-sub with multiple partitions (run without key)\n",
    "With no key, data is partitioned in a Round-Robin fashion.\n",
    "__Run PRODUCER w/o key__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER w/o partition__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER with partition__\n",
    "```bash\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 0\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 1\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 2\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "### pub-sub with multiple partitions (run with key)\n",
    "With key, data is partitioned using hash value.\n",
    "\n",
    ">__TOPIC with multiple-partitions__\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic logger\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs,logger\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run PRODUCER w/o key__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logger --property \"parse.key=true\" --property \"key.separator=:\"\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER with partition__\n",
    "```bash\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 0 --property \"print.key=true\"\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 1 --property \"print.key=true\"\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 2 --property \"print.key=true\"\n",
    "# --------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OFFSET\n",
    ">Offset is per partition. So, every partition has an offset that starts at 0.\n",
    "<br>Writing to a parition will increment the offset (automatic)\n",
    "<br>One can read from a specific offset by passing it as a parameter.\n",
    "\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# Read from offset 2 onwards..\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 0 -offset 2 --property \"print.key=true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### TOPIC\n",
    "#### TOPIC Creation\n",
    "Refer above 2 cells\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs,logger,custom\n",
    "```\n",
    "\n",
    "#### TOPIC Deletion\n",
    "Topic deletion is not immediate, but eventual. Don't be alarmed if you're able to read previous messages post deletion. <br>Remember, deletion will not happen immediately.\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic custom\n",
    "```\n",
    "\n",
    "#### TOPIC Alteration\n",
    "Number of paritions can only be increased (not decreased)\n",
    "<br>Existing partition information remain as-is, but future publishes may be altered.\n",
    "```bash\n",
    "# Create partition\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "# Increase partitions\n",
    "kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 4 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "# Reduce partitions\n",
    "kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 2 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "Error while executing topic command : Topic currently has 4 partitions, which is higher than the requested 2.\n",
    "[2021-05-20 21:47:46,243] ERROR org.apache.kafka.common.errors.InvalidPartitionsException: Topic currently has 4 partitions, which is higher than the requested 2.\n",
    " (kafka.admin.TopicCommand$)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka-Python\n",
    "\n",
    "```bash\n",
    "# 1\n",
    "pip install kafka-python\n",
    "\n",
    "# 2\n",
    "# Create Topic (ensure Zookeeper & Broker are already running)\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic transactions\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic transactions\n",
    "\n",
    "# 3\n",
    "# Create Producer\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test\n",
    "\n",
    "# 4\n",
    "# Create Consumer\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning \n",
    "```\n",
    "\n",
    "<br>Kafka-Python.ipynb\n",
    "<br>Kafka-Python Producer.ipynb\n",
    "<br>Kafka-Python Consumer.ipynb\n",
    "<br>Kafka-Python Producer-for-ConsumerGroups.ipynb\n",
    "<br>Kafka-Python Consumer1(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer2(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer3(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer4(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer5(group1)-for-Producer.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day19\">[Day 19](#root)</a>\n",
    "\n",
    "Something simple to understand how messaging works:\n",
    "<br>[Structured programming guide](https://spark.apache.org/docs/2.4.0/structured-streaming-programming-guide.html):\n",
    "```bash\n",
    "# terminal 1\n",
    "nc -lk 9999\n",
    "# telnet messages appear here\n",
    "\n",
    "# terminal 2\n",
    "telnet localhost 9999\n",
    "# Type something here\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP\n",
    "\n",
    "<br>Backup C:\\spark-2.4.7-bin-hadoop2.7\\conf\\spark-defaults.conf.template -> spark-defaults.comf.template.bak\n",
    "<br>Rename & Edit C:\\spark-2.4.7-bin-hadoop2.7\\conf\\spark-defaults.conf.template as \"spark-defaults.conf\"\n",
    "\n",
    "```bash\n",
    "spark-defaults.conf\n",
    "# is always loaded by spark runtime when it starts....\n",
    "# here we can mention default parameters needed for spark workers..\n",
    "\n",
    "# Paste at EOF\n",
    "spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0\n",
    "```\n",
    "\n",
    "Refer..\n",
    "<br>PySpark Streaming (basic) How streaming works.ipynb\n",
    "<br>PySpark Streaming (basic) How streaming works (WordCount).ipynb\n",
    "<br>PySpark Streaming (basic) How streaming works (Kafka-to-Spark).ipynb\n",
    "<br>PySpark Streaming (basic) How streaming works (Kafka-to-Spark)-Invoices.ipynb\n",
    "<br>Kafka-Invoice-Producer.ipynb\n",
    "<br>Kafka-Invoice-Consumer.ipynb\n",
    "<br>Kafka-Invoice-Consumer (Aggregate).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day20\">[Day 20](#root)</a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S3\n",
    "Create S3 bucket.\n",
    "Create IAM Access Key. \n",
    "<br>Upload `movielens` files.\n",
    "<br>On Windows, create:\n",
    ">_Create 2 files under.._\n",
    ">.aws [folder]\n",
    ">```toml\n",
    "|- credentials\n",
    "    [default]\n",
    "    aws_access_key_id = YOUR_ACESS_KEY\n",
    "    aws_secret_access_key = YOUR_SECRET_ACCESS_KEY\n",
    "|- config\n",
    "    [default]\n",
    "    region=us-east-2\n",
    "\n",
    "<br>\n",
    "\n",
    "## [boto3](https://boto3.amazonaws.com/v1/documentation/api/latest/guide/quickstart.html#installation) + [Databricks](https://community.databricks.com)\n",
    "Official python lib for aws S3.\n",
    "\n",
    "Refer..\n",
    "<br>AWS S3 - boto3.ipynb\n",
    "<br>databricks/*.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day21\">[Day 21](#root)</a> AWS Redshift as datalake \n",
    "Today, the idea is to connect to AWS RDS's DB from DBeaver via `JDBC jar`.\n",
    "1. Create RDS PostgreSQL DB\n",
    "2. Connect on DBeaver\n",
    "3. Download [JDBC Driver from maven repo](https://repo1.maven.org/maven2/org/postgresql/postgresql/42.2.19/postgresql-42.2.19.jar)\n",
    "_Most of the open sourced projects' drivers are available in Maven repo_\n",
    "4. Update the same in Spark Home's `jar` folder.\n",
    "5. Download [Redshift JDBC driver redshift-jdbc42-2.0.0.4](https://us-east-2.console.aws.amazon.com/redshiftv2/home?region=us-east-2#clusters)\n",
    "6. Update the same in Spark Home's `jar` folder\n",
    ">Edit `spark-defaults.conf` to include:\n",
    "><br>spark.jars.packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0\n",
    "><br>spark.jars file:///C:/spark-2.4.7-bin-hadoop2.7/jars/postgresql-42.2.19.jar\n",
    "\n",
    "#### Redshift\n",
    "1. for OLAP\n",
    "2. Built out of PostgreSQL\n",
    "\n",
    ">##### Redshift Query [from AWS Redshift left pane]\n",
    "default db: dev\n",
    "<br>default user: awsuser\n",
    "\n",
    "### AWS Glue (totally spark) + Athena\n",
    "Athena: Query engine, based on Presto, Serverless SQL\n",
    "AWS Glue: create DB, create crawler (this creates a schema)\n",
    "\n",
    "Refer\n",
    "<br>AWS RDS (jdbc postgresql).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "[Why Redshift?](https://docs.databricks.com/data/data-sources/aws/amazon-redshift.html)\n",
    "```\n",
    "Amazon Redshift <-> Spark\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day22\">[Day 22](#root)</a> AWS S3 as datalake\n",
    "## IAM role\n",
    "Create a `Role` [`training-bond-s3role`] with policies..\n",
    ">`AmazonS3FullAccess`\n",
    ">`AWSGlueServiceRole`\n",
    ">`AWSGlueConsoleFullAccess`\n",
    "\n",
    "\n",
    "## How AWS Glue works\n",
    "Glue can talk to S3 without an endpoint.\n",
    "<br>Create a VPC endpoint so that RDS and other JDBC dependent services can talk to Glue.\n",
    "<br>To use Glue data catalog as the Hive metastore, the IAM role used for the job should have glue:CreateDatabase permissions. A database called \"default\" is created in the Glue data catalog if it does not exist.\n",
    "<br>Glue Studio adds an ETL layer.\n",
    "\n",
    "For this usecase, I'm going to load S3's data into RDS's DB.\n",
    "\n",
    "Every source/target should be registered in AWS Glue's Data Catalog (in its hive schema).\n",
    "\n",
    "1. For Glue to talk to S3, `create a VPC endpoint` to `(search) S3` as a `Gateway` [check Routing Table option] in VPC -> Endpoints.\n",
    "2. Edit VPC Security Groups's default security group. Add an Inbound Rule listing its own security group ID[All traffic|All|All|Custom|sg-a0fc8cd1].\n",
    "3. Ensure AWS Glue Connections enlists RDS's DB (using RDS DB's endpoint:port/dbname as JDBC)\n",
    "\n",
    "4. Create separate crawlers for each source/destination to create their metadata [e.x: a JDBC crawler would have (`productdb/public/%`) -> (dbname/schemaname/%)]\n",
    "5. Run a Glue Job using the resulting metadata schemas [S3 -> Data Catalog]\n",
    "\n",
    "## [Why create a VPC endpoint for S3-Glue traffic?](https://docs.aws.amazon.com/glue/latest/dg/vpc-endpoints-s3.html)\n",
    "\n",
    "### assignment\n",
    "```\n",
    "big-picture: Move brands data from data lake to RDS using glue\n",
    "1. create a brands.csv (brand_id:int,brand_name:string), containing 5 records\n",
    "\n",
    "2. in S3 data lake, create a folder called `brands` in bucket + upload brands.csv into the bucket\n",
    "3. Then use AWS Glue crawler, on brands folder in s3, then create a table \"brands\" on database orderdb\n",
    "4. Then query data in Athena..\n",
    "\n",
    "5. in RDS, create table called brands(id:int, name:text) in dbeaver/vinsys machine.. [dont insert any records ]\n",
    "6. **Run the RDS JDBC Crawler; this will add `postgres_public_brands` table in its catalog db, \"orderdb\" \n",
    "\n",
    "7. Write a glue job in studio that copies brands table data from orderdb ->DB RDS DB postgres/public/brands (copy data lake data into rds)\n",
    "that is, copy orderdb.brands [s3] to orderdb.postgres_public_brands [RDS/postgres]\n",
    "8. run the glue job\n",
    "\n",
    "9. Go to RDS, `select * from brands`: we should see all the 5 records in csv\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day23\">[Day 23](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Redshit + Glue\n",
    "```sql\n",
    "-- 2 GOALs: Move data b/w Reshift & S3 using Glue\n",
    "\n",
    "\n",
    "-- Source: Redshift\n",
    "-- Target: S3 as datalake\n",
    "CREATE TABLE IF NOT EXISTS orders(id int,\n",
    "                                  amount FLOAT);\n",
    "INSERT INTO orders VALUES(1, 100);\n",
    "INSERT INTO orders VALUES(2, 300);\n",
    "INSERT INTO orders VALUES(3, 300);\n",
    "SELECT * FROM orders;\n",
    "\n",
    "\n",
    "-- Source: S3 datalake\n",
    "-- Target: Redshift\n",
    "CREATE TABLE IF NOT EXISTS invoices(id int,\n",
    "                                    qty INT,\n",
    "                                    unit_price FLOAT);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Glue script\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "import re\n",
    "from awsglue.dynamicframe import DynamicFrameCollection\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "\n",
    "def MyTransform(glueContext, dfc) -> DynamicFrameCollection:\n",
    "    import pyspark.sql.functions as F\n",
    "    \n",
    "    # convert dymamic frame to data frame\n",
    "    df = dfc.select(list(dfc.keys())[0]).toDF()\n",
    "    df = df.withColumn(\"title\", F.upper(F.col(\"title\") ))\n",
    "    \n",
    "    # create dynamic frame from dataframe\n",
    "    upperDf = DynamicFrame.fromDF(df, glueContext, \"filter_votes\")\n",
    "    \n",
    "    return(DynamicFrameCollection({\"CustomTransform0\": upperDf}, glueContext))\n",
    "\n",
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "## @type: DataSource\n",
    "## @args: [database = \"moviedb\", table_name = \"ratings\", transformation_ctx = \"DataSource0\"]\n",
    "## @return: DataSource0\n",
    "## @inputs: []\n",
    "DataSource0 = glueContext.create_dynamic_frame.from_catalog(database = \"moviedb\", table_name = \"ratings\", transformation_ctx = \"DataSource0\")\n",
    "## @type: ApplyMapping\n",
    "## @args: [mappings = [(\"userid\", \"long\", \"userid\", \"long\"), (\"movieid\", \"long\", \"movieid\", \"long\"), (\"rating\", \"double\", \"rating\", \"double\"), (\"timestamp\", \"long\", \"timestamp\", \"long\")], transformation_ctx = \"Transform0\"]\n",
    "## @return: Transform0\n",
    "## @inputs: [frame = DataSource0]\n",
    "Transform0 = ApplyMapping.apply(frame = DataSource0, mappings = [(\"userid\", \"long\", \"userid\", \"long\"), (\"movieid\", \"long\", \"movieid\", \"long\"), (\"rating\", \"double\", \"rating\", \"double\"), (\"timestamp\", \"long\", \"timestamp\", \"long\")], transformation_ctx = \"Transform0\")\n",
    "## @type: SelectFields\n",
    "## @args: [paths = [\"userid\", \"movieid\", \"rating\"], transformation_ctx = \"Transform1\"]\n",
    "## @return: Transform1\n",
    "## @inputs: [frame = Transform0]\n",
    "Transform1 = SelectFields.apply(frame = Transform0, paths = [\"userid\", \"movieid\", \"rating\"], transformation_ctx = \"Transform1\")\n",
    "## @type: Filter\n",
    "## @args: [f = lambda row : (row[\"rating\"] >= 3 and row[\"rating\"] <= 4), transformation_ctx = \"Transform7\"]\n",
    "## @return: Transform7\n",
    "## @inputs: [frame = Transform1]\n",
    "Transform7 = Filter.apply(frame = Transform1, f = lambda row : (row[\"rating\"] >= 3 and row[\"rating\"] <= 4), transformation_ctx = \"Transform7\")\n",
    "## @type: DataSource\n",
    "## @args: [database = \"moviedb\", table_name = \"movies\", transformation_ctx = \"DataSource1\"]\n",
    "## @return: DataSource1\n",
    "## @inputs: []\n",
    "DataSource1 = glueContext.create_dynamic_frame.from_catalog(database = \"moviedb\", table_name = \"movies\", transformation_ctx = \"DataSource1\")\n",
    "## @type: ApplyMapping\n",
    "## @args: [mappings = [(\"movieid\", \"long\", \"movieid\", \"long\"), (\"title\", \"string\", \"title\", \"string\"), (\"genres\", \"string\", \"genres\", \"string\")], transformation_ctx = \"Transform5\"]\n",
    "## @return: Transform5\n",
    "## @inputs: [frame = DataSource1]\n",
    "Transform5 = ApplyMapping.apply(frame = DataSource1, mappings = [(\"movieid\", \"long\", \"movieid\", \"long\"), (\"title\", \"string\", \"title\", \"string\"), (\"genres\", \"string\", \"genres\", \"string\")], transformation_ctx = \"Transform5\")\n",
    "## @type: DropFields\n",
    "## @args: [paths = [\"genres\"], transformation_ctx = \"Transform2\"]\n",
    "## @return: Transform2\n",
    "## @inputs: [frame = Transform5]\n",
    "Transform2 = DropFields.apply(frame = Transform5, paths = [\"genres\"], transformation_ctx = \"Transform2\")\n",
    "## @type: CustomCode\n",
    "## @args: [dynamicFrameConstruction = DynamicFrameCollection({\"Transform2\": Transform2}, glueContext), className = MyTransform, transformation_ctx = \"Transform3\"]\n",
    "## @return: Transform3\n",
    "## @inputs: [dfc = Transform2]\n",
    "Transform3 = MyTransform(glueContext, DynamicFrameCollection({\"Transform2\": Transform2}, glueContext))\n",
    "## @type: SelectFromCollection\n",
    "## @args: [key = list(Transform3.keys())[0], transformation_ctx = \"Transform6\"]\n",
    "## @return: Transform6\n",
    "## @inputs: [dfc = Transform3]\n",
    "Transform6 = SelectFromCollection.apply(dfc = Transform3, key = list(Transform3.keys())[0], transformation_ctx = \"Transform6\")\n",
    "## @type: Join\n",
    "## @args: [keys2 = [\"movieid\"], keys1 = [\"movieid\"], transformation_ctx = \"Transform8\"]\n",
    "## @return: Transform8\n",
    "## @inputs: [frame1 = Transform7, frame2 = Transform6]\n",
    "Transform8 = Join.apply(frame1 = Transform7, frame2 = Transform6, keys2 = [\"movieid\"], keys1 = [\"movieid\"], transformation_ctx = \"Transform8\")\n",
    "## @type: SelectFields\n",
    "## @args: [paths = [\"userid\", \"movieid\", \"rating\", \"title\"], transformation_ctx = \"Transform4\"]\n",
    "## @return: Transform4\n",
    "## @inputs: [frame = Transform8]\n",
    "Transform4 = SelectFields.apply(frame = Transform8, paths = [\"userid\", \"movieid\", \"rating\", \"title\"], transformation_ctx = \"Transform4\")\n",
    "## @type: DataSink\n",
    "## @args: [connection_type = \"s3\", format = \"csv\", connection_options = {\"path\": \"s3://bond-s3-forspark/movies-ratings/\", \"partitionKeys\": []}, transformation_ctx = \"DataSink0\"]\n",
    "## @return: DataSink0\n",
    "## @inputs: [frame = Transform4]\n",
    "DataSink0 = glueContext.write_dynamic_frame.from_options(frame = Transform4, connection_type = \"s3\", format = \"csv\", connection_options = {\"path\": \"s3://bond-s3-forspark/movies-ratings/\", \"partitionKeys\": []}, transformation_ctx = \"DataSink0\")\n",
    "job.commit()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS CloudFormation\n",
    "A way to automate ETL scripts in AWS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer\n",
    "<br>Kinesis-Invoice-Producer.ipynb\n",
    ">New role with..\n",
    "<br>`AmazonS3fullaccess`\n",
    "<br>`AWSGlueConsoleFullAccess`\n",
    "<br>`Amazonathena full access (optional)`\n",
    "<br>`AWSLambda full acccess (optional)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AWS Redshift Spectrum\n",
    "An add-on to AWS Redshift for extra-computations!\n",
    "\n",
    "AWS Redshift can query only S3 (not any other DB) via Glue.\n",
    "<br>The flow is: `AWS Redshift` (queries)-> `AWS Glue` (queries)-> `S3` datalake.\n",
    "\n",
    "```sql\n",
    "-- We have an existing DB, moviedb, in Glue catalog\n",
    "-- Now in Redshift, create an external schema (so this won't be in Redshift) -- points outside redshift\n",
    "-- create external schema, that links to moviedb in Glue Catalog\n",
    "\n",
    "CREATE EXTERNAL SCHEMA moviedb\n",
    "  FROM DATA CATALOG DATABASE 'moviedb' IAM_ROLE 'arn:aws:iam::779334642514:role/bond-role-redshift'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
    "\n",
    "\n",
    "SELECT * FROM moviedb.movies;\n",
    "```\n",
    "See below for full code from trainer.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "-- we have moviedb in glue catalog\n",
    "-- in redshift, we have to create a external schema - points outside redshift\n",
    "-- create external schema, that links moviedb in glue catalog.\n",
    "-- we alkready have glue db called moviedb..\n",
    "-- create a external schema within redshift in dev db\n",
    "-- link to glue database moviedb\n",
    "\n",
    "CREATE EXTERNAL SCHEMA moviedb \n",
    "FROM DATA CATALOG DATABASE 'moviedb' IAM_ROLE 'arn:aws:iam::495478549549:role/TRAINING_REDSHIFT'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
    "\n",
    "-- after above statement, \n",
    "-- check the schema, whether moviedb present\n",
    "-- the the tables of  moviedb\n",
    "-- check if you can query moviedb schema tables, Datalake files\n",
    "-- we are querting external tables, where data is located in s3\n",
    "\n",
    "SELECT * from moviedb.movies;\n",
    "\n",
    "-- Try to create  a database in glue, and a table that points to s3 location\n",
    "\n",
    "-- sales is redshift schema, salesdb is glue catalog database name\n",
    "CREATE EXTERNAL SCHEMA sales \n",
    "FROM DATA CATALOG DATABASE 'salesdb' IAM_ROLE 'arn:aws:iam::495478549549:role/TRAINING_REDSHIFT'\n",
    "CREATE EXTERNAL DATABASE IF NOT EXISTS;\n",
    "\n",
    "-- check salesdb created in catalog, sales schema created in redshift\n",
    "\n",
    "-- now create external table sales in salesdb catalog, sales schema\n",
    "CREATE EXTERNAL TABLE sales.sales(\n",
    "\tid INTEGER,\n",
    "    qty INTEGER,\n",
    "    price FLOAT\n",
    ")\n",
    "ROW FORMAT DELIMITED\n",
    "FIELDS TERMINATED BY ','\n",
    "STORED AS TEXTFILE\n",
    "LOCATION 's3://ctsspark01/sales'\n",
    "\n",
    "-- now check salesdb in catalog, it should have table called sales \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "### Note: the external table, we haven't mention the SKIP row or header, now we need to create a csv file without header\n",
    "\n",
    "create a file called sales1.csv\n",
    "\n",
    "paste below content and save\n",
    "\n",
    "```\n",
    "1,10,100\n",
    "2,5,3\n",
    "3,2,50\n",
    "```\n",
    "\n",
    "upload this to s3 bucket /sales director, create new sales directory in s3 if not present..\n",
    "\n",
    " Now check if we could query s3 data\n",
    " \n",
    "```sql\n",
    "select * from sales.sales\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day24\">[Day 24](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lambda\n",
    "\n",
    "Permissions for a role exclusive to Lambda:\n",
    ">LambdaFullAccess\n",
    "<br>S3FullAccess\n",
    "<br>DynamodbFullAccess\n",
    "<br>SNSFullAccess\n",
    "<br>CloudWatchFullAccess\n",
    "\n",
    "While creating Lambda function, ensure you choose the role created above else it keeps creating new roles for every function run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lambda functions\n",
    "event/stream data -> base64.decode -> json.loads -> process -> json.dumps -> base64.encode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import json\n",
    "import base64\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    output = []\n",
    "    \n",
    "    #'result' either 'Ok' (succes bucket) or ProcessingFailed (failed bucket)\n",
    "    # event[\"records\"] consist of 60 seconds catpured data as list\n",
    "    for record in event[\"records\"]:\n",
    "        print(\"record\", record)\n",
    "        # the data is encoded by kinesis firehose using base64\n",
    "        # we need to decode data back to normal json\n",
    "        encoded_payload = record[\"data\"] # base64 string\n",
    "        # json_payload is the one actually send by kinesis producer\n",
    "        json_payload = base64.b64decode(encoded_payload) # base64 to json string\n",
    "        print(\"decoded \", json_payload)\n",
    "        invoice = json.loads(json_payload) # load python object from json string\n",
    "        \n",
    "        invoice[\"Amount\"] = invoice[\"UnitPrice\"] * invoice[\"Quantity\"]\n",
    "        \n",
    "        # since now, no producer, and lambda is doing transformation, lambda should encode the json data\n",
    "        json_payload = json.dumps(invoice).encode(\"utf-8\")\n",
    "        encoded_payload = base64.b64encode(json_payload).decode(\"utf-8\")\n",
    "        \n",
    "        output_record = {\n",
    "            'recordId': record['recordId'],\n",
    "            'result': 'Ok', # OK, success, it should go to success bucket\n",
    "            'data': encoded_payload\n",
    "        }\n",
    "        \n",
    "        output.append(output_record)\n",
    "        \n",
    "    return {'records': output}\n",
    "```\n",
    "\n",
    "Make sure `Transform Source Records with Lambda` is enabled in AWS Firehose (Kinesis).\n",
    "<br>For this to happen, the role attached to Firehose should have Lambda execution permission.\n",
    "\n",
    "Use Kinesis-Invoice-Producer.ipynb to generate stream, then check S3 to check if the Lambda transformation went through."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day25\">[Day 25](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon MSK (Managed Services for Apache Kafka) \n",
    "EMR charges by the hour + EC2 charges.\n",
    "\n",
    "Create a cluster in MSK.\n",
    "<br>Navigate to EMR.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gitbash\n",
    "```bash\n",
    "# EC2 IPV4 --> ec2-18-117-143-76.us-east-2.compute.amazonaws.com\n",
    "cd c:\\\n",
    "cd awskeys\n",
    "chmod 400 ubuntu-bond-new.pem\n",
    "ssh -i ubuntu-bond-new.pem ubuntu@ec2-18-117-143-76.us-east-2.compute.amazonaws.com\n",
    "ssh -i ubuntu-bond-new.pem ubuntu@18-117-143-76\n",
    "```\n",
    "\n",
    "```bash\n",
    "\n",
    "sudo apt update \n",
    "\n",
    "\n",
    "sudo apt upgrade \n",
    "\n",
    "\n",
    "sudo apt install openjdk-8-jdk -y\n",
    "\n",
    "\n",
    "cd ~\n",
    "\n",
    "wget https://mirrors.estointernet.in/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
    "\n",
    "tar xf kafka_2.12-2.8.0.tgz\n",
    "\n",
    "\n",
    "\n",
    "sudo mv kafka_2.12-2.8.0 /opt\n",
    "\n",
    "sudo chmod 777 /opt/kafka_2.12-2.8.0\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "```\n",
    "export KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "\n",
    "export PATH=$PATH:$KAFKA_HOME/bin\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Trainer\n",
    "\n",
    "Open gitbash \n",
    "\n",
    "```bash\n",
    "cd c:\n",
    "cd keys\n",
    "chmod 400 <<key.pem>>\n",
    "cd:\\keys>ssh -i <<key.pem>> ubuntu@ec2-xx-yy-zz-abc.us-east-2.compute.amazonaws.com\n",
    "```\n",
    "\n",
    "once connected to ec2 instance, then install java/kafka etc.. \n",
    "```bash\n",
    "sudo apt update \n",
    "sudo apt upgrade \n",
    "sudo apt install openjdk-8-jdk -y\n",
    "\n",
    "cd ~\n",
    "wget https://mirrors.estointernet.in/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
    "tar xf kafka_2.12-2.8.0.tgz\n",
    "\n",
    "sudo mv kafka_2.12-2.8.0 /opt\n",
    "sudo chmod 777 /opt/kafka_2.12-2.8.0\n",
    "```\n",
    "\n",
    "```bash\n",
    "nano ~/.bashrc\n",
    "```\n",
    "\n",
    "paste below content..\n",
    "```bash\n",
    "export KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "export PATH=$PATH:$KAFKA_HOME/bin\n",
    "```\n",
    "\n",
    "Ctrl + O to save the file\n",
    "Ctrl + X to exit nano\n",
    "\n",
    "```\n",
    "logout\n",
    "```\n",
    "\n",
    "and re-login..\n",
    "\n",
    "get broker details from MSK / Kafka portal , View Client information, plaintext...\n",
    "\n",
    "```\n",
    "$KAFKA_HOME/bin/kafka-topics.sh  --create --bootstrap-server b-2.demo-cluster-1.444444.c2.kafka.us-east-2.amazonaws.com:9092  --replication-factor 1 --partitions 1 --topic test\n",
    " \n",
    "\n",
    "$KAFKA_HOME/bin/kafka-topics.sh --list --bootstrap-server b-2.demo-cluster-1.444444.c2.kafka.us-east-2.amazonaws.com:9092\n",
    " \n",
    "\n",
    "$KAFKA_HOME/bin/kafka-topics.sh --describe --bootstrap-server b-2.demo-cluster-1.444444.c2.kafka.us-east-2.amazonaws.com:9092 --topic test\n",
    "```\n",
    "\n",
    "gitbash, ssh into ubuntu/ec2,\n",
    "\n",
    "```\n",
    "$KAFKA_HOME/bin/kafka-console-producer.sh  --broker-list b-2.demo-cluster-1.444444.c2.kafka.us-east-2.amazonaws.com:9092 --topic test\n",
    "```\n",
    "\n",
    "\n",
    "opne new gitbash, ssh into ubutnu/ec2\n",
    "```\n",
    "$KAFKA_HOME/bin/kafka-console-consumer.sh --bootstrap-server b-2.demo-cluster-1.444444.c2.kafka.us-east-2.amazonaws.com:9092 --topic test \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SCRAP\n",
    "```bash\n",
    "ZooKeeper\n",
    "z-1.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:2181\n",
    "z-2.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:2181\n",
    "z-3.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:2181\n",
    "kafka-topics.sh --create --bootstrap-server z-1.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:2181 --replication-factor 1 --partitions 1 --topic bond_awstopic\n",
    "\n",
    "GOPAL\n",
    "PLAIN \n",
    "----------------------------------------------------------------------------------\n",
    "\n",
    "Bootsrap Server\n",
    "b-1.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:9092\n",
    "b-2.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:9092\n",
    "kafka-topics.sh --create --bootstrap-server b-1.bond-cluster-msk.ha72r1.c3.kafka.us-east-2.amazonaws.com:9092 --replication-factor 1 --partitions 1 --topic awstopic\n",
    "\n",
    "GOPAL\n",
    "PLAIN b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092,b-1.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092\n",
    "tls - -b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9094,b-1.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9094\n",
    "\n",
    "\n",
    "kafka-topics.sh --create --bootstrap-server b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092 --replication-factor 1 --partitions 1 --topic bond_awstopic\n",
    "kafka-topics.sh --list --bootstrap-server b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092\n",
    "kafka-topics.sh --describe --bootstrap-server b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092 --topic bond_awstopic\n",
    "\n",
    "\n",
    "GOPAL EC2\n",
    "ec2-18-191-239-31.us-east-2.compute.amazonaws.com\n",
    "ssh -i cts-gopal-key.pem ubuntu@ec2-18-191-239-31.us-east-2.compute.amazonaws.com\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "PRODUCER\n",
    "kafka-console-producer.sh --broker-list b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092 --topic bond_awstopic\n",
    "\n",
    "CONSUMER\n",
    "kafka-console-consumer.sh --bootstrap-server b-2.demo-cluster-1.8wyx7i.c2.kafka.us-east-2.amazonaws.com:9092 --topic test\n",
    "-------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the producer + consumer are run..\n",
    "<br>Create a Kafka connection in AWS Glue with one of the subnets that's associate d with MSK cluster + default sg\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "EMR Create separate securty group, subnet altogether..\n",
    "we need to whitelist ips again.. \n",
    "\n",
    "check for Security groups for Master in the UI\n",
    "\n",
    "Click on the Security groups link for master.. and \n",
    "\n",
    "inbound rule for ssh for the IP..\n",
    "\n",
    "you need to use the key which is available with you..\n",
    "\n",
    "\n",
    "For emr, hadoop is username, not ubuntu..\n",
    "\n",
    "Copy from Master public DNS or use the UI to copy the command\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "ssh -i cts-gopal-key.pem hadoop@ec2-3-20-234-226.us-east-2.compute.amazonaws.com\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View cluster visually\n",
    "<br>https://docs.aws.amazon.com/emr/latest/ManagementGuide/emr-web-interfaces.html\n",
    "\n",
    "FoxyProxy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day26\">[Day 26](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### gitbash / windows\n",
    "\n",
    "```bash\n",
    "cd veena/awskeys\n",
    "ssh -i mykeypair.pem -N -D 8157 hadoop@ec2-###-##-##-###.compute-1.amazonaws.com\n",
    "```\n",
    "\n",
    "#### Firefox `foxyproxy`\n",
    "Add `foxyproxy` extension.\n",
    ">Filter patterns:\n",
    "```bash\n",
    "*ec2*.amazonaws.com* \n",
    "*10*.amazonaws.com* \n",
    "```\n",
    "\n",
    "#### AWS EMR\n",
    "In EMR, click on the cluster.\n",
    "Navigate to `Master public DNS`. Copy for SSH'ing.\n",
    "\n",
    "\n",
    "<br>Now try to copy jupyter url \n",
    "<br>https://ec2-xx-yy-zzz-aaa.us-east-2.compute.amazonaws.com:9443/hub/login?next=%2Fhub%2F\n",
    ">jupyter username: jovyan\n",
    "<br>password: jupyter\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HBase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From Trainer\n",
    "HBase\n",
    "\n",
    "NoSQL\n",
    "No Schema, No types\n",
    "Column Oriented DB\n",
    "\n",
    "table 1\n",
    "    shall have records, called as rows, each row shall have unique value, row id - 100\n",
    "\n",
    "    columns are called column family\n",
    "\n",
    "    columns: \n",
    "        info - family, itself not a value, it is called column family\n",
    "        info.name\n",
    "        info.mobile\n",
    "        info.age\n",
    "        info.gender\n",
    "\n",
    "        in above, name, mobile, age, gender are columns within column family.\n",
    "        During query, use column family:column name. info:name\n",
    "\n",
    "        salary - column family\n",
    "            salary:base \n",
    "            salary:bonus\n",
    "            salary:incentives\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "create table persons\n",
    "\n",
    "two column families\n",
    "    info.name\n",
    "        info:mobile\n",
    "        info:age\n",
    "        info:gender\n",
    "\n",
    "    salary - column family\n",
    "        salary:base \n",
    "        salary:bonus\n",
    "\n",
    "Add two rows , row-id??\n",
    "\n",
    "Mary\n",
    "Joe\n",
    "\n",
    "```\n",
    "\n",
    " sudo hbase shell\n",
    "\n",
    "\n",
    " create 'persons','info','salary'\n",
    "\n",
    " alter 'persons', NAME => 'address'\n",
    "\n",
    " describe 'persons'\n",
    "\n",
    " put 'persons','1','info:name', 'Joe'\n",
    " put 'persons','1','info:age', '35'\n",
    " put 'persons','1','salary:base', '3500'\n",
    " put 'persons','1','salary:bonus', '1500'\n",
    "\n",
    "\n",
    " put 'persons','2','info:name', 'Mary'\n",
    " put 'persons','2','info:age', '32'\n",
    " put 'persons','2','salary:base', '4000'\n",
    " put 'persons','2','salary:bonus', '2000'\n",
    "\n",
    " \n",
    "scan 'persons' \n",
    "get 'persons', '1'\n",
    " \n",
    " get 'persons', '1', {COLUMN => ['info:age', 'salary:bonus']}\n",
    " \n",
    "\n",
    " scan 'persons'\n",
    "\n",
    "\n",
    "scan 'persons', {COLUMNS => ['info:name']}\n",
    "\n",
    "scan 'persons', {COLUMNS => ['info:name','salary:base']}\n",
    "\n",
    "scan 'persons', LIMIT => 2\n",
    "\n",
    " scan 'persons', {COLUMNS => 'salary:base', FILTER => \"ValueFilter(>=, 'binaryprefix:3000')\"}\n",
    " scan 'persons', {COLUMNS => 'salary:base', FILTER => \"ValueFilter(<, 'binaryprefix:4000')\"}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day27\">[Day 27](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KSQL, Kafka Stream processing library\n",
    "\n",
    "[source](https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/confluent-kafka.md)\n",
    "```bash\n",
    "gitbash \n",
    "ssh into local ubuntu server\n",
    "```\n",
    "\n",
    "```bash\n",
    "wget http://packages.confluent.io/archive/5.5/confluent-5.5.1-2.12.tar.gz\n",
    "tar xf confluent-5.5.1-2.12.tar.gz\n",
    "\n",
    "sudo mv confluent-5.5.1 /opt\n",
    "sudo chmod 777 /opt/confluent-5.5.1/\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "# Change kafka environment path to the new directory\n",
    "nano /etc/environment\n",
    "# set $KAFKA_HOME\n",
    "$KAFKA_HOME=/opt/confluent-5.5.1\n",
    "\n",
    "nano ~/.bashrc\n",
    "# set $KAFKA_HOME + $PATH to include $KAFKA_HOME/bin\n",
    "export $KAFKA_HOME=/opt/confluent-5.5.1/\n",
    "export $PATH=$PATH:$KAFKA_HOME/bin\n",
    "\n",
    "# apply new changes with re-login, or..\n",
    "source ~/.bashrc\n",
    "\n",
    "# check if it all works\n",
    "confluent local start\n",
    "\n",
    "# sometimes it may error with exit code 127 complaining on 'curl'. so,\n",
    "sudo apt-get install curl\n",
    "\n",
    "# now run..\n",
    "confluent local start\n",
    "\n",
    "# the above command starts zookeeper, broker, kafka connectors, kafka schema registry, kafka ksql\n",
    "ubuntu@ubuntuvm:~$ confluent local start\n",
    "    The local commands are intended for a single-node development environment\n",
    "    only, NOT for production usage. https://docs.confluent.io/current/cli/index.html\n",
    "\n",
    "Using CONFLUENT_CURRENT: /tmp/confluent.pijCdfoX\n",
    "Starting zookeeper\n",
    "zookeeper is [UP]\n",
    "Starting kafka\n",
    "kafka is [UP]\n",
    "Starting schema-registry\n",
    "schema-registry is [UP]\n",
    "Starting kafka-rest\n",
    "kafka-rest is [UP]\n",
    "Starting connect\n",
    "connect is [UP]\n",
    "Starting ksql-server\n",
    "ksql-server is [UP]\n",
    "Starting control-center\n",
    "control-center is [UP]\n",
    "\n",
    "\n",
    "# view confluent clusters\n",
    "http://192.168.93.128:9021/clusters\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KSQL, Kafka Stream processing library\n",
    "```html\n",
    "https://github.com/nodesense/deloitte-kafka-2020\n",
    "https://docs.confluent.io/platform/current/streams/concepts.html\n",
    "```\n",
    "\n",
    "KStream -> Changelog -> KTable\n",
    "\n",
    "KSQL is not ANSI SQL, but SQL-like\n",
    "KSQL is not customizable.\n",
    "\n",
    "```html\n",
    "https://docs.microsoft.com/en-us/stream-analytics-query/hopping-window-azure-stream-analytics\n",
    "https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STREAMS (JSON)\n",
    "\n",
    "Gitbash, you need to login into ubuntu using \n",
    "\n",
    "```\n",
    "ssh ubuntu@192.168.93.128\n",
    "```\n",
    "\n",
    "# KSQL \n",
    "\n",
    "## Preparation\n",
    "\n",
    "Launch Gitbash Shell 1\n",
    "\n",
    "below produce the records every 5 seconds, write to topic users\n",
    "\n",
    "```\n",
    "ksql-datagen quickstart=users format=json topic=users maxInterval=5000\n",
    "```\n",
    "\n",
    "http://192.168.93.128:8081/subjects/users-value/versions/1\n",
    "\n",
    "Launch Gitbash Shell 2\n",
    "\n",
    "below produce the records every 5 seconds, write to topic pageviews\n",
    "\n",
    "```\n",
    "ksql-datagen quickstart=pageviews format=json topic=pageviews maxInterval=5000\n",
    "```\n",
    "\n",
    "http://192.168.93.128:8081/subjects/pageviews-value/versions/1\n",
    "\n",
    "# KSQL Shell /Gitbash/SSH 3\n",
    "```\n",
    "ksql \n",
    "```\n",
    "prompt will appear\n",
    "\n",
    "```\n",
    "SHOW STREAMS;\n",
    "# if there's an error, reboot ubuntu\n",
    "\n",
    "SHOW TABLES;\n",
    "\n",
    "CREATE STREAM users_stream (userid varchar, regionid varchar, gender varchar) WITH (kafka_topic='users', value_format='JSON');\n",
    "\n",
    "SHOW STREAMS;\n",
    "\n",
    "DESCRIBE users_stream;\n",
    "```\n",
    "\n",
    "NON_PERSISTED QUERIES [Means, the output/result is not stored into KAfka Brokers]\n",
    "\n",
    "```\n",
    "select userid, regionid, gender from users_stream EMIT CHANGES;\n",
    "select userid, regionid, gender from users_stream where gender='FEMALE' EMIT CHANGES;\n",
    "select userid, regionid, gender from users_stream where gender='MALE' EMIT CHANGES;\n",
    "```\n",
    "PERSISTED QUERIES [CREATE STREAM AS ] results written to Kafka\n",
    "Will be runnign automatically, need to use TERMINATE command to stop them\n",
    "\n",
    "```\n",
    "CREATE STREAM users_female AS SELECT userid AS userid, regionid FROM users_stream where gender='FEMALE';\n",
    "CREATE STREAM users_male AS SELECT userid AS userid, regionid FROM users_stream where gender='MALE';\n",
    "\n",
    "CREATE STREAM pageviews_stream (userid varchar, pageid varchar) WITH (kafka_topic='pageviews', value_format='JSON');\n",
    " \n",
    "select * from pageviews_stream EMIT CHANGES;\n",
    "```\n",
    "\n",
    "JOIN\n",
    "```\n",
    "CREATE STREAM user_pageviews_enriched_stream AS SELECT users_stream.userid AS userid, pageid, regionid, gender FROM pageviews_stream LEFT JOIN users_stream WITHIN 1 HOURS ON pageviews_stream.userid = users_stream.userid;\n",
    "\n",
    "select * from user_pageviews_enriched_stream EMIT CHANGES;\n",
    "```\n",
    "\n",
    "Ctrl +C to exit\n",
    "```\n",
    "CREATE TABLE pageviews_region_table WITH (VALUE_FORMAT='JSON') AS SELECT gender, regionid, COUNT() AS numusers FROM user_pageviews_enriched_stream WINDOW TUMBLING (size 60 second) GROUP BY gender, regionid HAVING COUNT() >= 1;\n",
    "\n",
    "select * from pageviews_region_table EMIT CHANGES;\n",
    "```\n",
    "\n",
    "### run Kafka-Invoices-Producer.ipynb\n",
    "```bash\n",
    "# this will create 'invoices' topic (if it doesn't exist in Confluent cluster.\n",
    "\n",
    "# now in ksql..\n",
    "CREATE STREAM invoices_stream(InvoiceNo int, StockCode varchar, Quantity varchar, Description varchar, InvoiceDate varchar, UnitPrice double, CustomerID int, Country varchar ) WITH (kafka_topic='invoices', value_format='JSON');\n",
    "\n",
    "SELECT * FROM invoices_stream EMIT CHANGES;\n",
    "\n",
    "CREATE TABLE invoices_country_count WITH (VALUE_FORMAT='JSON') AS SELECT Country, COUNT(InvoiceNo) AS NumInvoices FROM invoices_stream WINDOW TUMBLING (size 60 second) GROUP BY Country HAVING COUNT(InvoiceNo) >= 1; \n",
    "\n",
    "# something's stopping create table?\n",
    "\n",
    "SELECT * FROM invoices_country_count EMIT CHANGES;\n",
    "```\n",
    "\n",
    "# gitbash 4\n",
    "\n",
    "```\n",
    "kafka-console-consumer --bootstrap-server 192.168.93.128:9092 --topic USERS_FEMALE --from-beginning \"\n",
    "\n",
    "kafka-console-consumer --bootstrap-server 192.168.93.128:9092 --topic PAGEVIEWS_REGION_TABLE --from-beginning \"\n",
    "```\n",
    "\n",
    "### on ksql prompt \n",
    "\n",
    "List the persisted queries\n",
    "```\n",
    "SHOW QUERIES;\n",
    "```\n",
    "List of queries with Query ID\n",
    "\n",
    "\n",
    "\n",
    "C***** - QUERY ID\n",
    "\n",
    "```\n",
    "EXPLAIN CTAS_PAGEVIEWS_REGION_TABLE_3; \n",
    "\n",
    "```\n",
    "\n",
    "To stop the query / once stopped, cannot be restarted, need to run fresh query\n",
    "```\n",
    "TERMINATE  CTAS_PAGEVIEWS_REGION_TABLE_3;\n",
    "\n",
    "DROP STREAM  users_male; \n",
    "\n",
    "\n",
    "DROP TABLE  pageviews_region;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# STREAMS (Avro)\n",
    " \n",
    "\n",
    "# KSQL \n",
    "\n",
    "## Preparation\n",
    "\n",
    "Launch Gitbash Shell 1\n",
    "\n",
    "below produce the records every 5 seconds, write to topic users\n",
    "\n",
    "```\n",
    "ksql-datagen quickstart=users format=avro topic=users maxInterval=5000\n",
    "```\n",
    "\n",
    "http://192.168.93.128:8081/subjects/users-value/versions/1\n",
    "\n",
    "Launch Gitbash Shell 2\n",
    "\n",
    "below produce the records every 5 seconds, write to topic pageviews\n",
    "\n",
    "```\n",
    "ksql-datagen quickstart=pageviews format=avro topic=pageviews maxInterval=5000\n",
    "```\n",
    "\n",
    "http://192.168.93.128:8081/subjects/pageviews-value/versions/1\n",
    "\n",
    "# KSQL Shell /Gitbash/SSH\n",
    "```\n",
    "ksql \n",
    "```\n",
    "prompt will appear\n",
    "\n",
    "```\n",
    "SHOW STREAMS;\n",
    "\n",
    "SHOW TABLES;\n",
    "\n",
    "CREATE STREAM users_stream (userid varchar, regionid varchar, gender varchar) WITH (kafka_topic='users', value_format='AVRO');\n",
    "\n",
    "SHOW STREAMS;\n",
    "\n",
    "DESCRIBE users_stream;\n",
    "```\n",
    "\n",
    "NON_PERSISTED QUERIES [Means, the output/result is not stored into KAfka Brokers]\n",
    "\n",
    "```\n",
    "select userid, regionid, gender from users_stream;\n",
    "\n",
    "select userid, regionid, gender from users_stream where gender='FEMALE';\n",
    "\n",
    "select userid, regionid, gender from users_stream where gender='MALE';\n",
    "```\n",
    "PERSISTED QUERIES [CREATE STREAM AS ] results written to Kafka\n",
    "Will be runnign automatically, need to use TERMINATE command to stop them\n",
    "\n",
    "```\n",
    "CREATE STREAM users_female AS SELECT userid AS userid, regionid FROM users_stream where gender='FEMALE';\n",
    "\n",
    "CREATE STREAM users_male AS SELECT userid AS userid, regionid FROM users_stream where gender='MALE';\n",
    "\n",
    "\n",
    " CREATE STREAM pageviews_stream (userid varchar, pageid varchar) WITH (kafka_topic='pageviews', value_format='AVRO');\n",
    " \n",
    " select * from pageviews_stream;\n",
    "\n",
    "```\n",
    "JOIN\n",
    "\n",
    "```\n",
    "CREATE STREAM user_pageviews_enriched_stream AS SELECT users_stream.userid AS userid, pageid, regionid, gender FROM pageviews_stream LEFT JOIN users_stream WITHIN 1 HOURS ON pageviews_stream.userid = users_stream.userid;\n",
    "\n",
    "select * from user_pageviews_enriched_stream;\n",
    "```\n",
    "\n",
    "Ctrl +C to exit\n",
    "```\n",
    "CREATE TABLE pageviews_region_table WITH (VALUE_FORMAT='AVRO') AS SELECT gender, regionid, COUNT() AS numusers FROM user_pageviews_enriched_stream WINDOW TUMBLING (size 60 second) GROUP BY gender, regionid HAVING COUNT() >= 1;\n",
    "\n",
    "select * from pageviews_region_table;\n",
    "\n",
    "\n",
    "kafka-avro-console-consumer --bootstrap-server 192.168.93.128:9092 --topic USERS_FEMALE --from-beginning --property schema.registry.url=\"http://192.168.93.128:8081\"\n",
    "\n",
    "kafka-avro-console-consumer --bootstrap-server 192.168.93.128:9092 --topic PAGEVIEWS_REGION_TABLE --from-beginning --property schema.registry.url=\"http://192.168.93.128:8081\"\n",
    "```\n",
    "\n",
    "List the persisted queries\n",
    "```\n",
    "SHOW QUERIES;\n",
    "```\n",
    "List of queries with Query ID\n",
    "\n",
    "\n",
    "\n",
    "C***** - QUERY ID\n",
    "\n",
    "```\n",
    "EXPLAIN CTAS_PAGEVIEWS_REGION_TABLE_3; \n",
    "\n",
    "```\n",
    "\n",
    "To stop the query / once stopped, cannot be restarted, need to run fresh query\n",
    "```\n",
    "TERMINATE  CTAS_PAGEVIEWS_REGION_TABLE_3;\n",
    "\n",
    "DROP STREAM  users_male; \n",
    "\n",
    "\n",
    "DROP TABLE  pageviews_region;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kineses Stream (no Firehose) -> Lambda (+trigger) -> DynamoDB\n",
    "https://github.com/nodesense/cts-aws-spark-april-2021/tree/main/lambda\n",
    "\n",
    "Once kinesis stream, lambda, trigger, dunamodb, setup is done, run..\n",
    "<br>Kinesis-Invoice-Producer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presto\n",
    "https://github.com/nodesense/cts-aws-spark-april-2021/tree/main/presto\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day28\">[Day 28](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Trainer\n",
    "Lambda for Kinesis stream:\n",
    "\n",
    "Test code:\n",
    "```\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"kinesis\": {\n",
    "        \"data\": \"eyJJbnZvaWNlTm8iOiA1MzkyNjcsICJTdG9ja0NvZGUiOiAiODQ0MDZFIiwgIlF1YW50aXR5IjogMiwgIkRlc2NyaXB0aW9uIjogIlRPRE8iLCAiSW52b2ljZURhdGUiOiAiMDYvMDQvMjAyMSAwMTowMyIsICJVbml0UHJpY2UiOiA0LjAsICJDdXN0b21lcklEIjogMTc4NTAsICJDb3VudHJ5IjogIkxWIn0=\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "Lambda\n",
    "```\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "   print (\"Event\", event)\n",
    "   client = boto3.resource(\"dynamodb\")\n",
    "   \n",
    "   invoiceTable = client.Table(\"invoices\")\n",
    "   \n",
    "   for record in event[\"Records\"]:\n",
    "       kinesis = record[\"kinesis\"]\n",
    "       encoded_payload = kinesis[\"data\"] # base64 string\n",
    "       # json_payload is the one actually send by kinesis producer\n",
    "       json_payload = base64.b64decode(encoded_payload) # base64 to json string\n",
    "       print(\"decoded \", json_payload)\n",
    "       invoice = json.loads(json_payload) # load python object from json string\n",
    "         \n",
    "       invoice['InvoiceNo'] = str(invoice['InvoiceNo'])\n",
    "       invoice[\"Amount\"] = invoice[\"Quantity\"] *   invoice[\"UnitPrice\"] \n",
    "       invoice['Amount'] = str(invoice['Amount'])\n",
    "       print(\"writing to dynamo\", invoice)\n",
    "       \n",
    "       result = invoiceTable.put_item(Item={'InvoiceNo': invoice['InvoiceNo'], 'Amount':  invoice['Amount'] })\n",
    "       print(result)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Use-case`\n",
    "\n",
    "### Kinesis stream\n",
    "Create `ds-order` kinesis stream\n",
    "### dynamoDB\n",
    "Create `order` table with `CountryID` as partition key.\n",
    "### [Lambda: Test code](https://www.base64encode.org/)\n",
    "```python\n",
    "{\n",
    "  \"Records\": [\n",
    "    {\n",
    "      \"kinesis\": {\n",
    "        \"data\": \"eyJPcmRlck5vIjogIjEyODkyNyIsICJDdXN0b21lcklEIjogIjM0NTU2IiwgIkNvdW50cnlJRCI6ICJJVCIsICJTdG9ja0NvZGUiOiAiODUxMjNBIiwgIlF1YW50aXR5IjogIjYiLCAiVW5pdFByaWNlIjogIjExLjI5IiwgIkRlc2NyaXB0aW9uIjogImxhdGVyLi4iLCAiT3JkZXJEYXRlIjogIjA2LzA1LzIwMjEgMDQ6NDgiLCAiUGF5bWVudE1vZGUiOiAiRG9nZWNvaW4ifQ==\"\n",
    "      }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### Lambda: lambda_function.py\n",
    "```python\n",
    "import json\n",
    "import boto3\n",
    "import base64\n",
    "from decimal import Decimal\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "   print (\"Event\", event)\n",
    "   client = boto3.resource(\"dynamodb\")\n",
    "   \n",
    "   orderTable = client.Table(\"order\")\n",
    "   \n",
    "   for record in event[\"Records\"]:\n",
    "       kinesis = record[\"kinesis\"]\n",
    "       encoded_payload = kinesis[\"data\"]  # base64 string\n",
    "\n",
    "       # json_payload is sent by kinesis producer\n",
    "       json_payload = base64.b64decode(encoded_payload) # base64 to json string\n",
    "       # print()\n",
    "       # print(\"decoded ->\", json_payload)\n",
    "\n",
    "       order = json.loads(json_payload)  # load python object from json string\n",
    "\n",
    "       order['Quantity'] = Decimal(order['Quantity'])\n",
    "       order['UnitPrice'] = Decimal(order['UnitPrice'])\n",
    "       order[\"Amount\"] = order[\"Quantity\"] * order[\"UnitPrice\"]\n",
    "       # print()\n",
    "       # print(\"writing to dynamo ->\", order)\n",
    "       \n",
    "       result = orderTable.put_item(Item=order)\n",
    "\n",
    "       # print(\"result ->\", result)\n",
    "       # print()\n",
    "```\n",
    "Test code with print() statements enabled.\n",
    "### Lambda: enable trigger\n",
    "### Kinesis-Order-Producer.ipynb\n",
    "Run to generate stream data.\n",
    "\n",
    "Check dynamoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"handy-cmds\">[Handy Commands](#root)</a>\n",
    "```bash\n",
    "telnet localhost < port >\n",
    "\n",
    "/etc/hosts  # DNS entry\n",
    "\n",
    "conda env remove --name base\n",
    "\n",
    "# Deactivates (base) environment\n",
    "conda config --set auto_activate_base false\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# In RDD DF, addressing a column with pyspark.sql.functions.col(column_name) will make it independent of the DF\n",
    "pyspark.sql.functions.col()\n",
    "```\n",
    "\n",
    "# <a id=\"references\">[References](#root)</a>\n",
    "[Why Hadoop?](https://www.slideshare.net/ApacheApex/introduction-to-hadoop-60884936?from_action=save)\n",
    "<br>[Movielens Dataset](https://files.grouplens.org/datasets/movielens/ml-latest-small.zip)\n",
    "<br>[pyspark Cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf)\n",
    "<br>[Complete training material](https://github.com/nodesense/cts-aws-spark-april-2021)\n",
    "<br>[PySpark SQL types](https://spark.apache.org/docs/2.1.1/api/python/_modules/pyspark/sql/types.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
