{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"root\"></root>\n",
    "__| [DAY 1](#day1)__\n",
    "Python (`List`, `Tuple`, `Dictionary`)<br>\n",
    "__| [DAY 2](#day2)__\n",
    "Python (`while..else`, `Strings`, String `formatting` (History), `Classes`)<br>\n",
    "__| [DAY 3](#day3)__\n",
    "Python Class variables (`static`), Class `Inheritance`, Scopes, `Set`, `Exceptions`, `DateTime`, `Math`, `Random`_<br>\n",
    "__| [DAY 4](#day4)__\n",
    "File Handling (with `context manager`), List `Comprehension`, `Spark` Installation on Windows<br>\n",
    "__| [DAY 5](#day5)__\n",
    "`RDD`s, Hadoop `MapReduce`, `MapReduce vs Spark`, Running with `spark-submit`, Linux `basic command-line commands`<br>\n",
    "__| [DAY 6](#day6)__\n",
    "JDK+spark+Hadoop Installation on Ubuntu via SSH!<br>\n",
    "__| [DAY 7](#day7)__\n",
    "Reconfig `core-site.xml` & `hdfs-site.xml` (enable read/write from/to HDFS from pyspark), Python-Spark-Jupyter Integration (working from notebook)<br>\n",
    "__| [DAY 8](#day8)__\n",
    "`YARN` Fundamentals, Driver Executor Jobs Stages Tasks, RDD Partitioning<br>\n",
    "__| [DAY 9](#day9)__\n",
    "Broadcast, Accumulator, Caching/Persisting<br>\n",
    "__| [DAY 10](#day10)__\n",
    "Hive, Databricks community (account)<br>\n",
    "__| [DAY 11](#day11)__\n",
    "Spark DataFrames (and its APIs)<br>\n",
    "__| [DAY 12](#day12)__\n",
    "Spark Cluster (Create Master | Workers | assign Workers to Driver), `spark-config`<br>\n",
    "__| [DAY 13](#day13)__\n",
    "RDD DF JOINs<br>\n",
    "__| [DAY 14](#day14)__\n",
    "Spark SQL<br>\n",
    "__| [DAY 15](#day15)__\n",
    ">Running Hive's `meta-server`<br>\n",
    ">Interface PostgreSQL with Hive<br>\n",
    ">Running ThriftServer (for web/jdbc/odbc interfaces)<br>\n",
    ">Connect hive using JDBC interface<br>\n",
    "\n",
    "__| [DAY 16](#day16)__\n",
    "`AWS EC2` | `AWS RDS` (PostgreSQL)<br>\n",
    "__| [DAY 17](#day17)__\n",
    "`Apache Kafka` (Zookeeper, Broker, Producer, Consumer) | RDS (PostgreSQL)<br>\n",
    "__| [DAY 18](#day18)__\n",
    ">`TOPIC` with multiple-partitions<br>\n",
    ">pub-sub with multiple partitions (run with/out key)<br>\n",
    ">`OFFSET`<br>\n",
    ">TOPIC (create/delete/alter)<br>\n",
    ">`Kafka-Python` (ConsumerGroup)<br>\n",
    "\n",
    "__| [References](#references)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day1\">[DAY 1](#root)</a>\n",
    "\n",
    "## List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers = [10, 20, 30, 40, 50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30, 40]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del numbers[-1]\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[20, 30, 40]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.remove(10)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[20, 30]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(numbers.pop())\n",
    "numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10, 20, 30]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numbers.insert(0, 10)\n",
    "numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## TUPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tuple"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ('python',)\n",
    "type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## DICTIONARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {}\n",
    "d['a'] = 1\n",
    "d['b'] = 2\n",
    "\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d.clear()\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## FUNCTION `*args`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome .veena ..veenas ...veenaas\n"
     ]
    }
   ],
   "source": [
    "def special_greeting(message, *names):\n",
    "    print(message, *names)\n",
    "    \n",
    "special_greeting('Welcome', '.veena', '..veenas' , '...veenaas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome veena veenas veenaas\n"
     ]
    }
   ],
   "source": [
    "def special_greeting(message, *names):\n",
    "    print(message, *names)\n",
    "    \n",
    "special_greeting('Welcome', *['veena', 'veenas' , 'veenaas'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day2\">[DAY 2](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `while`..`else`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "Final value of i = 6\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "\n",
    "while i < 6:\n",
    "    print(i)\n",
    "    i += 1\n",
    "else:\n",
    "    print(f\"Final value of i = {i}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to `while`, `for` loops have `else` clause."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s_multiline = \"\"\"string\"\"\"\n",
    "type(s_multiline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>In Windows systems, strings on new line always end in `\\r\\n`.\n",
    "<br>In Linux systems, strings always end only in `\\n`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\s = space\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\s = space\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t = tab\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\t = tab\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Line 1 Line 2 Line 3\n",
      "\\ = line continuation\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Line 1 \\\n",
    "Line 2 \\\n",
    "Line 3\\n\\\n",
    "\\\\ = line continuation\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome\tto\tPython\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome\\tto\\tPython\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second `\\t` always adds extra space.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unicode strings - TO BE DISCUSSED LATER\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Unicode strings - TO BE DISCUSSED LATER\", end='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Earthlingss    \n",
      "False False\n",
      "1\n",
      "2 -1\n",
      "2\n",
      "earthlingss\n",
      "earthlingss\n"
     ]
    }
   ],
   "source": [
    "s = \" earthlingss    \"\n",
    "print(s.title())\n",
    "print(s.startswith('Ea'), s.startswith('ea'))\n",
    "print(s.count('a'))\n",
    "\n",
    "# User find over index (.index() errors if match not found while .find() returns -1 when not found)\n",
    "print(s.find('a'), s.find('H'))\n",
    "print(s.index('a'))\n",
    "\n",
    "print(s.lstrip().rstrip())\n",
    "\n",
    "print(s.lstrip().rstrip().zfill(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0000000123\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('123'.zfill(10), end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### _String formatting (History)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Veena is 18 years old.\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old. As she's 18 years old, she can vote!\n",
      "Veena is 18 years old, and she can vote!\n",
      "Veena is 18 years old, and she can vote!\n"
     ]
    }
   ],
   "source": [
    "name, age = 'Veena', 18\n",
    "\n",
    "print(\"%s is %d years old.\" % (name, age))\n",
    "print(\"{} is {} years old. As she's {} years old, she can vote!\".format(name, age, age))\n",
    "print(\"{0} is {1} years old. As she's {1} years old, she can vote!\".format(name, age))\n",
    "print(\"{name} is {age} years old. As she's {age} years old, she can vote!\".format(name=name, age=age))\n",
    "print(f\"{name} is {age} years old, and she can vote!\")\n",
    "print(F\"{name} is {age} years old, and she can vote!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joe is 20 years old, and he can vote!\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'Joe'} is {age+2} years old, and he can vote!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I know just enough about Classes and Objects in Python, but I'll still try something!\n"
     ]
    }
   ],
   "source": [
    "print(\"I know just enough about Classes and Objects in Python, but I'll still try something!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Car:\n",
    "    \"\"\"Simple Car Model\"\"\"\n",
    "    __slots__ = ('brand', 'model', 'year', 'color')\n",
    "    \n",
    "    def __init__(self, brand, model, year, color):\n",
    "        self.brand = brand\n",
    "        self.model = model\n",
    "        self.year = year\n",
    "        self.color = color\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.brand}/{self.model}/{self.year}/{self.color}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} {self.__str__()!r}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Honda/Civic/2014/Black\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Car 'Honda/Civic/2014/Black'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "honda = Car('Honda', 'Civic', 2014, 'Black')\n",
    "\n",
    "print(honda)\n",
    "honda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Order is (Amount:200, Discount:10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Order (Amount:200, Discount:10)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Order:\n",
    "    \"\"\"Simple Order!\"\"\"\n",
    "    \n",
    "    def __init__(self, amount, discount=0):\n",
    "        self.amount = amount\n",
    "        self.discount = discount\n",
    "        \n",
    "    def calculate(self):\n",
    "        return self.amount - (self.amount)*(self.discount/100)\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Your {self.__class__.__name__} is (Amount:{self.amount}, Discount:{self.discount})\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} (Amount:{self.amount}, Discount:{self.discount})\"\n",
    "    \n",
    "order = Order(200, 10)\n",
    "print(order)\n",
    "order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your Order is (Amount:500, Discount:10) Grand Total: 450.0\n",
      "Your Order is (Amount:1000, Discount:20) Grand Total: 800.0\n",
      "Your Order is (Amount:100, Discount:5) Grand Total: 95.0\n"
     ]
    }
   ],
   "source": [
    "orders = [(500, 10), (1000, 20), (100, 5)]\n",
    "\n",
    "for order in orders:\n",
    "    o = Order(*order)\n",
    "    print(o, f\"Grand Total: {o.calculate()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day3\">[DAY 3](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class variables (static variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Cognizant, Balance: 140,000,000,000,000\n",
      "Welcome! Tesla, Balance: 1,000,000,000,000,000,000,000\n",
      "\n",
      "CapitalOne, You have 2 loyal customers!\n"
     ]
    }
   ],
   "source": [
    "class CapitalOne:\n",
    "    \"\"\" Static variables (other languages) <==> Class variablea (Python)\n",
    "    \"\"\"\n",
    "    _customers = 0\n",
    "    __slots__ = ('__name', '__balance')\n",
    "    \n",
    "    def __init__(self, name, balance):\n",
    "        self.__name = name\n",
    "        self.__balance = balance\n",
    "        self._customer_add()\n",
    "        \n",
    "    def _customer_add(self):\n",
    "        CapitalOne._customers += 1\n",
    "        \n",
    "    @classmethod\n",
    "    def customers(cls):\n",
    "        return f\"{cls.__name__}, You have {cls._customers} loyal customer{'s' if cls._customers > 1 else ''}!\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.info()} {self.__name}, Balance: {self.__balance:,}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__} (Customer: {self.__name}, Balance: {self.__balance:,})\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def info():\n",
    "        \"\"\"staticmethod is a class method that doesn't need a cls reference\"\"\"\n",
    "        return \"Welcome!\"\n",
    "\n",
    "\n",
    "cognizant = CapitalOne('Cognizant', 140_000_000_000_000)\n",
    "tesla = CapitalOne('Tesla', 1000_000_000_000_000_000_000)\n",
    "\n",
    "print(cognizant)\n",
    "print(tesla)\n",
    "print()\n",
    "print(CapitalOne.customers())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Class Inheritance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome! Cognizant, Balance: 140,000,000,000,000\n",
      "Welcome! Tesla, Balance: 1,000,000,000,000,000,000,000\n",
      "\n",
      "BankOfAmerica, You have 2 loyal customers!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Welcome!, Tesla,\n",
       "          You're with Bank Of America,\n",
       "          Your balance: 1,000,000,000,000,000,000,000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Bank:\n",
    "    \"\"\"Banking Base class\n",
    "    \"\"\"\n",
    "    name = \"Set your name\"\n",
    "    address = \"Set your address\"\n",
    "    currency = \"Set your currency\"\n",
    "    \n",
    "    def __init__(self, name, address, currency):\n",
    "        Bank.name = name\n",
    "        Bank.address = address\n",
    "        Bank.currency = currency\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{Bank.name}/{Bank.address}/{Bank.currency}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return self.__str__()\n",
    "    \n",
    "    \n",
    "    \n",
    "class BankOfAmerica(Bank):\n",
    "    \"\"\" Static variables (other languages) <==> Class variablea (Python)\n",
    "    \"\"\"\n",
    "    _customers = 0\n",
    "    __slots__ = ('__name', '__balance')\n",
    "    \n",
    "    \n",
    "    def __init__(self, name, balance):\n",
    "        super().__init__(\"Bank Of America\",\n",
    "                         \"Colony 1, USA 12345\",\n",
    "                         \"$\")\n",
    "        self.__name = name\n",
    "        self.__balance = balance\n",
    "        self._customer_add()\n",
    "        \n",
    "    @classmethod\n",
    "    def _customer_add(cls):\n",
    "        cls._customers += 1\n",
    "        \n",
    "    @classmethod\n",
    "    def customers(cls):\n",
    "        return f\"{cls.__name__}, You have {cls._customers} loyal customer{'s' if cls._customers > 1 else ''}!\"\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.info()} {self.__name}, Balance: {self.__balance:,}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"\"\"{self.info()}, {self.__name},\n",
    "          You're with {super().name},\n",
    "          Your balance: {self.__balance:,})\"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def info():\n",
    "        \"\"\"staticmethod is a class method that doesn't need a cls reference\n",
    "        \"\"\"\n",
    "        return \"Welcome!\"\n",
    "\n",
    "\n",
    "cognizant = BankOfAmerica('Cognizant', 140_000_000_000_000)\n",
    "tesla = BankOfAmerica('Tesla', 1000_000_000_000_000_000_000)\n",
    "\n",
    "print(cognizant)\n",
    "print(tesla)\n",
    "print()\n",
    "print(BankOfAmerica.customers())\n",
    "tesla\n",
    "\n",
    "\n",
    "# del BankOfAmerica\n",
    "# del Bank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`isinstance()`, `issubclass()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "isinstance(tesla, BankOfAmerica) ? True\n",
      "isinstance(tesla, Bank) ? True\n",
      "\n",
      "issubclass(BankOfAmerica, Bank) ? True\n",
      "issubclass(Bank, BankOfAmerica) ? False\n"
     ]
    }
   ],
   "source": [
    "print()\n",
    "print(f\"isinstance(tesla, BankOfAmerica) ? {isinstance(tesla, BankOfAmerica)}\")\n",
    "print(f\"isinstance(tesla, Bank) ? {isinstance(tesla, Bank)}\")\n",
    "print()\n",
    "print(f\"issubclass(BankOfAmerica, Bank) ? {issubclass(BankOfAmerica, Bank)}\")\n",
    "print(f\"issubclass(Bank, BankOfAmerica) ? {issubclass(Bank, BankOfAmerica)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "global\n",
      "changed in local\n"
     ]
    }
   ],
   "source": [
    "# globals\n",
    "name = \"global\"\n",
    "\n",
    "def change_scope():\n",
    "    global name\n",
    "    name = \"changed in local\"\n",
    "\n",
    "\n",
    "print(name)\n",
    "change_scope()    \n",
    "print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language = {'Python', 'SQL', 'Java', 'JS', 'Perl'}\n",
    "\n",
    "language.add('NoSQL')\n",
    "language.remove('Python')  # Errors, if not found\n",
    "language.discard('JS') # Silent return, if item not found\n",
    "\n",
    "language.clear()  # Empty the Set\n",
    "language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Exception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Oh no!\n",
      "Cleaning up!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(1/1)\n",
    "except:\n",
    "    print(\"Catch me if you can!\")\n",
    "else:\n",
    "    print(\"Oh no!\")\n",
    "finally:\n",
    "    print(\"Cleaning up!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catch me if you can!\n",
      "Cleaning up!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(1/0)\n",
    "except:\n",
    "    print(\"Catch me if you can!\")\n",
    "else:\n",
    "    print(\"Oh no!\")\n",
    "finally:\n",
    "    print(\"Cleaning up!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<attribute 'args' of 'BaseException' objects>\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raise TypeError(\"I'm trying something new\")\n",
    "except TypeError:\n",
    "    print(TypeError.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generic Exception\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    raise Exception(\"I'm not being specific!\")\n",
    "except Exception:\n",
    "    print(\"Generic Exception\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catch me if you can! <class '__main__.CustomError'>\n"
     ]
    }
   ],
   "source": [
    "class CustomError(Exception):\n",
    "    def __init__(self, message):\n",
    "        self.message = message\n",
    "\n",
    "\n",
    "try:\n",
    "#     raise\n",
    "    raise CustomError(\"Catch me if you can!\")\n",
    "except CustomError as e:\n",
    "    print(e, type(e))\n",
    "except Exception as e:\n",
    "    print(e, type(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## DateTime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-30 23:53:35.183373\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "\n",
    "today = dt.datetime.now()\n",
    "print(today)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Math, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2335600547204243\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "print(random.random())\n",
    "\n",
    "\n",
    "del random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day4\">[DAY 4](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is A88C-3222\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\veena\\data\n",
      "\n",
      "04/30/2021  09:02 PM    <DIR>          .\n",
      "04/30/2021  09:02 PM    <DIR>          ..\n",
      "04/30/2021  09:02 PM            38,049 all_us_counties.csv\n",
      "04/30/2021  09:00 PM               656 all_us_states.csv\n",
      "04/30/2021  09:02 PM         2,072,181 all_us_zipcodes.csv\n",
      "               3 File(s)      2,110,886 bytes\n",
      "               2 Dir(s)  493,276,196,864 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[State: abbr = name, State: AL = Alabama, State: AK = Alaska, State: AZ = Arizona, State: AR = Arkansas, State: CA = California, State: CO = Colorado, State: CT = Connecticut, State: DE = Delaware, State: DC = District of Columbia, State: FL = Florida, State: GA = Georgia, State: HI = Hawaii, State: ID = Idaho, State: IL = Illinois, State: IN = Indiana, State: IA = Iowa, State: KS = Kansas, State: KY = Kentucky, State: LA = Louisiana, State: ME = Maine, State: MD = Maryland, State: MA = Massachusetts, State: MI = Michigan, State: MN = Minnesota, State: MS = Mississippi, State: MO = Missouri, State: MT = Montana, State: NE = Nebraska, State: NV = Nevada, State: NH = New Hampshire, State: NJ = New Jersey, State: NM = New Mexico, State: NY = New York, State: NC = North Carolina, State: ND = North Dakota, State: OH = Ohio, State: OK = Oklahoma, State: OR = Oregon, State: PA = Pennsylvania, State: RI = Rhode Island, State: SC = South Carolina, State: SD = South Dakota, State: TN = Tennessee, State: TX = Texas, State: UT = Utah, State: VT = Vermont, State: VA = Virginia, State: WA = Washington, State: WV = West Virginia, State: WI = Wisconsin, State: WY = Wyoming]\n"
     ]
    }
   ],
   "source": [
    "class State:\n",
    "    def __init__(self, code, name):\n",
    "        self.code = code\n",
    "        self.name = name\n",
    "        \n",
    "    def __str__(self):\n",
    "        return f\"{self.code} = {self.name}\"\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"{self.__class__.__name__}: {self.__str__()}\"\n",
    "\n",
    "\n",
    "states = []\n",
    "with open(\"data/all_us_states.csv\") as f:\n",
    "    states.extend(State(*state_info.strip().split(',')) for state_info in f)\n",
    "    # .readlines() reads all of file into memory. NO! Simply iterate over file handle.\n",
    "\n",
    "print(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## List Comprehension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 7, 9]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "odds = [o for o in range(10) if o%2 if o>4]\n",
    "odds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Spark\n",
    "\n",
    "__Requirements:__\n",
    ">`Java 1.8 only` (a.k.a Java 8) (as Spark works only on 1.8)\n",
    "<br>Oracle Java (stable, performant, no support for unpaid version)\n",
    "<br>Open source community (`AdaptJava JDK 8`) https://adoptopenjdk.net/\n",
    "<br>- JDK (compiler+JRE)\n",
    "<br>- JRE (no compiler, just a runtime environment)\n",
    "<br>- Language |Statically-typed Compiled language\n",
    "\n",
    ">`Spark 2.x only`\n",
    "<br>Supports: Java, Scala (largely writen in Java), Python, R, SQL\n",
    "\n",
    ">`Hadoop`\n",
    "<br>* HDFS/Hive/YARN/Kafka\n",
    "\n",
    "Look out for the installation of the above (in order) with necessary system settings on Windows below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "__Installation (in dependency order):__\n",
    ">1. Install `python 3.7` only (as Spark2.x works best with Python 3.7)\n",
    "<br>To install 3.7 on conda:\n",
    "<br>`$conda create -n py37 anaconda=2020.07 python=3.7`\n",
    "<br>`$conda activate py37`\n",
    "<br>`$conda deactivate`\n",
    "\n",
    ">2. Install `AdaptJDK 8` (check `JAVA_HOME` (system variable) with `echo %JAVA_HOME%`, else set)\n",
    "\n",
    ">3. Install `Spark 2.7 only` (google spark download)\n",
    "<br>Set `SPARK_HOME` (system variable) to the spark installation directory\n",
    "<br>Set `Path` (user/system variable) to Spark's bin directory\n",
    "<br>Launch `spark-shell` (allow access if prompted)\n",
    "<br>- `$spark-shell`\n",
    "<br>Launch `pyspark`\n",
    "<br>- `$pyspark`\n",
    "<br>pyspark will require winutils.\n",
    "\n",
    ">4. Install `winutils` (google `github winutils`) (https://github.com/steveloughran/winutils)\n",
    "<br>Extract Hadoop2.7.x folder\n",
    "<br>Set `HADOOP_HOME` to this directory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day5\">[DAY 5](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ecosystem compatibility:\n",
    "<br>Spark 2.4\n",
    "<br>JDK 8 (1.8)\n",
    "<br>Python 3.7\n",
    "<br>Scala 2.11\n",
    "<br>Hadoop 2.x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Spark:__\n",
    "<br>Apache Spark is a general-purpose, distributed, fast engine for large-scale data-processing.\n",
    "\n",
    "\n",
    "__Spark Core Ideas:__\n",
    "<br>1. Data Processing Framework\n",
    "<br>2. Expressive computing system (not limited to MapReduce model)\n",
    "<br>3. Facilitates faster memory (no saving disks + caching)\n",
    "<br>In-memory processing (instead of writing to disks) [DDR-Double Data Rate]\n",
    "\n",
    "__Apache Spark__\n",
    "<br>Runs on JVM.\n",
    "<br>Written using Scala (Scala is written entirely in Java)\n",
    "<br>Faster adaption into Big Data world.\n",
    "<br>Supports Python, Java, R, Scala, SQL.\n",
    "\n",
    "Spark is more developer focused (not focused on end-user query processing engines)\n",
    "<br>(insert picture here)\n",
    "<br>Java + JVM Knowledge required. Having `Scala` knowledge is grand.\n",
    "\n",
    "__Hadoop MapReduce__\n",
    "<br>Offers `Parallelization`\n",
    "<br>`Scalable`\n",
    "<br>`Fault-tolerant` (via redundancy)\n",
    "<br>A programming model for distributed proessing of large datasets.\n",
    "<br>basic unit of information is a (key, value) pair\n",
    "<br>Map-function + Reduce-function with Shuffling in-between\n",
    "<br>Mapper brings structure to unstructured data\n",
    "<br>Shuffler consolidates output from Mapper (moved the data from Mapper, and passes it to Reducer)\n",
    "<br>Reducer outputs summarized values from Shuffler\n",
    "<br>Hadoop's MappReduce is SLOW because there's lot of I/O operations & no in-memory processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SparkContext (sc)\n",
    "\n",
    "\n",
    "__`$ pyspark`__\n",
    "\n",
    "__`>>> sc`__\n",
    "<br><SparkContext master=local[*] appName=PySparkShell>\n",
    "\n",
    "__`>>> spark`__\n",
    "<br><pyspark.sql.session.SparkSession object at 0x000001BBF7C76648>\n",
    "\n",
    "`Ctrl+l` to clear the screen\n",
    "\n",
    "__`>>> data = [10, 15, 20, 25, 30, 35, 40, 45, 50]`__\n",
    "\n",
    "__`>>> type(data)`__\n",
    "<br><class 'list'>\n",
    "\n",
    "__`>>> data_rdd = sc.parallelize(data)`__\n",
    "<br># This will load data into a Spark partition._\n",
    "\n",
    "__`>>> data_rdd.count`__\n",
    "<br>data_rdd.count(               data_rdd.countApprox(         data_rdd.countApproxDistinct( data_rdd.countByKey(          data_rdd.countByValue(\n",
    "\n",
    "__`>>> data_rdd.count()`__\n",
    "<br>9\n",
    "\n",
    "__`>>>  type(data_rdd)`__\n",
    "<br><class 'pyspark.rdd.RDD'>\n",
    "\n",
    "__`>>> data_rdd.filter(lambda x: x*x)`__\n",
    "<br>PythonRDD[2] at RDD at PythonRDD.scala:53\n",
    "\n",
    "__`>>> data_squares = data_rdd.filter(lambda x: x * x)`   # .filter() is a Transformation (not an Action)__\n",
    "\n",
    "__`>>> data_squares`__\n",
    "<br>PythonRDD[3] at RDD at PythonRDD.scala:53\n",
    "\n",
    "__`>>> data_squares.collect()`  # .collect(), .sum() are all Actions (not Transformation)__\n",
    "<br>[10, 15, 20, 25, 30, 35, 40, 45, 50]\n",
    "\n",
    "__`>>> data_squares.sum()`__\n",
    "<br>270\n",
    "\n",
    "__`>>> data_`__\n",
    "<br>data_rdd     data_squares\n",
    "\n",
    "__`>>> data_rdd`__\n",
    "<br>ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:195  # .scala shows that data_rdd is an internal Java object\n",
    "\n",
    "__`>>> data_100times = data_rdd.map(lambda x: x * 100)`__\n",
    "\n",
    "__`>>> data_100times.collect()`__\n",
    "<br>[1000, 1500, 2000, 2500, 3000, 3500, 4000, 4500, 5000]\n",
    "\n",
    "__`>>> type(data_100times)`__\n",
    "<br><class 'pyspark.rdd.PipelinedRDD'>\n",
    "\n",
    "__`>>> result = data_100times.collect()`__\n",
    "\n",
    "__`>>> type(result)`__\n",
    "<br><class 'list'>\n",
    "\n",
    "__`>>> def odds(n):`\n",
    "<br>...     `return n % 2`\n",
    "<br>...__\n",
    "\n",
    "__`>>> data_odds = data_rdd.filter(odds)`__\n",
    "\n",
    "__`>>> data_odds.collect()`__\n",
    "<br>[15, 25, 35, 45]\n",
    "\n",
    "__`>>> sc.`__\n",
    "```python\n",
    "sc.PACKAGE_EXTENSIONS   sc.cancelAllJobs(       sc.getOrCreate(         sc.pythonExec           sc.setLocalProperty(    sc.textFile(\n",
    "sc.accumulator(         sc.cancelJobGroup(      sc.hadoopFile(          sc.pythonVer            sc.setLogLevel(         sc.uiWebUrl\n",
    "sc.addFile(             sc.defaultMinPartitions sc.hadoopRDD(           sc.range(               sc.setSystemProperty(   sc.union(\n",
    "sc.addPyFile(           sc.defaultParallelism   sc.master               sc.runJob(              sc.show_profiles(       sc.version\n",
    "sc.appName              sc.dump_profiles(       sc.newAPIHadoopFile(    sc.sequenceFile(        sc.sparkHome            sc.wholeTextFiles(\n",
    "sc.applicationId        sc.emptyRDD(            sc.newAPIHadoopRDD(     sc.serializer           sc.sparkUser(\n",
    "sc.binaryFiles(         sc.environment          sc.parallelize(         sc.setCheckpointDir(    sc.startTime\n",
    "sc.binaryRecords(       sc.getConf(             sc.pickleFile(          sc.setJobDescription(   sc.statusTracker(\n",
    "sc.broadcast(           sc.getLocalProperty(    sc.profiler_collector   sc.setJobGroup(         sc.stop(\n",
    "```\n",
    "__`>>> exit()`__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<br>To run a custom spark file from the command-line:\n",
    "<br>__`$ spark-submit.py spark_hello.py`__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C has no label.\n",
      " Volume Serial Number is A88C-3222\n",
      "\n",
      " Directory of C:\\Users\\Administrator\\veena\\bigdata\n",
      "\n",
      "05/04/2021  12:21 AM    <DIR>          .\n",
      "05/04/2021  12:21 AM    <DIR>          ..\n",
      "05/04/2021  12:28 AM               292 spark_hello.py\n",
      "               1 File(s)            292 bytes\n",
      "               2 Dir(s)  490,375,802,880 bytes free\n"
     ]
    }
   ],
   "source": [
    "!cat ./bigdata/spark_hello.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: Reading Text File\n",
    "`sc.textFile()`  # __TRANSFORMATION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: What's in the Text File?\n",
    "`rdd.collect()`  # __ACTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Spark: rdd.flatMap()\n",
    "```python\n",
    "counts =\n",
    "data_rdd.flatMap(lambda line: line.split(\" \")).\\   # __TRANSFORMATION__\n",
    " map(lambda word: (word, 1)).\\\n",
    " reduceByKey(lambda x, y: x + y)\n",
    "```\n",
    "<br># Each .map().reduceByKey() will operate on its own chunk of data.\n",
    "\n",
    "__rdd.map()__:\n",
    "<br>Returns a new RDD by applying the function to each element of the RDD.\n",
    "<br>Function in map returns only one item.\n",
    "                                             \n",
    "__rdd.flatMap()__:\n",
    "<br>Similar to .map(), returns a new RDD by applying a function to each element of the RDD, but the output is flattened.\n",
    "<br>This means, .flatMap() gets rid of any structure within the RDD.\n",
    "\n",
    "`counts.take(4)`  # Similar to DataFrame.head(4)  # __ACTION__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark: rdd.saveAsTextFile()\n",
    "counts.saveAsTextFile(\"state_count.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# <a id=\"day6\">[DAY 6](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S p a r k - H a d o o p    O n   U b u n t u\n",
    "\n",
    "<br>\n",
    "\n",
    "### Install OpenJDK\n",
    "`$sudo apt install openjdk-8-jdk -y`\n",
    "\n",
    "__Where's Java installed?__\n",
    "<br>`/usr/bin/jvm/java-8-openjdk-amd64`\n",
    "\n",
    "__Set /etc/environment:__\n",
    "<br>__JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64__\n",
    "<br>__JRE_HOME=/usr/lib/jvm/java-8-openjdk-amd64__\n",
    "<br>with `$sudo nano /etc/environment`\n",
    "\n",
    "\n",
    "### Install Spark\n",
    "`$wget https://downloads.apache.org/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz`\n",
    "<br>`tar xf spark-2.4.7-bin-hadoop2.7.tgz`\n",
    "<br>`sudo mv spark-2.4.7-bin-hadoop2.7 /opt`  # `/opt` is the Optional Packages Directory on Linux\n",
    "<br>`sudo chmod 777 /opt/spark-2.4.7-bin-hadoop2.7`  # In production environment, Read+Execute permission to User ONLY!\n",
    "\n",
    "Set /etc/environment:\n",
    "<br>__SPARK_HOME=/opt/spark-2.4.7-bin-hadoop2.7__\n",
    "\n",
    "### Setup PATH for user:\n",
    "```bash\n",
    "cd ~\n",
    "nano .profile  # Similar to User-Profile on Windows\n",
    "# Paste:\n",
    "export SPARK_HOME=/opt/spark-2.4.7-bin-hadoop2.7\n",
    "export PATH=$PATH:$SPARK_HOME/bin:$SPARK_HOME/sbin\n",
    "source .profile\n",
    "```\n",
    "\n",
    "__note__ Spark directory includes a YARN, and this will conflict with Hadoop's YARN.\n",
    "<br>To run YARN command, use $YARN_HOME/bin/yarn instead of plain YARN command.\n",
    "\n",
    "__Install Python__\n",
    "<br>`sudo add-apt-repository ppa:deadsnakes/ppa`  -- Makes repositories/builds available for python installation\n",
    "<br>`sudo apt-get update`  -- Now, update the repositores\n",
    "<br>`sudo apt-get install python3.7`\n",
    "\n",
    "I'm creating my own alias for python3.7 in ~/.bash_aliases:\n",
    "<br>`alias python36=python3`\n",
    "<br>`alias python37=python3.7`\n",
    "<br>`$source .bashrc`\n",
    "\n",
    "\n",
    "### To enable Spark to run pyspark..\n",
    "`sudo ln -s /usr/bin/python3.7 /usr/bin/python`  -- Soft link\n",
    "\n",
    "### Reboot Ubuntu [This is for changes in /etc/environment file to take effect, needs restart]\n",
    "`sudo reboot`\n",
    "<br>`connect via SSH (if using SSH)`\n",
    "<br>Test with `$pyspark`, or\n",
    "<br>Test with `$spark-shell`\n",
    "\n",
    "\n",
    "### Install Hadoop\n",
    "__Download Hadoop Binaries__\n",
    "```bash\n",
    "wget https://archive.apache.org/dist/hadoop/common/hadoop-2.7.7/hadoop-2.7.7.tar.gz\n",
    "tar xf hadoop-2.7.7.tar.gz\n",
    "sudo mv hadoop-2.7.7 /opt\n",
    "```\n",
    "\n",
    "Assign read/write/execute permission for complete hadoop folder. [not recommended for production]\n",
    "```bash\n",
    "sudo chmod 777 /opt/hadoop-2.7.7\n",
    "sudo nano /etc/environment\n",
    "# Paste in /etc/environment:\n",
    "HADOOP_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_INSTALL=/opt/hadoop-2.7.7\n",
    "HADOOP_MAPRED_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_COMMON_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_HDFS_HOME=/opt/hadoop-2.7.7\n",
    "YARN_HOME=/opt/hadoop-2.7.7\n",
    "HADOOP_COMMON_LIB_NATIVE_DIR=/opt/hadoop-2.7.7/lib/native\n",
    "```\n",
    "\n",
    "Update profile (per user) environment (no `sudo` here)\n",
    "```bash\n",
    "nano ~/.profile\n",
    "# Paste below at the end of the file:\n",
    "export HADOOP_HOME=/opt/hadoop-2.7.7\n",
    "export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin\n",
    "# Close existing terminal and open new terminal.\n",
    "```\n",
    "\n",
    "<br>__Backup original Hadoop config files, mapred-site.xml won't be there by default, ignore the error__\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/core-site.xml $HADOOP_HOME/etc/hadoop/core-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/hdfs-site.xml $HADOOP_HOME/etc/hadoop/hdfs-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/mapred-site.xml $HADOOP_HOME/etc/hadoop/mapred-site.xml.original`\n",
    "<br>`mv $HADOOP_HOME/etc/hadoop/yarn-site.xml $HADOOP_HOME/etc/hadoop/yarn-site.xml.original`\n",
    "\n",
    "<br>Truncate the existing content in the conf file [for easy editing, optional]:\n",
    "```bash\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/hdfs-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "cat /dev/null > $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "```\n",
    "\n",
    "<br>__Edit config files:__\n",
    "<br>__-- core-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/core-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/core-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>fs.defaultFS</name>\n",
    "        <value>hdfs://localhost:9000</value> \n",
    "    </property>\n",
    "  \n",
    "    <property>\n",
    "        <name>hadoop.tmp.dir</name>\n",
    "        <value>/data/hdfs</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>hadoop.proxyuser.hive.hosts</name>\n",
    "        <value>*</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>hadoop.proxyuser.hive.groups</name>\n",
    "        <value>*</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- hdfs-site.xml--__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/hdfs-site.xml`\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/hdfs-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>dfs.replication</name>\n",
    "        <value>1</value>\n",
    "    </property>\n",
    "    \n",
    "    <property>\n",
    "        <name>dfs.datanode.use.datanode.hostname</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- mapred-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/mapred-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/mapred-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>mapreduce.framework.name</name>\n",
    "        <value>yarn</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "\n",
    "__-- yarn-site.xml --__\n",
    "```bash\n",
    "sudo nano $HADOOP_HOME/etc/hadoop/yarn-site.xml\n",
    "```\n",
    "Replace with content from https://raw.githubusercontent.com/nodesense/kafka-workshop/master/hadoop/yarn-site.xml\n",
    "<br>or\n",
    "```xml\n",
    "<configuration>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services</name>\n",
    "        <value>mapreduce_shuffle</value>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>\n",
    "        <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
    "    </property>\n",
    "\n",
    "    <property>\n",
    "        <name>yarn.log-aggregation-enable</name>\n",
    "        <value>true</value>\n",
    "    </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "<br>\n",
    "__Restart the linux system once__\n",
    "\n",
    "For running hadoop cluster with ssh, as hadoop user needs ssh permission.\n",
    "\n",
    "Install ssh server if not there\n",
    "`sudo apt install openssh-server` -- installation\n",
    "`sudo systemctl status ssh` -- Check SSH status\n",
    "`sudo systemctl enable ssh` -- enable server if status is down\n",
    "`sudo systemctl start ssh` -- start server\n",
    "`sudo ufw allow ssh` -- Ubuntu Firewall (ufw)\n",
    "`sudo systemctl status ssh` -- Re-check\n",
    "\n",
    "\n",
    "___Hadoop internally uses SSH for communication. So enable Hadoop to use SSH. So, SSH again from within Ubuntu (to white-list ubuntu to be used in Hadoop):___\n",
    "\n",
    "Test if all's ok:\n",
    "<br>`ssh username@ip_address`  -- White-listing Ubuntu to be used in Hadoop\n",
    "<br>`ssh-keygen`  -- To enable password-less authentication\n",
    "<br>`ssh-keyscan localhost,0.0.0.0 > ~/.ssh/known_hosts`\n",
    "<br>`chmod +x $HADOOP_HOME/sbin/start-all.sh`\n",
    "\n",
    "__Prepare HDFS data directories:__\n",
    "<br>`sudo mkdir -p /data/hdfs`\n",
    "<br>`sudo chmod 777 /data/hdfs`\n",
    "\n",
    "__Format NameNode:__\n",
    "<br>`hdfs namenode -format`\n",
    "\n",
    "__Now, start all Hadoop services:__\n",
    "<br>`$HADOOP_HOME/sbin/start-all.sh` -- Note: Spark also has start-all.sh\n",
    "<br>`$HADOOP_HOME/sbin/stop-all.sh`  -- To shutdown (Spark also has one by the same name)\n",
    "\n",
    "__Check with browser:__\n",
    "<br>http://hostname:50070, or\n",
    "<br>http://localhost:50070, or\n",
    "<br>http://localhost:50070/explorer.html#/\n",
    "\n",
    "\n",
    "__YARN__\n",
    "<br>http://localhost:8088/cluster\n",
    "\n",
    "Default port references https://kontext.tech/column/hadoop/265/default-ports-used-by-hadoop-services-hdfs-mapreduce-yarn\n",
    "\n",
    "__Hadoop Commmands__\n",
    "<br>`jps`  -- Similar to Task Manager on Windows (jps = Java Processes)\n",
    "<br>`hdfs dfs -ls /` -- List all HDFS directories \n",
    "<br>`hdfs dfs -ls -R /*.csv`\n",
    "\n",
    "__NOTE__\n",
    "```bash\n",
    "# If shart-all.sh doesn't work normally\n",
    "$HADOOP_HOME/bin/hadoop dfsadmin -safemode leave\n",
    "```\n",
    "\n",
    "__Hadoop Utility commands__\n",
    "<br>`hdfs dfs -du /directory_name` -- Disk usage\n",
    "<br>`hdfs dfs -dus /directory_name` -- Total size of file/directory\n",
    "<br>`hdfs dfs -stat /directory_name` -- Get last modified information\n",
    "<br>`hdfs dfs -stat /path_to_filename` -- Get last modified information\n",
    "\n",
    "<br>`hdfs/bin -mkdir /user`\n",
    "<br>`hdfs/bin -mkdir /user/username`\n",
    "\n",
    "Write the username of your computer. Example:\n",
    "<br>bin/hdfs dfs -mkdir /geeks\n",
    "<br>bin/hdfs dfs -mkdir geeks2\n",
    "\n",
    "Examples:To run sample pi application\n",
    "\n",
    "Number of Maps = 4 Samples per Map = 4\n",
    "\n",
    "hadoop jar $HADOOP_HOME/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.7.jar pi 4 4\n",
    "\n",
    "<br>To list out all the applications\n",
    "```bash\n",
    "yarn application -list -appStates ALL\n",
    "works only when HA enabled, we will discuss in later sessions\n",
    "\n",
    "yarn rmadmin -checkHealth\n",
    "To get application ID use yarn application -list\n",
    "\n",
    "yarn application -status application_XXXYYYZZZKKKK_0002\n",
    "To view logs of application,\n",
    "\n",
    "yarn logs -applicationId application_XXXYYYZZZKKKK_0002\n",
    "yarn application -kill application_XXXYYYZZZKKKK_0002\n",
    "yarn application -list\n",
    "\n",
    "yarn application -list -appStates FINISHED\n",
    "yarn application -list -appStates ALL\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "more https://docs.cloudera.com/runtime/7.0.0/yarn-monitoring-clusters-applications/topics/yarn-use-cli-view-logs-applications.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day7\">[Day 7](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Edit `$HADOOP_HOME/etc/hadoop/core-site.xml` (set `fs.defaultFS` property to `hdfs://192.168.93.128:9000`)\n",
    "<br>This will enable reading/writing of `hdfs://hostip:port/hdfs_paths` from within python.\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>fs.defaultFS</name>\n",
    "    <value>hdfs://192.168.93.128:9000</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "<name>hadoop.tmp.dir</name>\n",
    "<value>/data/hdfs</value>\n",
    "</property>\n",
    "\n",
    "  <property>\n",
    "    <name>hadoop.proxyuser.hive.hosts</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>hadoop.proxyuser.hive.groups</name>\n",
    "    <value>*</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "```\n",
    "<br>\n",
    "\n",
    "### Edit `$HADOOP_HOME/etc/hadoop/hdfs-site.xml` (set `dfs.permissions` property to `false`)\n",
    "```xml\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.permissions</name>\n",
    "    <value>false</value>\n",
    "</property>\n",
    "\n",
    "<property>\n",
    "    <name>dfs.datanode.use.datanode.hostname</name>\n",
    "    <value>true</value>\n",
    "</property>\n",
    "\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrating Python, Spark, Jupyter\n",
    "__2 options:__\n",
    "<br>1. Initialize $SPARK_HOME/findspark library  (NOTE: Not for Production!)\n",
    "<br>`$pip install findspark`\n",
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext()\n",
    "....\n",
    "```\n",
    "<br>2. Configure ENV setting enabling pyspark to launch Jupyter\n",
    "<br>__?__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day8\">[Day 8](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Yarn fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jobs, Stages, Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## RDD Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc = SparkContext(\"local[1]\", \"US-ZIPCODES\")\n",
    "\n",
    "rdd_us_zipcodes = sc.textFile(\"hdfs://192.168.93.128:9000/input/all_us_zipcodes.csv\")\n",
    "\n",
    "# How many partitions currently?\n",
    "print(f\"Partitions : {rdd_us_zipcodes.getNumPartitions()}\")\n",
    "\n",
    "## One way to scale up/down partitioning/chunking data\n",
    "rdd_us_zipcodes = rdd_us_zipcodes.repartition(4)\n",
    "print(f\"Partitions : {rdd_us_zipcodes.getNumPartitions()}\")\n",
    "\n",
    "# Print partition data\n",
    "def show_data(partition_name):\n",
    "    print(\"----------------------------------------------\")\n",
    "    return(element for i, element in enumerate(partition_name) if i <= 5)\n",
    "rdd_mappedPartition = rdd_us_zipcodes.mapPartitions(show_data)\n",
    "rdd_mappedPartition.collect()\n",
    "\n",
    "rdd_header = rdd_us_zipcodes.first()\n",
    "rdd_zipcodes = (\n",
    "    rdd_us_zipcodes\n",
    "    .filter(lambda l: l != rdd_header)\n",
    "    .map(lambda l: tuple(l.split(\",\"))\n",
    "        )\n",
    "\n",
    "## Another way of partitioning data\n",
    "rdd_zipcodes_partitions = rdd_zipcodes.partitionBy(7)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day9\">[Day 9](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Broadcast, Accumulator, Caching/Persisting\n",
    "\n",
    "[Code Files](./bigdata)<br>\n",
    "--> PySpark RDD APIs.ipynb)<br>\n",
    "--> PySpark Broadcasting (basics).ipynb<br>\n",
    "--> PySpark Accumulator (basics).ipynb<br>\n",
    "--> PySpark Caching (basics).ipynb<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day10\">[Day 10](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hive, Databricks community (account)\n",
    "\n",
    "Databricks - maintains Spark\n",
    "Databricks - works with AWS, Azure.\n",
    "\n",
    "Databricks - Community Edition\n",
    "Create a cluster (valid for 2Hrs once idle)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive commands\n",
    "\n",
    "```bash\n",
    "cd ~\n",
    "\n",
    "wget https://mirrors.estointernet.in/apache/hive/hive-2.3.8/apache-hive-2.3.8-bin.tar.gz\n",
    "\n",
    "tar xf apache-hive-2.3.8-bin.tar.gz\n",
    "sudo mv apache-hive-2.3.8-bin /opt/apache-hive-2.3.8\n",
    "\n",
    "sudo chmod 777 /opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "```bash\n",
    "# paste HIVE_HOME \n",
    "sudo nano /etc/environment \n",
    "HIVE_HOME=/opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "```bash\n",
    "# Now, export after exiting from file\n",
    "export HIVE_HOME=/opt/apache-hive-2.3.8\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "cp $HIVE_HOME/conf/hive-site.xml $HIVE_HOME/conf/hive-site.xml.bak\n",
    "\n",
    "wget -P $HIVE_HOME/conf https://raw.githubusercontent.com/nodesense/cts-aws-spark-april-2021/main/hive-site.xml\n",
    "\n",
    "ls $HIVE_HOME/conf\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/schematool -initSchema -dbType derby\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "# You should see these stats:\n",
    "SLF4J: Class path contains multiple SLF4J bindings.\n",
    "SLF4J: Found binding in [jar:file:/opt/apache-hive-2.3.8/lib/log4j-slf4j-impl-2.6.2.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: Found binding in [jar:file:/opt/hadoop-2.7.7/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]\n",
    "SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.\n",
    "SLF4J: Actual binding is of type [org.apache.logging.slf4j.Log4jLoggerFactory]\n",
    "Metastore connection URL:        jdbc:derby:;databaseName=metastore_db;create=true\n",
    "Metastore Connection Driver :    org.apache.derby.jdbc.EmbeddedDriver\n",
    "Metastore connection User:       APP\n",
    "Starting metastore schema initialization to 2.3.0\n",
    "Initialization script hive-schema-2.3.0.derby.sql\n",
    "Initialization script completed\n",
    "schemaTool completed\n",
    "```\n",
    "\n",
    "```bash\n",
    "# NOTE: Stay in the same directory as above!\n",
    "$HIVE_HOME/bin/hive\n",
    "\n",
    "```\n",
    "\n",
    "```bash\n",
    "hive> show tables;\n",
    "\n",
    "# to create database, you have two options, create database/schema\n",
    "hive> SHOW DATABASES;\n",
    "hive > CREATE DATABASE ordersdb;\n",
    "hive> SHOW DATABASES;\n",
    "# Create tables in the new DB\n",
    "hive? USER ordersdb;\n",
    "\n",
    "hive> CREATE TABLE IF NOT EXISTS brands(id INT, name STRING);\n",
    "hive> INSERT INTO brands(id,name) values(1, 'Apple');\n",
    "hive> SELECT * from brands;\n",
    "hive>  DROP TABLE BRANDS;\n",
    "hive >  DROP DATABASE ordersdb;\n",
    "```\n",
    "\n",
    "\n",
    "```bash\n",
    "create table test(id int ,name string ) clustered by (id) into 2 buckets stored as orc TBLPROPERTIES('transactional'='true');\n",
    "\n",
    "INSERT INTO test(id,name) values(1, 'Apple');\n",
    " SELECT * from test;\n",
    "\n",
    "update test set name='Google' where id=1;\n",
    "\n",
    "SELECT * from test;\n",
    "delete from test;\n",
    "truncate table test;\n",
    "SELECT * from test; \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive help\n",
    "\n",
    "```bash\n",
    "\n",
    "# remove meta data store\n",
    "cd $HIVE_HOME\n",
    "rm -rf metastore_db/\n",
    "\n",
    "\n",
    "# Create a new one\n",
    "$HIVE_HOME/bin/schematool -initSchema -dbType derby\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All about Hive\n",
    "\n",
    "Hive - a Datawarehousing solution over a MapReduce framework\n",
    "<br>Hive doesn't use Spark, instead Spark uses Hive\n",
    "\n",
    "Hive Key priciples:\n",
    "1. SQL like HQL (HiveQL) - not compatible with ANSI SQL.\n",
    "2. Allows programmers to plugin custom Mappers and Reducers (Extension)\n",
    "3. Data Warehouse Infrastructure\n",
    "4. Provides tools to enable easy ETL\n",
    "\n",
    "Hive Data Model: Data in HIve organized into..\n",
    "<br>Tables\n",
    "<br>Partitions\n",
    "<br>Buckets\n",
    "\n",
    "\n",
    "NOTE: Look into External Tables support for Hive\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hive Tables\n",
    "2 types of tables:\n",
    "<br>1. Managed Tables\n",
    "<br>2. External Tables\n",
    "\n",
    "#### Managed Tables / Internal Tables\n",
    "Table ops/ data in HDFS/ lifecycle(CRUD ops)) - Managed via Hive\n",
    "<br>We cannot directly use data managed by Hive. It has to be only done via CLI/JDBC/ODBC via HQL\n",
    "\n",
    "#### [External Tables](https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/notes/may-10-2021-hive-tables.md)\n",
    "Only tables (metadata) can be managed by Hive. Everything else (data/ lifecycle ops) are managed externally (developers/ other systems)\n",
    "<br>Data can also be stored in AWS S3 buckets (insted of HDFS) These external locations are supported by Drivers. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Hive  External Tables](https://github.com/nodesense/cts-aws-spark-april-2021/blob/main/hive-external-table.md)\n",
    "\n",
    "```bash\n",
    "echo \"1,jane,sales\" > employees1.csv\n",
    "echo \"2,john,marketting\" >> employees1.csv\n",
    "\n",
    "echo \"3,will,account\" > employees2.csv\n",
    "echo \"4,smith,qa\" >> employees2.csv\n",
    "\n",
    "hdfs dfs -mkdir /employees\n",
    "hdfs dfs -chmod 777 /employees\n",
    "\n",
    "hdfs dfs -put employees1.csv /employees\n",
    "hdfs dfs -put employees2.csv /employees\n",
    "\n",
    "# Launch Hive ($HIVE_HOME/bin/hive)\n",
    "\n",
    "# This staging/intermediate table is called a `Layer` in DATALAKE terminology.\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS employees_ext(\n",
    "  employee_id INT, \n",
    "  name STRING, \n",
    "  dept STRING\n",
    "  )\n",
    "  COMMENT 'Employee Names'\n",
    "  ROW FORMAT DELIMITED\n",
    "  FIELDS TERMINATED BY ','\n",
    "  STORED AS TEXTFILE\n",
    "  LOCATION '/employees';\n",
    "\n",
    "\n",
    "SELECT * FROM employees_ext;\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS employees(\n",
    "  employee_id INT, \n",
    "  name STRING, \n",
    "  dept STRING\n",
    "  )\n",
    "  COMMENT 'employees names managed';\n",
    "\n",
    "INSERT OVERWRITE TABLE employees SELECT * FROM employees_ext;\n",
    "\n",
    "SELECT * from employees; \n",
    "DROP TABLE employees_ext;\n",
    "SELECT * from employees;   \n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    " insert into employees_ext (employee_id, name, dept) values(8,'krish','sales');\n",
    "```\n",
    "\n",
    "```\n",
    " hdfs dfs -ls /employees\n",
    "  hdfs dfs -cat /employees/000000_0\n",
    "\n",
    "```\n",
    "\n",
    "```\n",
    "insert into employees_ext (employee_id, name, dept) values(9,'nila','sales');\n",
    "\n",
    "```\n",
    "```\n",
    "hdfs dfs -ls /employees\n",
    "\n",
    "you may notice /employees/000000_0_copy_1 created \n",
    "\n",
    "hdfs dfs -cat /employees/000000_0\n",
    "hdfs dfs -cat /employees/000000_0_copy_1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable JDBC Interface for Hive (to enable python and others to connect to Hive)\n",
    "```bash\n",
    "$ cd HIVE_HOME/bin\n",
    "$ beeline -u jdbc:/hive2://\n",
    "# -u (URL)\n",
    "```\n",
    " \n",
    "_Connect to any DB using `beeline`._\n",
    "<br>___NOTE: This will not run Thrift Server. It only uses a Thrift Server library.___\n",
    "\n",
    "\n",
    "```bash\n",
    "$ beeline --service metastore\n",
    "# This will start ThriftServer\n",
    "\n",
    "$ jps\n",
    "# Look for `RunJar` entry\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day11\">[Day 11](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Dataframes (and its APIs)\n",
    "\n",
    "PySpark DataFrames (Code in individual Jupyter notebooks)\n",
    "<br>PySpark SparkSession with Context\n",
    "<br>PySpark DataFrame with Custom Schema\n",
    "<br>PySpark DataFrame from CSV\n",
    "<br>PySpark RDD DataFrame from CSV\n",
    "<br>PySpark Read JSON\n",
    "<br>PySpark RDD's DataFrames and its APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day12\">[Day 12](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Cluster (Create Master, Workers, and assign Cluster Master to the Driver)\n",
    "<br><br>\n",
    "___Create Spark Cluster Master___\n",
    "```bash\n",
    "# Open command prompt, run cluster master a.k.a cluster manager\n",
    "$ spark-class org.apache.spark.deploy.master.Master\n",
    "# This will start spark cluster manager. In Spark, it's termed Spark Master\n",
    "# Look for the URL in the terminal, ex: 192.168.11.77:7077\n",
    "\n",
    "21/05/12 18:45:51 INFO Utils: Successfully started service 'sparkMaster' on port 7077.\n",
    "21/05/12 18:45:51 INFO Master: Starting Spark master at spark://192.168.11.77:7077\n",
    "21/05/12 18:45:51 INFO Master: Running Spark version 2.4.7\n",
    "21/05/12 18:45:51 INFO Utils: Successfully started service 'MasterUI' on port 8080.\n",
    "21/05/12 18:45:51 INFO MasterWebUI: Bound MasterWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8080\n",
    "21/05/12 18:45:51 INFO Master: I have been elected leader! New state: ALIVE\n",
    "21/05/12 18:47:19 INFO Master: 192.168.11.77:63058 got disassociated, removing it.\n",
    "```\n",
    "Now check, http://localhost:8080/\n",
    "\n",
    "<br><br>\n",
    "___Create Worker 1___\n",
    "<br>Open another (separate) py37 terminal:\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "\n",
    "21/05/12 18:54:53 INFO Utils: Successfully started service 'sparkWorker' on port 63142.\n",
    "21/05/12 18:54:53 INFO Worker: Starting Spark worker 192.168.11.77:63142 with 8 cores, 23.0 GB RAM\n",
    "21/05/12 18:54:53 INFO Worker: Running Spark version 2.4.7\n",
    "21/05/12 18:54:53 INFO Worker: Spark home: C:\\spark-2.4.7-bin-hadoop2.7\n",
    "21/05/12 18:54:53 INFO Utils: Successfully started service 'WorkerUI' on port 8081.\n",
    "21/05/12 18:54:53 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8081\n",
    "21/05/12 18:54:53 INFO Worker: Connecting to master 192.168.11.77:7077...\n",
    "21/05/12 18:54:53 INFO TransportClientFactory: Successfully created connection to /192.168.11.77:7077 after 40 ms (0 ms spent in bootstraps)\n",
    "21/05/12 18:54:53 INFO Worker: Successfully registered with master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "___Create Worker 2___\n",
    "<br>Open yet another (separate) py37 terminal:\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "\n",
    "21/05/12 19:06:21 INFO Utils: Successfully started service 'sparkWorker' on port 63243.\n",
    "21/05/12 19:06:22 INFO Worker: Starting Spark worker 192.168.11.77:63243 with 8 cores, 23.0 GB RAM\n",
    "21/05/12 19:06:22 INFO Worker: Running Spark version 2.4.7\n",
    "21/05/12 19:06:22 INFO Worker: Spark home: C:\\spark-2.4.7-bin-hadoop2.7\n",
    "21/05/12 19:06:22 WARN Utils: Service 'WorkerUI' could not bind on port 8081. Attempting port 8082.\n",
    "21/05/12 19:06:22 INFO Utils: Successfully started service 'WorkerUI' on port 8082.\n",
    "21/05/12 19:06:22 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://DESKTOP-BTFVSHG:8082\n",
    "21/05/12 19:06:22 INFO Worker: Connecting to master 192.168.11.77:7077...\n",
    "21/05/12 19:06:22 INFO TransportClientFactory: Successfully created connection to /192.168.11.77:7077 after 41 ms (0 ms spent in bootstraps)\n",
    "21/05/12 19:06:22 INFO Worker: Successfully registered with master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "___Associate Cluster Master with the Driver___\n",
    "<br>Open command prompt (anaconda py37), run spark shell with default config\n",
    "<br>Copy master url and paste as below:\n",
    "```bash\n",
    "$ pyspark --master spark://192.168.11.77:7077\n",
    "```\n",
    "\n",
    "Stop spark shell.\n",
    "\n",
    "<br><br>\n",
    "___Customizing Worker requirements___\n",
    "<br>Start spark shell with specific requirements..\n",
    "\n",
    "```bash\n",
    "# Use one of these to work on locally (Run on anaconda py37)\n",
    "$ pyspark --master spark://192.168.11.77:7077 --driver-memory 6G --driver-cores 2 --executor-memory 4G --executor-cores 2\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2\n",
    "```\n",
    "\n",
    "<br><br>\n",
    "__Customizing Number of Executors__\n",
    "```bash\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "\n",
    "\n",
    "(py37) C:\\Users\\Administrator>pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "Python 3.7.7 (default, May  6 2020, 11:45:54) [MSC v.1916 64 bit (AMD64)] :: Anaconda, Inc. on win32\n",
    "Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n",
    "21/05/12 19:37:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
    "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
    "Setting default log level to \"WARN\".\n",
    "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
    "21/05/12 19:37:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
    "Welcome to\n",
    "      ____              __\n",
    "     / __/__  ___ _____/ /__\n",
    "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.7\n",
    "      /_/\n",
    "\n",
    "Using Python version 3.7.7 (default, May  6 2020 11:45:54)\n",
    "SparkSession available as 'spark'.\n",
    ">>>\n",
    "```\n",
    "\n",
    "* Check the port numbers on console/terminal for Web UI.\n",
    "\n",
    "\n",
    "\n",
    "<br><br>\n",
    "__CONCLUSION__\n",
    "```bash\n",
    "$ spark-class org.apache.spark.deploy.master.Master\n",
    "# Create Spark Master (Get the IP:port)\n",
    "\n",
    "# Launch http://192.168.11.77:8080/ --> Get this from the terminal (from above command)\n",
    "\n",
    "$ spark-class org.apache.spark.deploy.worker.Worker spark://192.168.11.77:7077\n",
    "# Create Workers (in separate terminals)\n",
    "\n",
    "$ pyspark --master spark://192.168.11.77:7077 --executor-memory 2G --executor-cores 2 --num-executors 1\n",
    "# Assign workers to the Driver\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `spark-config`\n",
    "PySpark spark-config.ipynb\n",
    "<br>PySpark spark-config - In Parallel.ipynb\n",
    "<br>assignment HDFS-(spark-config)-DF.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day13\">[Day 13](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "assignment e-commerce.ipynb \n",
    "\n",
    "## RDD DF JOINs\n",
    "PySpark DF JOINs.ipynb _`INNER, OUTER, LEFT/RIGHT OUTER, LEFTSEMI, LEFTANTI`_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day14\">[Day 14](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PySpark Spark SQL - createOrReplaceGlobalTempView().ipynb\n",
    "<br>PySpark Spark SQL - createOrReplaceTempView().ipynb\n",
    "<br>PySpark Spark Database.ipynb\n",
    "<br>PySpark Spark Database - COPY (LOCAL).ipynb\n",
    "<br>PySpark Spark Database - DF JOINs (LOCAL).ipynb\n",
    "<br>PySpark Spark Database - DF JOINs (HIVE).ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Spark SQL Warehouse preparation\n",
    "(Refer Day 10's Hive)\n",
    "<br>__WINDOWS ONLY__\n",
    "<br>Create `c:\\spark`, `c:\\spark-temp`\n",
    "\n",
    "In command prompt, run\n",
    "```bash\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\spark\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\spark-temp\n",
    "%HADOOP_HOME%\\bin\\winutils.exe  chmod 777  C:\\tmp\\hive\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day15\">[Day 15](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Hive's `meta-server`\n",
    ">NOTE: This will run Hive's `meta-server` (not `thrift-server`)\n",
    ">\n",
    ">__Start meta server on port 9083__ [`9083` is configured in `hive-site.xml`]\n",
    ">```bash\n",
    "cd $HIVE_HOME \n",
    "$HIVE_HOME/bin/hive --service metastore\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Redshift\n",
    "\n",
    "Modified PostgreSQL == Redshift"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Interface PostgreSQL with Hive\n",
    "\n",
    "```bash\n",
    "# --------------------------------------------------------------------\n",
    "# From `git-bash`\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "sudo sh -c 'echo \"deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n",
    "\n",
    "wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
    "\n",
    "sudo apt-get update\n",
    "sudo apt-get -y install postgresql-12\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Create Metastore database and user accounts:\n",
    "sudo -u postgres psql\n",
    "\n",
    "postgres=# CREATE USER hiveuser WITH PASSWORD 'mypassword';\n",
    "postgres=# CREATE DATABASE metastore;\n",
    "postgres=# exit\n",
    "\n",
    "\n",
    "# check username and password exist, type mypassword for password\n",
    "psql -h localhost -U hiveuser -d metastore\n",
    "metastore=# exit\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Download JDBC driver for Spark/Python program to connect.. also for Hive meta-store\n",
    "cd ~\n",
    "wget https://jdbc.postgresql.org/download/postgresql-42.2.20.jar\n",
    "sudo mv postgresql-42.2.20.jar /usr/share/java/postgresql-jdbc.jar\n",
    "\n",
    "\n",
    "sudo chmod 644 /usr/share/java/postgresql-jdbc.jar\n",
    "sudo ln -s /usr/share/java/postgresql-jdbc.jar $HIVE_HOME/lib/postgresql-jdbc.jar\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Updating config files that has postgresql user credentials..\n",
    "\n",
    "# Make postgres listen on ip 192.168.93.128 \n",
    "# Edit via Ubuntu Desktop (gedit is a GUI editor)\n",
    "sudo gedit  /etc/postgresql/12/main/postgresql.conf\n",
    "\n",
    "\n",
    "# Search & update entry \"listen_adderss\" to\n",
    "listen_addresses = '*'\n",
    "# Save and exit\n",
    "\n",
    "# Enable postgresql to accept connection from specific ip. \n",
    "sudo gedit  /etc/postgresql/12/main/pg_hba.conf \n",
    "# Paste below line at the very end (replace the one you add if there's anything with this)\n",
    "host    all             all             0.0.0.0/0            md5\n",
    "# Save and exit\n",
    "\n",
    "# Restart\n",
    "sudo service postgresql restart\n",
    "\n",
    "# Update hive-site.xml\n",
    "mv $HIVE_HOME/conf/hive-site.xml $HIVE_HOME/conf/hive-site.xml.bak\n",
    "wget -P $HIVE_HOME/conf https://raw.githubusercontent.com/nodesense/cts-aws-spark-april-2021/main/pg/hive-site.xml\n",
    "\n",
    "\n",
    "# Close any running Hive meta-server\n",
    "# init postgres schema\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/schematool -dbType postgres -initSchema\n",
    "\n",
    "# Finally, to use postgres as a metastore..\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/hive --service metastore\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Running ThriftServer (for web/jdbc interface)\n",
    "__$HIVE_HOME/bin/hive/`hiveserver2` is ThriftServer's service name.__\n",
    "<br><br>\n",
    "\n",
    "```bash\n",
    "# Running Hive's thrift-server\n",
    "# hiveserver2 for JDBC, Web Access\n",
    "\n",
    "cd $HIVE_HOME\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Option 1: Run as a server\n",
    "$HIVE_HOME/bin/hiveserver2\n",
    "\n",
    "# Option 2: Run as service --> (Used in Training)\n",
    "$HIVE_HOME/bin/hive --service hiveserver2 --hiveconf hive.server2.thrift.port=10000 --hiveconf hive.root.logger=INFO,console\n",
    "# $HIVE_HOME/bin/hive --service hiveserver2\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Optional check: Is hiveserver2 running on port 10000 (default)?\n",
    "netstate -anp | grep 10000\n",
    "```\n",
    "\n",
    "__Hive server web UI runs on port 10002 (displayed on thrift server console)__\n",
    "[http://192.168.93.128:10002](http://192.168.93.128:10002)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## Connect hive using JDBC interface\n",
    "\n",
    "JDBC is useful to connect to Database using Java, Python, other languages, including Spark.\n",
    "<br>JDBC Client application [our python,java, scala code, spark] shall connect to DB server using JDBC technologies.\n",
    "<br>Then client application send queries to DB server; DB server executes queries and returns the result.\n",
    "\n",
    "### Beeline (is a JDBC client)\n",
    ">.. is a simple, JDBC client tool\n",
    "<br>.. useful in connecting to as many databases available\n",
    " \n",
    "```bash\n",
    "# connect to hive using jdbc, using metastore_db location\n",
    "cd $HIVE_HOME\n",
    "$HIVE_HOME/bin/beeline -u jdbc:hive2://\n",
    " \n",
    "# perform queries\n",
    "show tables;\n",
    "show databases;\n",
    "\n",
    "create table invoices(id STRING, amount INT);\n",
    "insert into invoices values('1', 1000); \n",
    "insert into invoices values('2', 2000);\n",
    "\n",
    "select * from invoices;\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day16\">[Day 16](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS EC2 | RDS (PostgreSQL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day17\">[Day 17](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apache Kafka\n",
    "__Install KAFKA__\n",
    "```bash\n",
    "# 1 --> Set KAFKA_HOME\n",
    "ssh ubuntu@192.168.93.128\n",
    "cd ~\n",
    "\n",
    "wget https://mirrors.estointernet.in/apache/kafka/2.8.0/kafka_2.12-2.8.0.tgz\n",
    "tar xf kafka_2.12-2.8.0.tgz\n",
    "\n",
    "sudo mv kafka_2.12-2.8.0 /opt\n",
    "sudo chmod 777 /opt/kafka_2.12-2.8.0\n",
    "\n",
    "# edit /etc/environment to add KAFKA_HOME\n",
    "sudo nano /etc/environment\n",
    "    KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "    \n",
    "# Load KAFKA_HOME for immediate use w/o restarting..\n",
    "nano ~/.bashrc\n",
    "    export KAFKA_HOME=/opt/kafka_2.12-2.8.0\n",
    "    export PATH=$PATH:$KAFKA_HOME/bin\n",
    "\n",
    "source ~/.bashrc\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__Run ZOOKEEPER__\n",
    "```bash\n",
    "# 2 --> (new terminal) Run Zookeeper; runs on port 2181\n",
    "ssh ubuntu@192.168.93.128\n",
    "zookeeper-server-start.sh $KAFKA_HOME/config/zookeeper.properties\n",
    "\n",
    "# 3 --> (new terminal) Run Kafka Broker: runs on port 9092\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-server-start.sh $KAFKA_HOME/config/server.properties\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__Create TOPIC__\n",
    "```bash\n",
    "# 4 --> (new terminal) Create Kafka's Topic(s)\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# Create Kafka Topic\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic test\n",
    "    > Created topic test.\n",
    "\n",
    "# List Kafka Topics\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "    > test\n",
    "\n",
    "# Describe Topics\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test\n",
    "    >       Topic: test     Partition: 0    Leader: 0       Replicas: 0     Isr: 0\n",
    "    # Isr --> InSyncReplicas\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__PRODUCER__\n",
    "```bash\n",
    "# 5 --> (new terminal) Create Kafka Producer(s) with comma-separated brokers [Produce message(s) to Kafka Broker]\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-console-producer.sh --broker-list localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "__CONSUMER__\n",
    "```bash\n",
    "# 6 --> (new terminal) Create Kafka Consumer(s) with comma-separated brokers [Comsume message(s) from Kafka Broker]\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning \n",
    "# --------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"day18\">[Day 18](#root)</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TOPIC with multiple-partitions\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic logs\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "### pub-sub with multiple partitions (run without key)\n",
    "With no key, data is partitioned in a Round-Robin fashion.\n",
    "__Run PRODUCER w/o key__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER w/o partition__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logs\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER with partition__\n",
    "```bash\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 0\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 1\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logs --partition 2\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "<br>\n",
    "\n",
    "### pub-sub with multiple partitions (run with key)\n",
    "With key, data is partitioned using hash value.\n",
    "\n",
    ">__TOPIC with multiple-partitions__\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic logger\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs,logger\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run PRODUCER w/o key__\n",
    "```bash\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic logger --property \"parse.key=true\" --property \"key.separator=:\"\n",
    "# --------------------------------------------------------------------\n",
    "```\n",
    "__Run CONSUMER with partition__\n",
    "```bash\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 0 --property \"print.key=true\"\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 1 --property \"print.key=true\"\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 2 --property \"print.key=true\"\n",
    "# --------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OFFSET\n",
    ">Offset is per partition. So, every partition has an offset that starts at 0.\n",
    "<br>Writing to a parition will increment the offset (automatic)\n",
    "<br>One can read from a specific offset by passing it as a parameter.\n",
    "\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "\n",
    "# Read from offset 2 onwards..\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic logger --partition 0 -offset 2 --property \"print.key=true\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "### TOPIC\n",
    "#### TOPIC Creation\n",
    "Refer above 2 cells\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 3 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic test,greetings,logs,logger,custom\n",
    "```\n",
    "\n",
    "#### TOPIC Deletion\n",
    "Topic deletion is not immediate, but eventual. Don't be alarmed if you're able to read previous messages post deletion. <br>Remember, deletion will not happen immediately.\n",
    "```bash\n",
    "ssh ubuntu@192.168.93.128\n",
    "kafka-topics.sh --delete --bootstrap-server localhost:9092 --topic custom\n",
    "```\n",
    "\n",
    "#### TOPIC Alteration\n",
    "Number of paritions can only be increased (not decreased)\n",
    "<br>Existing partition information remain as-is, but future publishes may be altered.\n",
    "```bash\n",
    "# Create partition\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 2 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "# Increase partitions\n",
    "kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 4 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "# Reduce partitions\n",
    "kafka-topics.sh --alter --bootstrap-server localhost:9092 --partitions 2 --topic custom\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic custom\n",
    "\n",
    "Error while executing topic command : Topic currently has 4 partitions, which is higher than the requested 2.\n",
    "[2021-05-20 21:47:46,243] ERROR org.apache.kafka.common.errors.InvalidPartitionsException: Topic currently has 4 partitions, which is higher than the requested 2.\n",
    " (kafka.admin.TopicCommand$)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kafka-Python\n",
    "\n",
    "```bash\n",
    "# 1\n",
    "pip install kafka-python\n",
    "\n",
    "# 2\n",
    "# Create Topic (ensure Zookeeper & Broker are already running)\n",
    "kafka-topics.sh --create --bootstrap-server localhost:9092 --replication-factor 1 --partitions 1 --topic transactions\n",
    "kafka-topics.sh --list --bootstrap-server localhost:9092\n",
    "kafka-topics.sh --describe --bootstrap-server localhost:9092 --topic transactions\n",
    "\n",
    "# 3\n",
    "# Create Producer\n",
    "kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test\n",
    "\n",
    "# 4\n",
    "# Create Consumer\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test\n",
    "OR\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning \n",
    "```\n",
    "\n",
    "<br>Kafka-Python.ipynb\n",
    "<br>Kafka-Python Producer.ipynb\n",
    "<br>Kafka-Python Consumer.ipynb\n",
    "<br>Kafka-Python Producer-for-ConsumerGroups.ipynb\n",
    "<br>Kafka-Python Consumer1(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer2(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer3(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer4(group1)-for-Producer.ipynb\n",
    "<br>Kafka-Python Consumer5(group1)-for-Producer.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "# <a id=\"handy-cmds\">[Handy Commands](#root)</a>\n",
    "```bash\n",
    "telnet localhost < port >\n",
    "\n",
    "/etc/hosts  # DNS entry\n",
    "\n",
    "conda env remove --name base\n",
    "\n",
    "# Deactivates (base) environment\n",
    "conda config --set auto_activate_base false\n",
    "```\n",
    "<br>\n",
    "\n",
    "```python\n",
    "# In RDD DF, addressing a column with pyspark.sql.functions.col(column_name) will make it independent of the DF\n",
    "pyspark.sql.functions.col()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id=\"references\">[References](#root)</a>\n",
    "[Why Hadoop?](https://www.slideshare.net/ApacheApex/introduction-to-hadoop-60884936?from_action=save)\n",
    "<br>[Movielens Dataset](https://files.grouplens.org/datasets/movielens/ml-latest-small.zip)\n",
    "<br>[pyspark Cheatsheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/PySpark_Cheat_Sheet_Python.pdf)\n",
    "<br>[Complete training material](https://github.com/nodesense/cts-aws-spark-april-2021)\n",
    "<br>[PySpark SQL types](https://spark.apache.org/docs/2.1.1/api/python/_modules/pyspark/sql/types.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
